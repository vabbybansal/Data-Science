{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/tweet-sentiment-extraction/train.csv\n/kaggle/input/glove-twitter/glove.twitter.27B.25d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.50d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.200d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.100d.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load the competetion datasets\n\nclass KaggleReader(object):\n    def __init__(self):\n        self.df_objects = {}        \n    def read_kaggle_df(self, df_name, df_path):\n        self.df_objects[df_name] = pd.read_csv(df_path)\n        print()\n        print(df_name + ' loaded')\n        print(\"Shape => \" + str(self.df_objects[df_name].shape))\n    \ndf_objs = KaggleReader()\ndf_objs.read_kaggle_df('train', \"../input/tweet-sentiment-extraction/train.csv\")\ndf_objs.read_kaggle_df('test', \"../input/tweet-sentiment-extraction/test.csv\")\ndf_objs.read_kaggle_df('submission', \"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":2,"outputs":[{"output_type":"stream","text":"\ntrain loaded\nShape => (27481, 4)\n\ntest loaded\nShape => (3534, 3)\n\nsubmission loaded\nShape => (3534, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom numpy import hstack, vstack\n# Tokenize text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.utils import to_categorical\nfrom nltk.tokenize import WordPunctTokenizer\n\n# Model\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, LSTM, Embedding, Dropout, Input, Bidirectional, TimeDistributed\nfrom keras.initializers import Constant\nimport keras\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.merge import concatenate","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.5","execution_count":169,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Handlers(object):\n    def __init__(self):\n        self.run_tests()\n        pass\n    \n    def draw_pr_curve_plt(self, Y_valid, y_pred, x_range=1.0):\n    #     (precision, recall, x_range=1.0):\n\n            precision, recall, thresholds_pr = precision_recall_curve(Y_valid, y_pred)\n\n            # import dependencies\n            import matplotlib.pyplot as plt\n\n            plt.step(recall, precision, color='b', alpha=0.2,\n                     where='post')\n            plt.fill_between(recall, precision, alpha=0.2, color='b')\n            plt.xlabel('Recall')\n            plt.ylabel('Precision')\n            plt.ylim([0.0, 1.05])\n            plt.xlim([0.0, x_range])\n            plt.show()\n     \n    def draw_roc_curve_plt(self, Y_valid, y_pred):\n#     (fpr, tpr, auc):\n        # import dependencies\n        import matplotlib.pyplot as plt\n        fpr, tpr, thresholds_roc = roc_curve(Y_valid, y_pred)\n        auc_roc = auc(fpr, tpr)\n        \n        plt.figure(1)\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.plot(fpr, tpr, label='(area = {:.3f})'.format(auc_roc))\n        # plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n        plt.xlabel('False positive rate')\n        plt.ylabel('True positive rate')\n        plt.title('ROC curve')\n        plt.legend(loc='best')\n        plt.show()\n\n    # Optimize using KMP\n    def return_start_end_indices(self, big, small):\n        if len(small) <= 0 or len(big) <= 0:\n            return (-1,-1)\n        i = 0\n        j = 0\n        started = False\n        while i < len(big):\n            if big[i] == small[j]:\n                if started == False:\n                    started = True\n                    i_start = i\n\n                if j == len(small) - 1:\n                    return (i-len(small)+1, i)\n                j += 1\n\n            else:\n                if started == True:\n                    started = False\n                    i = i_start + 1\n                j = 0\n            i += 1\n        return (-1,-1)\n    \n    def run_tests(self):\n        \n        assert self.return_start_end_indices(['i', '`', 'd', 'have', 'responded', ',', 'if', 'i', 'were', 'going'], ['have', 'responded', ',']) == (3,5)\n        assert self.return_start_end_indices('abc', '') == (-1,-1)\n        assert self.return_start_end_indices('abc', 'a') == (0,0)\n        assert self.return_start_end_indices('abc', 'b') == (1,1)\n        assert self.return_start_end_indices('abc', 'c') == (2,2)\n        assert self.return_start_end_indices('abc', 'ab') == (0,1)\n        assert self.return_start_end_indices('abc', 'bc') == (1,2)\n        assert self.return_start_end_indices('abc', 'ac') == (-1,-1)\n        assert self.return_start_end_indices('abc', 'abc') == (0,2)\n        assert self.return_start_end_indices('abcabcabc', 'abc') == (0,2)\n        assert self.return_start_end_indices('ababcc', 'abc') == (2,4)\n    \n    def jaccard(self, str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    \n    def load_gloVe_embeddings(self):\n        # Boilerplate taken from here - https://www.kaggle.com/stacykurnikova/using-glove-embedding\n        embeddings_index = {}\n        f = open('/kaggle/input/glove-twitter/glove.twitter.27B.25d.txt')\n        for line in f:\n            values = line.split(' ')\n            word = values[0] ## The first entry is the word\n            coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n            embeddings_index[word] = coefs\n        f.close()\n        print('GloVe data loaded')\n        return embeddings_index\n    \n    def load_embeddings_matrix(self, embeddings_index, index_tokenizer):\n        # https://www.kaggle.com/stacykurnikova/using-glove-embedding\n        # Create an embedding matrix with embedding vectors for the tokens recognized in the vocab of tweets\n        \n        EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n        # num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n        num_words = len(index_tokenizer.word_index) + 1\n\n        # To Do: constrain the vocab size\n        embedding_matrix = np.random.uniform(-1,+1,(num_words, EMBEDDING_DIM))\n        count_in_embedding_vocab = 0\n        for word, i in index_tokenizer.word_index.items():\n            embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n            if embedding_vector is not None:\n                # words not found in embedding index will be all-zeros.\n                embedding_matrix[i] = embedding_vector\n                count_in_embedding_vocab += 1\n#         TO DO:\n#             else:\n#                 PASS EMBEDDINGS OF SYNONYMS (!!! == !)\n        return embedding_matrix\n        # print(count_in_embedding_vocab)\n        # print(len(embedding_matrix))\n    \n    def get_token_indices(self, x):\n        span_generator = WordPunctTokenizer().span_tokenize(x)\n        spans = [span for span in span_generator]\n        return spans\n    \n    # TO DO: compensate for end and start on top of padding\n    def get_pred_text_span(self, x):\n        start_token_index = int(x['pred_start'])\n        end_token_index = int(x['pred_end'])\n        token_indices_list = x['tokens_indices']\n        text = x['text']\n\n        # start token ind > end token ind (could change logic to also have either 0:end)\n        if start_token_index > end_token_index:\n            return text\n\n        # start token ind and end token ind within bounds\n        elif start_token_index < len(token_indices_list) and end_token_index < len(token_indices_list):\n            return text[token_indices_list[start_token_index][0]: token_indices_list[end_token_index][1]]\n\n\n        # start token ind after bounds (could change logic to also have either 0:end or sth similar)\n        elif start_token_index >= len(token_indices_list):\n            return text\n\n        # only end token ind out of bounds\n        elif start_token_index < len(token_indices_list) and end_token_index >= len(token_indices_list):\n            return text[token_indices_list[start_token_index][0]: len(text)-1]\n        \n    def get_preds_out(self, preds_indexes, test_df):\n        temp_df = pd.concat([test_df, pd.DataFrame(preds_indexes, columns=['pred_start', 'pred_end'])], axis=1)\n        temp_df['tokens_indices'] = temp_df['trans_text'].apply(handlers.get_token_indices)\n        # print((temp_df['tokens_indices'].apply(len) == temp_df['tokens'].apply(len)).value_counts())\n\n        # Get the predictions\n        return temp_df.apply(handlers.get_pred_text_span, axis = 1)\n    \n    def get_indexes_from_argmax(self, preds, test_df):\n        start_preds = preds[0]\n        end_preds = preds[1]\n        start_preds = start_preds.reshape((start_preds.shape[0], start_preds.shape[1]))\n        end_preds = end_preds.reshape((end_preds.shape[0], end_preds.shape[1]))\n        start_inds = start_preds.argmax(axis=1)\n        end_inds = end_preds.argmax(axis=1)\n        return vstack([start_inds, end_inds]).transpose()\n        \n        \n    def batch_jaccard(self, preds, test_df):\n        temp_df = pd.concat([test_df, pd.DataFrame(preds, columns=['pred_start', 'pred_end'])], axis=1)\n        temp_df['tokens_indices'] = temp_df['trans_text'].apply(handlers.get_token_indices)\n        # print((temp_df['tokens_indices'].apply(len) == temp_df['tokens'].apply(len)).value_counts())\n\n        # Get the predictions\n        temp_df['out_pred_span'] = temp_df.apply(handlers.get_pred_text_span, axis = 1)\n        return temp_df.apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean()\n    \n    \n    def batch_jaccard_from_argmax(self, preds, test_df):\n        start_preds = preds[0]\n        end_preds = preds[1]\n        start_preds = start_preds.reshape((start_preds.shape[0], start_preds.shape[1]))\n        end_preds = end_preds.reshape((end_preds.shape[0], end_preds.shape[1]))\n        start_inds = start_preds.argmax(axis=1)\n        end_inds = end_preds.argmax(axis=1)\n        return self.batch_jaccard(vstack([start_inds, end_inds]).transpose(), test_df)    \n\n    def batch_jaccard_from_continuous_span(self, preds, test_df, cont_span_method='get_min_max_span_indices'):\n        return self.batch_jaccard(\n            handlers.get_pos_indices_continuous_span(preds, cont_span_method), \n            test_df\n        )        \n    \n\n\n    def get_pos_indices_multi_output(self, preds):\n        return (preds[:,1]>THRESHOLD)\n\n    def get_min_max_span_indices(self, preds):\n        pos_preds_multi_output = self.get_pos_indices_multi_output(preds)\n    #     print(pos_preds_multi_output)\n        out_min, out_max = MAX_SEQUENCE_LENGTH+1, -1\n        for i in range(len(pos_preds_multi_output)):\n            if pos_preds_multi_output[i] == True:\n                out_min = min(out_min, i)\n                out_max = max(out_max, i)\n        # check validity\n        if out_min>out_max:return((-1,-1))\n        else: return((out_min, out_max))\n\n    def get_max_continuous_span_indices(self, preds):\n        pos_preds_multi_output = self.get_pos_indices_multi_output(preds)\n        left, right = -1, -2\n        i = 0\n        while i < len(pos_preds_multi_output):\n            if pos_preds_multi_output[i]==True:\n                temp_left = i\n                while (i < len(pos_preds_multi_output)) and (pos_preds_multi_output[i] == True):\n                    temp_right = i\n                    i+=1    \n                if temp_right-temp_left > right - left:\n                    right = temp_right\n                    left = temp_left\n            i+=1\n        if right-left < 0: return (-1,-1)\n        else: return((left, right))\n\n    def get_pos_indices_continuous_span(self, preds, method='get_min_max_span_indices'):\n        arr = np.empty((0,2), int)\n        if method == 'get_min_max_span_indices':\n            func = self.get_min_max_span_indices\n        elif method == \"get_max_continuous_span_indices\":\n            func = self.get_max_continuous_span_indices\n        for i in range(len(preds)):\n            row = func(preds[i])\n            arr = vstack([arr, np.array([row[0], row[1]])])\n        return arr\n\n        # get_pos_indices_continuous_span(preds, 'get_min_max_span_indices')\n\nhandlers = Handlers()\n\n\n# To add Jaccard Similarity Value on Validation set after each epoch\nclass Metrics(keras.callbacks.Callback):\n    # https://stackoverflow.com/questions/37657260/how-to-implement-custom-metric-in-keras\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, X_sentiment, y_val = self.validation_data[0], self.validation_data[1], self.validation_data[2]\n        y_predict = np.asarray(self.model.predict([X_val, X_sentiment]))\n        print('Val Jaccard Similarity: {}'.format(handlers.batch_jaccard(y_predict, test_df))) \n        return\n\n    def get_data(self):\n        return self._data\n    \n    \nclass MetricsCategorical(keras.callbacks.Callback):\n    # https://stackoverflow.com/questions/37657260/how-to-implement-custom-metric-in-keras\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, X_numerics, y_val = self.validation_data[0], self.validation_data[1], self.validation_data[2]\n        y_predict = np.asarray(self.model.predict([X_val, X_numerics]))\n        print('Val Jaccard Similarity: {}'.format(handlers.batch_jaccard_from_argmax(y_predict, test_df))) \n        return\n\n    def get_data(self):\n        return self._data    \n\nclass MetricsContinuousSpan(keras.callbacks.Callback):\n    # https://stackoverflow.com/questions/37657260/how-to-implement-custom-metric-in-keras\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, X_numerics, y_val = self.validation_data[0], self.validation_data[1], self.validation_data[2]\n        y_predict = np.asarray(self.model.predict([X_val, X_numerics]))\n        print('Val Jaccard Similarity (min_max): {}'.format(handlers.batch_jaccard_from_continuous_span(y_predict, test_df, 'get_min_max_span_indices'))) \n        print('Val Jaccard Similarity (max_con): {}'.format(handlers.batch_jaccard_from_continuous_span(y_predict, test_df, 'get_max_continuous_span_indices'))) \n        return\n\n    def get_data(self):\n        return self._data\n    \nmetrics = Metrics()\nmetrics_categorical = MetricsCategorical()\nmetrics_continuous_span = MetricsContinuousSpan()","execution_count":197,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 40\n\nclass DataManipulationPipeline(object):\n    def __init__(self):\n        # Load dependencies\n        try:\n            self.handlers = handlers\n        except NameError:\n            self.handlers = Handlers()\n            \n        # init vars\n        self.vars = {\n            'MAX_SEQUENCE_LENGTH' : MAX_SEQUENCE_LENGTH\n        }\n\n    def pre_fit(self, X):\n        \n        # Strip the text\n        X['text'] = X['text'].str.strip()\n        # Lower case the text\n        X['trans_text'] = X['text'].apply(str.lower)\n        return X\n\n    def create_sequences(self, X):\n        # Tokenize the word tokens to word_indexes\n        sequences = self.vars['keras_index_tokenizer'].texts_to_sequences(X['tokens'])\n        # Pad the sequences to be fed to NN [Note that this will effectively change the start, end index if padded on post]\n        sequences_padded = pad_sequences(sequences, maxlen=self.vars['MAX_SEQUENCE_LENGTH'], padding='post', truncating='post')        \n\n        return sequences_padded\n    \n    def create_Y(self, X):\n        X['trans_selected_text'] = X['selected_text'].apply(str.lower)\n        X = self.punct_tokenize(X, 'trans_selected_text', 'tokens_selected_text', self.vars['punct_tokenizer'])\n        X['start_end_indices'] = X.apply(lambda x: handlers.return_start_end_indices(x['tokens'], x['tokens_selected_text']), axis=1)\n        # truncate the start end indices to end of MAX LEN of sequence\n        X['start_end_indices'] = X['start_end_indices'].apply(lambda x: (x[0], (MAX_SEQUENCE_LENGTH-1) if x[1]>= MAX_SEQUENCE_LENGTH else x[1]))\n        \n        X = X[(X['start_end_indices'] != (-1,-1))]\n        X = X.reset_index(drop=True)\n        X['start_ind'] = X['start_end_indices'].apply(lambda x: x[0])\n        X['end_ind'] = X['start_end_indices'].apply(lambda x: x[1])\n        Y = hstack(\n            (\n                X['start_ind'].values.reshape(X.shape[0],1),\n                X['end_ind'].values.reshape(X.shape[0],1)\n            )\n        )\n        return Y,X\n    \n    def create_Y_Categorical(self, Y):\n        Y_categorical = np.array([[0]*MAX_SEQUENCE_LENGTH for j in range(Y.shape[0])])\n        for i in range(Y.shape[0]):\n            for j in range(Y[i][0], Y[i][1]+1):\n                Y_categorical[i][j] = 1\n        \n        return keras.utils.to_categorical(Y_categorical, 2)\n        \n    \n    def handle_sentiment_feature_eng(self, X):\n        sentiment_transform = self.vars['sentiment_one_hot'].transform(X['sentiment'].values.reshape((X['sentiment'].shape[0],1)))\n        # Copy over each data row for MAX_SEQUENCE_LENGTH times to send it inside each LSTM sequence\n#         sentiment_transform_repeated = np.array([([sentiment_transform[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n        \n        # To Do: Scale?\n        return sentiment_transform\n    \n    def text_stats(self, X):\n        X['char_len'] = X['trans_text'].apply(len)\n        X['word_len'] = X['tokens'].apply(len)\n        X['char_word_ratio'] = X['char_len']/X['word_len']\n            \n        text_stats_singular = X[['char_len', 'word_len', 'char_word_ratio']].values\n        \n        # Standardize\n#         text_stats_singular        \n\n#         text_stats_repeated = np.array([([text_stats_singular[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n        return text_stats_singular\n\n    def repeater(self, X):\n        return np.array([([X[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n    \n        \n    \n    # Fits the parameters on train\n    def fit_transform(self, X):\n        \n        # {x}\n        # Remove nulls from train\n        X = X.dropna()\n        \n        X = self.pre_fit(X)\n        \n        # Create word tokens from sentences using NTLK\n        self.vars['punct_tokenizer'] = WordPunctTokenizer()\n        # put sth else inseatd of X foe expanded vocab?\n        X = self.punct_tokenize(X, 'trans_text', 'tokens', self.vars['punct_tokenizer'])\n\n        # Fit Index-Tokenizer the vocab using Keras\n        self.vars['keras_index_tokenizer'] = Tokenizer()\n        # Use all words for extended voacb\n        self.vars['keras_index_tokenizer'].fit_on_texts(\n            pd.concat(\n            [\n                df_objs.df_objects['train']['text'].dropna().str.strip().apply(str.lower),\n                df_objs.df_objects['test']['text'].dropna().str.strip().apply(str.lower)\n            ]\n        ).reset_index(drop=True).apply(WordPunctTokenizer().tokenize)\n        )\n        # len(keras_tokenizer.word_index)\n        \n        # load glove embeddings\n        self.vars['embeddings_index'] = self.handlers.load_gloVe_embeddings() \n        self.vars['embeddings_matrix'] = self.handlers.load_embeddings_matrix(self.vars['embeddings_index'], self.vars['keras_index_tokenizer'])\n\n        # {y}\n        # Create label column - Word Tokenize selected_text, and create label indices\n        Y,X = self.create_Y(X)\n        Y_categorical = self.create_Y_Categorical(Y)\n        # Run final X transform after Y since Y transform filters out some X\n        sequences_padded = self.create_sequences(X)\n        \n        # Fit one hot encoder for sentiment (Try standard scalar downstream as well?)\n        self.vars['sentiment_one_hot'] = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        self.vars['sentiment_one_hot'].fit(X['sentiment'].values.reshape((X['sentiment'].shape[0],1)))\n        # print(enc.get_feature_names())\n        X_sentiment_one_hot = self.handle_sentiment_feature_eng(X)\n        \n        # Create text stats\n        X_text_stats = self.text_stats(X)\n        self.vars['X_text_stats_standard_scaler'] = StandardScaler()\n        self.vars['X_text_stats_standard_scaler'].fit(X_text_stats)\n        X_text_stats_scaled = self.vars['X_text_stats_standard_scaler'].transform(X_text_stats)\n        X_agg_numerics_all = hstack([X_sentiment_one_hot, X_text_stats_scaled])\n        \n        X_numerics_repeated = self.repeater(X_agg_numerics_all)\n        \n        return (sequences_padded, X_numerics_repeated, Y_categorical, X)\n\n\n    # Transforms using the parameters on train\n    def transform(self, X):\n        \n        X = self.pre_fit(X)\n        \n        # Create word tokens\n        X = self.punct_tokenize(X, 'trans_text', 'tokens', self.vars['punct_tokenizer'])\n        \n        Y = None\n        Y_categorical = None\n        if 'selected_text' in X.columns:\n            Y,X = self.create_Y(X)\n            Y_categorical = self.create_Y_Categorical(Y)\n        sequences_padded = self.create_sequences(X)\n        X_sentiment_one_hot = self.handle_sentiment_feature_eng(X)        \n        # Create text stats\n        X_text_stats = self.text_stats(X)\n        X_text_stats_scaled = self.vars['X_text_stats_standard_scaler'].transform(X_text_stats)\n        \n        X_agg_numerics_all = hstack([X_sentiment_one_hot, X_text_stats_scaled])\n        X_numerics_repeated = self.repeater(X_agg_numerics_all)\n    \n        \n        return (sequences_padded, X_numerics_repeated, Y_categorical, X)\n        \n        \n    def punct_tokenize(self, X,old_col, new_col, punct_tokenizer):\n        X[new_col] = X[old_col].apply(punct_tokenizer.tokenize)\n        return X\n        \n        \n\n\n\n# Get the optimum number of word length for sequence model - 40\n# plt.hist(df_objs.df_objects['train']['tokens'].apply(len), bins=100)\n# plt.show()\n    \n        \n        ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_objs.df_objects['train'], test_size=0.15, random_state=42)\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n# X_train, X_test, Y_train, Y_test, idx1, idx2 = train_test_split(df_objs.df_objects['train'], Y, np.arange(Y.shape[0]), test_size=0.15, random_state=42)\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mainpulation_pipeline = DataManipulationPipeline()\n(train_manip_X, train_numerics_repeated, train_manip_Y, train_df) = data_mainpulation_pipeline.fit_transform(train)\n(test_manip_X, test_numerics_repeated, test_manip_Y, test_df) = data_mainpulation_pipeline.transform(test)","execution_count":8,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:169: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","name":"stderr"},{"output_type":"stream","text":"GloVe data loaded\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_manip_Y.shape\n# train_df\n\n\n    \n# [0]*MAX_SEQUENCE_LENGTH\n\n\n# [j for j in range(11,22)]\n# train_manip_Y\n# import matplotlib.pyplot as plt\n# plt.hist(train_df.tokens.apply(len), bins=100)\n# plt.show()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (train_manip_X, train_numerics_repeated, train_manip_Y, train_df)\n# test_df\n# test_manip_Y[1]\n\n# print(test_manip_Y[2])\n\ntest_manip_Y.shape\n# print([2])","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(3804, 40, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = len(data_mainpulation_pipeline.vars['keras_index_tokenizer'].word_index)+1\nEMBEDDING_DIM = len(data_mainpulation_pipeline.vars['embeddings_index']['a'])","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\n# ----------------------------------------------------\ninputLayer_words = Input(shape=(MAX_SEQUENCE_LENGTH,))\ninputLayer_agg_numerics = Input(shape=(\n    MAX_SEQUENCE_LENGTH,\n    train_numerics_repeated.shape[2]\n))\n\n# Embedding layer for the tap names\nwordEmbeddings = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputLayer_words)\nwordEmbeddings = Embedding(\n    input_dim=VOCAB_SIZE, \n    output_dim=EMBEDDING_DIM, \n    input_length=MAX_SEQUENCE_LENGTH, \n    weights=[data_mainpulation_pipeline.vars['embeddings_matrix']],\n    trainable=False)(inputLayer_words)\n# \nmerged_Input = concatenate([wordEmbeddings, inputLayer_agg_numerics])\n# inputLayer_agg_indices\n# # LSTM\nlstm_1 = Bidirectional(LSTM(50, \n                            return_sequences = True,\n                            dropout=0.3,\n                            recurrent_dropout=0.3,\n                           ))(merged_Input)\n# input_shape=(25,(EMBEDDING_DIM))\ntd = TimeDistributed(Dense(25))(lstm_1)\n\n\n# drp1 = Dropout(0.2)(lstm_1)\n\n# # Dense\n# dense_0 = Dense(75, activation='relu')(drp1)\n# drp2 = Dropout(0.2)(dense_0)\n# dense_1 = Dense(30, activation='relu')(drp2)\noutputLayer = Dense(2, activation='softmax')(td)\n\nmodel_2 = Model(inputs=[inputLayer_words, inputLayer_agg_numerics], outputs=outputLayer)\n\nmodel_2.compile(loss='categorical_crossentropy', optimizer='adam')\n# model.compile(loss='mse', optimizer='adam', metrics=[jaccard])\nmodel_2.summary()\n# ----------------------------------------------------\n\nmodel_2.fit([train_manip_X, train_numerics_repeated], train_manip_Y, validation_data=([test_manip_X, test_numerics_repeated], test_manip_Y), batch_size=100, epochs=200, callbacks=[metrics_continuous_span])","execution_count":198,"outputs":[{"output_type":"stream","text":"Model: \"model_7\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_11 (InputLayer)           (None, 40)           0                                            \n__________________________________________________________________________________________________\nembedding_14 (Embedding)        (None, 40, 25)       728175      input_11[0][0]                   \n__________________________________________________________________________________________________\ninput_12 (InputLayer)           (None, 40, 6)        0                                            \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 40, 31)       0           embedding_14[0][0]               \n                                                                 input_12[0][0]                   \n__________________________________________________________________________________________________\nbidirectional_7 (Bidirectional) (None, 40, 100)      32800       concatenate_5[0][0]              \n__________________________________________________________________________________________________\ntime_distributed_7 (TimeDistrib (None, 40, 25)       2525        bidirectional_7[0][0]            \n__________________________________________________________________________________________________\ndense_14 (Dense)                (None, 40, 2)        52          time_distributed_7[0][0]         \n==================================================================================================\nTotal params: 763,552\nTrainable params: 35,377\nNon-trainable params: 728,175\n__________________________________________________________________________________________________\nTrain on 21561 samples, validate on 3804 samples\nEpoch 1/200\n21561/21561 [==============================] - 12s 572us/step - loss: 0.2092 - val_loss: 0.1508\nVal Jaccard Similarity (min_max): 0.5713710705058285\nVal Jaccard Similarity (max_con): 0.5622049741505509\nEpoch 2/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1713 - val_loss: 0.1440\nVal Jaccard Similarity (min_max): 0.5620865087749575\nVal Jaccard Similarity (max_con): 0.5535939586489874\nEpoch 3/200\n21561/21561 [==============================] - 11s 516us/step - loss: 0.1667 - val_loss: 0.1440\nVal Jaccard Similarity (min_max): 0.597315994793087\nVal Jaccard Similarity (max_con): 0.5832869655177227\nEpoch 4/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1643 - val_loss: 0.1432\nVal Jaccard Similarity (min_max): 0.6057922637812044\nVal Jaccard Similarity (max_con): 0.5929713124692305\nEpoch 5/200\n21561/21561 [==============================] - 12s 541us/step - loss: 0.1634 - val_loss: 0.1399\nVal Jaccard Similarity (min_max): 0.5993427468776753\nVal Jaccard Similarity (max_con): 0.5873965888496171\nEpoch 6/200\n21561/21561 [==============================] - 11s 523us/step - loss: 0.1613 - val_loss: 0.1386\nVal Jaccard Similarity (min_max): 0.605589094841504\nVal Jaccard Similarity (max_con): 0.5930803021042858\nEpoch 7/200\n21561/21561 [==============================] - 11s 520us/step - loss: 0.1592 - val_loss: 0.1398\nVal Jaccard Similarity (min_max): 0.6292010214760656\nVal Jaccard Similarity (max_con): 0.6165286946093458\nEpoch 8/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1604 - val_loss: 0.1371\nVal Jaccard Similarity (min_max): 0.6040578147265653\nVal Jaccard Similarity (max_con): 0.5955301342945849\nEpoch 9/200\n21561/21561 [==============================] - 12s 538us/step - loss: 0.1602 - val_loss: 0.1365\nVal Jaccard Similarity (min_max): 0.6206211942253472\nVal Jaccard Similarity (max_con): 0.6070179489235755\nEpoch 10/200\n21561/21561 [==============================] - 11s 517us/step - loss: 0.1606 - val_loss: 0.1357\nVal Jaccard Similarity (min_max): 0.6004629911146022\nVal Jaccard Similarity (max_con): 0.591385180311786\nEpoch 11/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1576 - val_loss: 0.1381\nVal Jaccard Similarity (min_max): 0.6436159386848009\nVal Jaccard Similarity (max_con): 0.6286027492974364\nEpoch 12/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1571 - val_loss: 0.1354\nVal Jaccard Similarity (min_max): 0.6045896815835367\nVal Jaccard Similarity (max_con): 0.5947443297216133\nEpoch 13/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1576 - val_loss: 0.1356\nVal Jaccard Similarity (min_max): 0.6319505282010811\nVal Jaccard Similarity (max_con): 0.6174083075863661\nEpoch 14/200\n21561/21561 [==============================] - 12s 535us/step - loss: 0.1561 - val_loss: 0.1341\nVal Jaccard Similarity (min_max): 0.6259787189318626\nVal Jaccard Similarity (max_con): 0.6143566795756041\nEpoch 15/200\n21561/21561 [==============================] - 11s 522us/step - loss: 0.1569 - val_loss: 0.1357\nVal Jaccard Similarity (min_max): 0.5958109415569939\nVal Jaccard Similarity (max_con): 0.5900786184678594\nEpoch 16/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1575 - val_loss: 0.1337\nVal Jaccard Similarity (min_max): 0.62855652691783\nVal Jaccard Similarity (max_con): 0.6177726988040484\nEpoch 17/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1568 - val_loss: 0.1347\nVal Jaccard Similarity (min_max): 0.6042154781422183\nVal Jaccard Similarity (max_con): 0.5962335794955815\nEpoch 18/200\n21561/21561 [==============================] - 12s 539us/step - loss: 0.1553 - val_loss: 0.1339\nVal Jaccard Similarity (min_max): 0.6442271215958354\nVal Jaccard Similarity (max_con): 0.6289395883614641\nEpoch 19/200\n21561/21561 [==============================] - 11s 517us/step - loss: 0.1550 - val_loss: 0.1342\nVal Jaccard Similarity (min_max): 0.649130274547666\nVal Jaccard Similarity (max_con): 0.6333893896104876\nEpoch 20/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1555 - val_loss: 0.1336\nVal Jaccard Similarity (min_max): 0.6465921122359286\nVal Jaccard Similarity (max_con): 0.6324906506412176\nEpoch 21/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1555 - val_loss: 0.1353\nVal Jaccard Similarity (min_max): 0.6512287350325784\nVal Jaccard Similarity (max_con): 0.6403615361622383\nEpoch 22/200\n21561/21561 [==============================] - 11s 510us/step - loss: 0.1541 - val_loss: 0.1351\nVal Jaccard Similarity (min_max): 0.6499373702690369\nVal Jaccard Similarity (max_con): 0.6375854819802381\nEpoch 23/200\n21561/21561 [==============================] - 12s 537us/step - loss: 0.1535 - val_loss: 0.1329\nVal Jaccard Similarity (min_max): 0.6475554323980077\nVal Jaccard Similarity (max_con): 0.6355615649056809\nEpoch 24/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1551 - val_loss: 0.1325\nVal Jaccard Similarity (min_max): 0.6464007819205521\nVal Jaccard Similarity (max_con): 0.6320775573763656\nEpoch 25/200\n21561/21561 [==============================] - 11s 523us/step - loss: 0.1538 - val_loss: 0.1316\nVal Jaccard Similarity (min_max): 0.6438011344335975\nVal Jaccard Similarity (max_con): 0.6306344617211024\nEpoch 26/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1540 - val_loss: 0.1346\nVal Jaccard Similarity (min_max): 0.6613158102138441\nVal Jaccard Similarity (max_con): 0.6469844389897678\nEpoch 27/200\n21561/21561 [==============================] - 12s 543us/step - loss: 0.1533 - val_loss: 0.1336\nVal Jaccard Similarity (min_max): 0.6498243395438195\nVal Jaccard Similarity (max_con): 0.637278457948037\nEpoch 28/200\n21561/21561 [==============================] - 11s 521us/step - loss: 0.1529 - val_loss: 0.1321\nVal Jaccard Similarity (min_max): 0.634692632146385\nVal Jaccard Similarity (max_con): 0.6220817294216272\nEpoch 29/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1530 - val_loss: 0.1321\nVal Jaccard Similarity (min_max): 0.6360634669805375\nVal Jaccard Similarity (max_con): 0.621733568238731\nEpoch 30/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 11s 523us/step - loss: 0.1533 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.646511873058198\nVal Jaccard Similarity (max_con): 0.6338391444929203\nEpoch 31/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1525 - val_loss: 0.1321\nVal Jaccard Similarity (min_max): 0.6564103254132793\nVal Jaccard Similarity (max_con): 0.6431578457879716\nEpoch 32/200\n21561/21561 [==============================] - 12s 536us/step - loss: 0.1537 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.636838118451814\nVal Jaccard Similarity (max_con): 0.6266106977923461\nEpoch 33/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1525 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.6363187326706004\nVal Jaccard Similarity (max_con): 0.626037153433276\nEpoch 34/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1515 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6443676069737725\nVal Jaccard Similarity (max_con): 0.6333193940788927\nEpoch 35/200\n21561/21561 [==============================] - 11s 516us/step - loss: 0.1513 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6428810339611988\nVal Jaccard Similarity (max_con): 0.6305406155582962\nEpoch 36/200\n21561/21561 [==============================] - 12s 538us/step - loss: 0.1525 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.6504426813618617\nVal Jaccard Similarity (max_con): 0.6376724224997831\nEpoch 37/200\n21561/21561 [==============================] - 11s 520us/step - loss: 0.1509 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6518509000984426\nVal Jaccard Similarity (max_con): 0.6389234052847442\nEpoch 38/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1512 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6372231600649053\nVal Jaccard Similarity (max_con): 0.6265534340868878\nEpoch 39/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1521 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6527579034278082\nVal Jaccard Similarity (max_con): 0.6415250131724916\nEpoch 40/200\n21561/21561 [==============================] - 11s 510us/step - loss: 0.1520 - val_loss: 0.1348\nVal Jaccard Similarity (min_max): 0.6655696826832532\nVal Jaccard Similarity (max_con): 0.6554652170593221\nEpoch 41/200\n21561/21561 [==============================] - 11s 531us/step - loss: 0.1518 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6435262893882769\nVal Jaccard Similarity (max_con): 0.6311917213566054\nEpoch 42/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1512 - val_loss: 0.1304\nVal Jaccard Similarity (min_max): 0.6512894559877305\nVal Jaccard Similarity (max_con): 0.6391567275446998\nEpoch 43/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1521 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.655178294486514\nVal Jaccard Similarity (max_con): 0.644205701456507\nEpoch 44/200\n21561/21561 [==============================] - 11s 516us/step - loss: 0.1517 - val_loss: 0.1337\nVal Jaccard Similarity (min_max): 0.6691735037766473\nVal Jaccard Similarity (max_con): 0.6579608387271153\nEpoch 45/200\n21561/21561 [==============================] - 12s 541us/step - loss: 0.1505 - val_loss: 0.1336\nVal Jaccard Similarity (min_max): 0.6676410086379204\nVal Jaccard Similarity (max_con): 0.6544744084494452\nEpoch 46/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1496 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6635196536217897\nVal Jaccard Similarity (max_con): 0.6515902222324028\nEpoch 47/200\n21561/21561 [==============================] - 11s 520us/step - loss: 0.1498 - val_loss: 0.1300\nVal Jaccard Similarity (min_max): 0.6540516548364382\nVal Jaccard Similarity (max_con): 0.642329724443303\nEpoch 48/200\n21561/21561 [==============================] - 12s 537us/step - loss: 0.1501 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6598538575487085\nVal Jaccard Similarity (max_con): 0.6495465159618229\nEpoch 49/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1503 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.6650632651970203\nVal Jaccard Similarity (max_con): 0.6527358273431387\nEpoch 50/200\n21561/21561 [==============================] - 12s 537us/step - loss: 0.1493 - val_loss: 0.1297\nVal Jaccard Similarity (min_max): 0.661449706880015\nVal Jaccard Similarity (max_con): 0.648781650881491\nEpoch 51/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1493 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6573382255252035\nVal Jaccard Similarity (max_con): 0.642942272741164\nEpoch 52/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1499 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6638438370460567\nVal Jaccard Similarity (max_con): 0.6548631663040675\nEpoch 53/200\n21561/21561 [==============================] - 16s 728us/step - loss: 0.1497 - val_loss: 0.1294\nVal Jaccard Similarity (min_max): 0.6557209151364616\nVal Jaccard Similarity (max_con): 0.6434002325089019\nEpoch 54/200\n21561/21561 [==============================] - 12s 544us/step - loss: 0.1494 - val_loss: 0.1296\nVal Jaccard Similarity (min_max): 0.6644843659917778\nVal Jaccard Similarity (max_con): 0.6513459294031904\nEpoch 55/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1487 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6657877496734645\nVal Jaccard Similarity (max_con): 0.6532722600737764\nEpoch 56/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1499 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6552958108159957\nVal Jaccard Similarity (max_con): 0.6415839586910079\nEpoch 57/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1497 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6575522055801246\nVal Jaccard Similarity (max_con): 0.6453890208192194\nEpoch 58/200\n21561/21561 [==============================] - 11s 521us/step - loss: 0.1498 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6652198684166675\nVal Jaccard Similarity (max_con): 0.6542771307669624\nEpoch 59/200\n21561/21561 [==============================] - 11s 523us/step - loss: 0.1493 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6717179966880881\nVal Jaccard Similarity (max_con): 0.6626630530448773\nEpoch 60/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1490 - val_loss: 0.1288\nVal Jaccard Similarity (min_max): 0.6684845044289335\nVal Jaccard Similarity (max_con): 0.6550383773495275\nEpoch 61/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1500 - val_loss: 0.1289\nVal Jaccard Similarity (min_max): 0.6698793278776924\nVal Jaccard Similarity (max_con): 0.6589829655804196\nEpoch 62/200\n21561/21561 [==============================] - 11s 518us/step - loss: 0.1484 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6692134200578557\nVal Jaccard Similarity (max_con): 0.6579121049262506\nEpoch 63/200\n21561/21561 [==============================] - 12s 538us/step - loss: 0.1481 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6661536624221804\nVal Jaccard Similarity (max_con): 0.6542683052212779\nEpoch 64/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1487 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6678744320871794\nVal Jaccard Similarity (max_con): 0.6580130941478658\nEpoch 65/200\n21561/21561 [==============================] - 11s 505us/step - loss: 0.1490 - val_loss: 0.1302\nVal Jaccard Similarity (min_max): 0.671033715778814\nVal Jaccard Similarity (max_con): 0.6631878627281478\nEpoch 66/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1488 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6718054450673977\nVal Jaccard Similarity (max_con): 0.6635510613594593\nEpoch 67/200\n21561/21561 [==============================] - 11s 525us/step - loss: 0.1483 - val_loss: 0.1302\nVal Jaccard Similarity (min_max): 0.6713244609151275\nVal Jaccard Similarity (max_con): 0.6630893893308973\nEpoch 68/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 11s 529us/step - loss: 0.1472 - val_loss: 0.1291\nVal Jaccard Similarity (min_max): 0.6720212635965314\nVal Jaccard Similarity (max_con): 0.6598558480700073\nEpoch 69/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1493 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6718129504004594\nVal Jaccard Similarity (max_con): 0.6596569935932761\nEpoch 70/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1479 - val_loss: 0.1294\nVal Jaccard Similarity (min_max): 0.6711264661101918\nVal Jaccard Similarity (max_con): 0.6601568739057176\nEpoch 71/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1471 - val_loss: 0.1291\nVal Jaccard Similarity (min_max): 0.6672147381749353\nVal Jaccard Similarity (max_con): 0.6563333804867292\nEpoch 72/200\n21561/21561 [==============================] - 12s 547us/step - loss: 0.1476 - val_loss: 0.1290\nVal Jaccard Similarity (min_max): 0.6712377506436686\nVal Jaccard Similarity (max_con): 0.6610932935893943\nEpoch 73/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1482 - val_loss: 0.1294\nVal Jaccard Similarity (min_max): 0.6683823610385006\nVal Jaccard Similarity (max_con): 0.656325466660802\nEpoch 74/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1483 - val_loss: 0.1287\nVal Jaccard Similarity (min_max): 0.6621227822241748\nVal Jaccard Similarity (max_con): 0.6480401229224225\nEpoch 75/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1477 - val_loss: 0.1289\nVal Jaccard Similarity (min_max): 0.6691602319916937\nVal Jaccard Similarity (max_con): 0.6598125700450481\nEpoch 76/200\n21561/21561 [==============================] - 11s 533us/step - loss: 0.1484 - val_loss: 0.1297\nVal Jaccard Similarity (min_max): 0.6699687478633992\nVal Jaccard Similarity (max_con): 0.6628406085883882\nEpoch 77/200\n21561/21561 [==============================] - 11s 529us/step - loss: 0.1479 - val_loss: 0.1288\nVal Jaccard Similarity (min_max): 0.6665171701987541\nVal Jaccard Similarity (max_con): 0.6537661003957641\nEpoch 78/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1480 - val_loss: 0.1298\nVal Jaccard Similarity (min_max): 0.6742206885551848\nVal Jaccard Similarity (max_con): 0.6675209563497281\nEpoch 79/200\n21561/21561 [==============================] - 11s 518us/step - loss: 0.1464 - val_loss: 0.1290\nVal Jaccard Similarity (min_max): 0.673068440326412\nVal Jaccard Similarity (max_con): 0.662089401737796\nEpoch 80/200\n21561/21561 [==============================] - 11s 505us/step - loss: 0.1478 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6712324152839446\nVal Jaccard Similarity (max_con): 0.662407497185472\nEpoch 81/200\n21561/21561 [==============================] - 12s 534us/step - loss: 0.1472 - val_loss: 0.1296\nVal Jaccard Similarity (min_max): 0.6692564382332766\nVal Jaccard Similarity (max_con): 0.6614204429429392\nEpoch 82/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1479 - val_loss: 0.1317\nVal Jaccard Similarity (min_max): 0.6713440271831045\nVal Jaccard Similarity (max_con): 0.6643169125931406\nEpoch 83/200\n21561/21561 [==============================] - 11s 505us/step - loss: 0.1471 - val_loss: 0.1288\nVal Jaccard Similarity (min_max): 0.6714897838320449\nVal Jaccard Similarity (max_con): 0.6618561809500079\nEpoch 84/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1470 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6732040685847819\nVal Jaccard Similarity (max_con): 0.6609791721573857\nEpoch 85/200\n21561/21561 [==============================] - 11s 519us/step - loss: 0.1464 - val_loss: 0.1295\nVal Jaccard Similarity (min_max): 0.67347180291603\nVal Jaccard Similarity (max_con): 0.6653412851500788\nEpoch 86/200\n21561/21561 [==============================] - 11s 530us/step - loss: 0.1477 - val_loss: 0.1297\nVal Jaccard Similarity (min_max): 0.6767577918873651\nVal Jaccard Similarity (max_con): 0.6694398998103209\nEpoch 87/200\n21561/21561 [==============================] - 11s 510us/step - loss: 0.1465 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.6727009442781203\nVal Jaccard Similarity (max_con): 0.6648842167933088\nEpoch 88/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1467 - val_loss: 0.1295\nVal Jaccard Similarity (min_max): 0.6746300359653681\nVal Jaccard Similarity (max_con): 0.6670631277828616\nEpoch 89/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1473 - val_loss: 0.1283\nVal Jaccard Similarity (min_max): 0.667687216637\nVal Jaccard Similarity (max_con): 0.6569261855441649\nEpoch 90/200\n21561/21561 [==============================] - 12s 537us/step - loss: 0.1469 - val_loss: 0.1290\nVal Jaccard Similarity (min_max): 0.6684420809941994\nVal Jaccard Similarity (max_con): 0.657852973657178\nEpoch 91/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1457 - val_loss: 0.1297\nVal Jaccard Similarity (min_max): 0.6649727857185371\nVal Jaccard Similarity (max_con): 0.6531113699092334\nEpoch 92/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1461 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6726608582035528\nVal Jaccard Similarity (max_con): 0.6662458718263339\nEpoch 93/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1460 - val_loss: 0.1316\nVal Jaccard Similarity (min_max): 0.673996049396045\nVal Jaccard Similarity (max_con): 0.6656365616422414\nEpoch 94/200\n21561/21561 [==============================] - 11s 525us/step - loss: 0.1472 - val_loss: 0.1284\nVal Jaccard Similarity (min_max): 0.6718053235703265\nVal Jaccard Similarity (max_con): 0.6615715055915341\nEpoch 95/200\n21561/21561 [==============================] - 11s 523us/step - loss: 0.1461 - val_loss: 0.1284\nVal Jaccard Similarity (min_max): 0.670045506342841\nVal Jaccard Similarity (max_con): 0.658588974221341\nEpoch 96/200\n21561/21561 [==============================] - 11s 505us/step - loss: 0.1471 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6761863661805242\nVal Jaccard Similarity (max_con): 0.6660873019287811\nEpoch 97/200\n21561/21561 [==============================] - 11s 519us/step - loss: 0.1473 - val_loss: 0.1291\nVal Jaccard Similarity (min_max): 0.6738938924659448\nVal Jaccard Similarity (max_con): 0.6616346423645924\nEpoch 98/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1457 - val_loss: 0.1287\nVal Jaccard Similarity (min_max): 0.6740273700794295\nVal Jaccard Similarity (max_con): 0.6609762089341363\nEpoch 99/200\n21561/21561 [==============================] - 16s 746us/step - loss: 0.1464 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.6745227944925087\nVal Jaccard Similarity (max_con): 0.6648676812123886\nEpoch 100/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1462 - val_loss: 0.1294\nVal Jaccard Similarity (min_max): 0.6694567036437408\nVal Jaccard Similarity (max_con): 0.6630185089189776\nEpoch 101/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1448 - val_loss: 0.1296\nVal Jaccard Similarity (min_max): 0.6749734830333812\nVal Jaccard Similarity (max_con): 0.6648401091093313\nEpoch 102/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1456 - val_loss: 0.1291\nVal Jaccard Similarity (min_max): 0.6732190961707147\nVal Jaccard Similarity (max_con): 0.6643418659838715\nEpoch 103/200\n21561/21561 [==============================] - 11s 524us/step - loss: 0.1460 - val_loss: 0.1297\nVal Jaccard Similarity (min_max): 0.6752687107027804\nVal Jaccard Similarity (max_con): 0.6664783973825881\nEpoch 104/200\n21561/21561 [==============================] - 11s 519us/step - loss: 0.1450 - val_loss: 0.1300\nVal Jaccard Similarity (min_max): 0.6776430598587957\nVal Jaccard Similarity (max_con): 0.6667410856429329\nEpoch 105/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1455 - val_loss: 0.1323\nVal Jaccard Similarity (min_max): 0.6779128410247716\nVal Jaccard Similarity (max_con): 0.6687268716003323\nEpoch 106/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 11s 511us/step - loss: 0.1458 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6709395701752333\nVal Jaccard Similarity (max_con): 0.6603161697542057\nEpoch 107/200\n21561/21561 [==============================] - 11s 517us/step - loss: 0.1471 - val_loss: 0.1299\nVal Jaccard Similarity (min_max): 0.6760311432387912\nVal Jaccard Similarity (max_con): 0.6676341516364879\nEpoch 108/200\n21561/21561 [==============================] - 11s 527us/step - loss: 0.1459 - val_loss: 0.1290\nVal Jaccard Similarity (min_max): 0.6762085128727864\nVal Jaccard Similarity (max_con): 0.6657549263809303\nEpoch 109/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1454 - val_loss: 0.1295\nVal Jaccard Similarity (min_max): 0.6743331428475724\nVal Jaccard Similarity (max_con): 0.6650239791939716\nEpoch 110/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1453 - val_loss: 0.1336\nVal Jaccard Similarity (min_max): 0.6805746221678789\nVal Jaccard Similarity (max_con): 0.6742560053050829\nEpoch 111/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1453 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6718107366060547\nVal Jaccard Similarity (max_con): 0.6618901863521252\nEpoch 112/200\n21561/21561 [==============================] - 11s 519us/step - loss: 0.1451 - val_loss: 0.1288\nVal Jaccard Similarity (min_max): 0.6783197919160308\nVal Jaccard Similarity (max_con): 0.6670472388191222\nEpoch 113/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1458 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6738973762467457\nVal Jaccard Similarity (max_con): 0.6656755082215258\nEpoch 114/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1448 - val_loss: 0.1289\nVal Jaccard Similarity (min_max): 0.6715047717798013\nVal Jaccard Similarity (max_con): 0.6597215644323782\nEpoch 115/200\n21561/21561 [==============================] - 11s 519us/step - loss: 0.1445 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6651084423240463\nVal Jaccard Similarity (max_con): 0.6559722691142787\nEpoch 116/200\n21561/21561 [==============================] - 11s 527us/step - loss: 0.1455 - val_loss: 0.1328\nVal Jaccard Similarity (min_max): 0.6780361533392355\nVal Jaccard Similarity (max_con): 0.6687533931224934\nEpoch 117/200\n21561/21561 [==============================] - 12s 542us/step - loss: 0.1448 - val_loss: 0.1288\nVal Jaccard Similarity (min_max): 0.669991114379732\nVal Jaccard Similarity (max_con): 0.6586101928568668\nEpoch 118/200\n21561/21561 [==============================] - 11s 507us/step - loss: 0.1452 - val_loss: 0.1286\nVal Jaccard Similarity (min_max): 0.6769109267217781\nVal Jaccard Similarity (max_con): 0.6642129135376074\nEpoch 119/200\n21561/21561 [==============================] - 11s 519us/step - loss: 0.1455 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6802953593249037\nVal Jaccard Similarity (max_con): 0.6723820896483683\nEpoch 120/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1446 - val_loss: 0.1291\nVal Jaccard Similarity (min_max): 0.6758878060970133\nVal Jaccard Similarity (max_con): 0.6662216152379168\nEpoch 121/200\n21561/21561 [==============================] - 11s 533us/step - loss: 0.1448 - val_loss: 0.1290\nVal Jaccard Similarity (min_max): 0.6727121994795525\nVal Jaccard Similarity (max_con): 0.6615235563921398\nEpoch 122/200\n21561/21561 [==============================] - 11s 522us/step - loss: 0.1454 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.679217644256218\nVal Jaccard Similarity (max_con): 0.6677449817050243\nEpoch 123/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1445 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6748093043550347\nVal Jaccard Similarity (max_con): 0.6632137789870272\nEpoch 124/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1448 - val_loss: 0.1290\nVal Jaccard Similarity (min_max): 0.6729622229255532\nVal Jaccard Similarity (max_con): 0.6614952641410808\nEpoch 125/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1449 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.680626438282643\nVal Jaccard Similarity (max_con): 0.6705342332190425\nEpoch 126/200\n21561/21561 [==============================] - 12s 550us/step - loss: 0.1448 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6700186548667043\nVal Jaccard Similarity (max_con): 0.6594134480092326\nEpoch 127/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1439 - val_loss: 0.1291\nVal Jaccard Similarity (min_max): 0.6755906241390622\nVal Jaccard Similarity (max_con): 0.6633132292329444\nEpoch 128/200\n21561/21561 [==============================] - 11s 505us/step - loss: 0.1442 - val_loss: 0.1298\nVal Jaccard Similarity (min_max): 0.6775479152871753\nVal Jaccard Similarity (max_con): 0.6699469315918872\nEpoch 129/200\n21561/21561 [==============================] - 11s 507us/step - loss: 0.1467 - val_loss: 0.1299\nVal Jaccard Similarity (min_max): 0.6781112714491814\nVal Jaccard Similarity (max_con): 0.6684892308045924\nEpoch 130/200\n21561/21561 [==============================] - 11s 523us/step - loss: 0.1437 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.6769901703922903\nVal Jaccard Similarity (max_con): 0.6670919033095531\nEpoch 131/200\n21561/21561 [==============================] - 11s 528us/step - loss: 0.1442 - val_loss: 0.1294\nVal Jaccard Similarity (min_max): 0.6709531533052289\nVal Jaccard Similarity (max_con): 0.6618694660891863\nEpoch 132/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1444 - val_loss: 0.1291\nVal Jaccard Similarity (min_max): 0.6732073832365665\nVal Jaccard Similarity (max_con): 0.6649523550651144\nEpoch 133/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1441 - val_loss: 0.1292\nVal Jaccard Similarity (min_max): 0.6769010994584963\nVal Jaccard Similarity (max_con): 0.665389800971826\nEpoch 134/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1445 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.6799803987346537\nVal Jaccard Similarity (max_con): 0.668218232413065\nEpoch 135/200\n21561/21561 [==============================] - 12s 540us/step - loss: 0.1443 - val_loss: 0.1298\nVal Jaccard Similarity (min_max): 0.6740971824624588\nVal Jaccard Similarity (max_con): 0.6635320867256004\nEpoch 136/200\n21561/21561 [==============================] - 11s 518us/step - loss: 0.1446 - val_loss: 0.1300\nVal Jaccard Similarity (min_max): 0.6797472227494477\nVal Jaccard Similarity (max_con): 0.6678737148318187\nEpoch 137/200\n21561/21561 [==============================] - 11s 510us/step - loss: 0.1432 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6797439170741757\nVal Jaccard Similarity (max_con): 0.6695127305794191\nEpoch 138/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1445 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6712539081671618\nVal Jaccard Similarity (max_con): 0.660564583794719\nEpoch 139/200\n21561/21561 [==============================] - 11s 525us/step - loss: 0.1445 - val_loss: 0.1295\nVal Jaccard Similarity (min_max): 0.6646402659562203\nVal Jaccard Similarity (max_con): 0.658648213220496\nEpoch 140/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1448 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6686277188551294\nVal Jaccard Similarity (max_con): 0.6562936785430133\nEpoch 141/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1441 - val_loss: 0.1297\nVal Jaccard Similarity (min_max): 0.6809824022247919\nVal Jaccard Similarity (max_con): 0.6657891008107774\nEpoch 142/200\n21561/21561 [==============================] - 11s 507us/step - loss: 0.1439 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.6779597605843113\nVal Jaccard Similarity (max_con): 0.6661233627724907\nEpoch 143/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1439 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6744768510375395\nVal Jaccard Similarity (max_con): 0.6660029045286635\nEpoch 144/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 13s 597us/step - loss: 0.1423 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6684419050483369\nVal Jaccard Similarity (max_con): 0.6592006167581083\nEpoch 145/200\n21561/21561 [==============================] - 14s 634us/step - loss: 0.1435 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6739790359572941\nVal Jaccard Similarity (max_con): 0.6645658649064417\nEpoch 146/200\n21561/21561 [==============================] - 11s 518us/step - loss: 0.1424 - val_loss: 0.1300\nVal Jaccard Similarity (min_max): 0.6736616703247419\nVal Jaccard Similarity (max_con): 0.6627213939301647\nEpoch 147/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1430 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6797850052414686\nVal Jaccard Similarity (max_con): 0.6670833732015576\nEpoch 148/200\n21561/21561 [==============================] - 12s 545us/step - loss: 0.1440 - val_loss: 0.1304\nVal Jaccard Similarity (min_max): 0.6784090580842821\nVal Jaccard Similarity (max_con): 0.666271520852585\nEpoch 149/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1435 - val_loss: 0.1298\nVal Jaccard Similarity (min_max): 0.6726457273743953\nVal Jaccard Similarity (max_con): 0.6601189838885104\nEpoch 150/200\n21561/21561 [==============================] - 11s 507us/step - loss: 0.1437 - val_loss: 0.1300\nVal Jaccard Similarity (min_max): 0.671593402619147\nVal Jaccard Similarity (max_con): 0.6615076043458495\nEpoch 151/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1422 - val_loss: 0.1298\nVal Jaccard Similarity (min_max): 0.6750495496946678\nVal Jaccard Similarity (max_con): 0.6665770746831102\nEpoch 152/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1423 - val_loss: 0.1320\nVal Jaccard Similarity (min_max): 0.6823282358485584\nVal Jaccard Similarity (max_con): 0.6736715000367991\nEpoch 153/200\n21561/21561 [==============================] - 12s 543us/step - loss: 0.1441 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6781388270237299\nVal Jaccard Similarity (max_con): 0.6682175255740703\nEpoch 154/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1445 - val_loss: 0.1294\nVal Jaccard Similarity (min_max): 0.6742951932590002\nVal Jaccard Similarity (max_con): 0.6647317990577348\nEpoch 155/200\n21561/21561 [==============================] - 11s 507us/step - loss: 0.1433 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6796527430139275\nVal Jaccard Similarity (max_con): 0.6697694969749305\nEpoch 156/200\n21561/21561 [==============================] - 11s 510us/step - loss: 0.1430 - val_loss: 0.1300\nVal Jaccard Similarity (min_max): 0.6759350145532875\nVal Jaccard Similarity (max_con): 0.6640104608321987\nEpoch 157/200\n21561/21561 [==============================] - 12s 539us/step - loss: 0.1433 - val_loss: 0.1328\nVal Jaccard Similarity (min_max): 0.6821133667655175\nVal Jaccard Similarity (max_con): 0.6712712668795312\nEpoch 158/200\n21561/21561 [==============================] - 11s 518us/step - loss: 0.1440 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.682097719904374\nVal Jaccard Similarity (max_con): 0.6707074123106497\nEpoch 159/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1419 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.6804603475346402\nVal Jaccard Similarity (max_con): 0.6674178736545439\nEpoch 160/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1433 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6804144734370846\nVal Jaccard Similarity (max_con): 0.6701341657044098\nEpoch 161/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1433 - val_loss: 0.1299\nVal Jaccard Similarity (min_max): 0.6778377380172289\nVal Jaccard Similarity (max_con): 0.6682910737180714\nEpoch 162/200\n21561/21561 [==============================] - 11s 531us/step - loss: 0.1434 - val_loss: 0.1304\nVal Jaccard Similarity (min_max): 0.6745219464895875\nVal Jaccard Similarity (max_con): 0.6632901315196199\nEpoch 163/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1436 - val_loss: 0.1328\nVal Jaccard Similarity (min_max): 0.6785064756531239\nVal Jaccard Similarity (max_con): 0.6687976680952123\nEpoch 164/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1443 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6776812662101248\nVal Jaccard Similarity (max_con): 0.6663264511375\nEpoch 165/200\n21561/21561 [==============================] - 11s 507us/step - loss: 0.1429 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6805307137429492\nVal Jaccard Similarity (max_con): 0.6696491564479345\nEpoch 166/200\n21561/21561 [==============================] - 12s 544us/step - loss: 0.1420 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.673982128870811\nVal Jaccard Similarity (max_con): 0.663723139937518\nEpoch 167/200\n21561/21561 [==============================] - 11s 511us/step - loss: 0.1433 - val_loss: 0.1320\nVal Jaccard Similarity (min_max): 0.6806820683618342\nVal Jaccard Similarity (max_con): 0.6702958238637613\nEpoch 168/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1424 - val_loss: 0.1297\nVal Jaccard Similarity (min_max): 0.6672051745975904\nVal Jaccard Similarity (max_con): 0.6597727530242463\nEpoch 169/200\n21561/21561 [==============================] - 11s 528us/step - loss: 0.1439 - val_loss: 0.1296\nVal Jaccard Similarity (min_max): 0.6703810985326019\nVal Jaccard Similarity (max_con): 0.6599529933709503\nEpoch 170/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1431 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6798900637264729\nVal Jaccard Similarity (max_con): 0.6703561361785568\nEpoch 171/200\n21561/21561 [==============================] - 12s 535us/step - loss: 0.1416 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6764956028848508\nVal Jaccard Similarity (max_con): 0.6655430894553283\nEpoch 172/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1429 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.6755115903823244\nVal Jaccard Similarity (max_con): 0.6659638802591848\nEpoch 173/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1424 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6781913754710598\nVal Jaccard Similarity (max_con): 0.6668207061275372\nEpoch 174/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1424 - val_loss: 0.1304\nVal Jaccard Similarity (min_max): 0.6669807845599682\nVal Jaccard Similarity (max_con): 0.6585618693828293\nEpoch 175/200\n21561/21561 [==============================] - 12s 534us/step - loss: 0.1438 - val_loss: 0.1304\nVal Jaccard Similarity (min_max): 0.6766135531752997\nVal Jaccard Similarity (max_con): 0.6656543465149783\nEpoch 176/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1437 - val_loss: 0.1299\nVal Jaccard Similarity (min_max): 0.680103427312115\nVal Jaccard Similarity (max_con): 0.6685463983693724\nEpoch 177/200\n21561/21561 [==============================] - 11s 509us/step - loss: 0.1416 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.6788117639834587\nVal Jaccard Similarity (max_con): 0.6666423726712937\nEpoch 178/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1433 - val_loss: 0.1322\nVal Jaccard Similarity (min_max): 0.6827128912583379\nVal Jaccard Similarity (max_con): 0.670777459927464\nEpoch 179/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1426 - val_loss: 0.1298\nVal Jaccard Similarity (min_max): 0.6737368189205696\nVal Jaccard Similarity (max_con): 0.6632082107165831\nEpoch 180/200\n21561/21561 [==============================] - 12s 536us/step - loss: 0.1424 - val_loss: 0.1321\nVal Jaccard Similarity (min_max): 0.6818160929463947\nVal Jaccard Similarity (max_con): 0.6696830763790469\nEpoch 181/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1428 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6774835823518014\nVal Jaccard Similarity (max_con): 0.6663418427075565\nEpoch 182/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 11s 508us/step - loss: 0.1423 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6797915670142605\nVal Jaccard Similarity (max_con): 0.6699707014969868\nEpoch 183/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1430 - val_loss: 0.1358\nVal Jaccard Similarity (min_max): 0.6805810031533864\nVal Jaccard Similarity (max_con): 0.6741390931985577\nEpoch 184/200\n21561/21561 [==============================] - 12s 572us/step - loss: 0.1421 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.6787083334444827\nVal Jaccard Similarity (max_con): 0.6677811349021833\nEpoch 185/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1407 - val_loss: 0.1302\nVal Jaccard Similarity (min_max): 0.6766151623173279\nVal Jaccard Similarity (max_con): 0.664034095915537\nEpoch 186/200\n21561/21561 [==============================] - 11s 508us/step - loss: 0.1431 - val_loss: 0.1293\nVal Jaccard Similarity (min_max): 0.6729055424022229\nVal Jaccard Similarity (max_con): 0.6613991386297443\nEpoch 187/200\n21561/21561 [==============================] - 11s 504us/step - loss: 0.1428 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6807498140573374\nVal Jaccard Similarity (max_con): 0.6703982553415913\nEpoch 188/200\n21561/21561 [==============================] - 11s 517us/step - loss: 0.1425 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.679561833141406\nVal Jaccard Similarity (max_con): 0.6697595070064483\nEpoch 189/200\n21561/21561 [==============================] - 11s 530us/step - loss: 0.1402 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6792297349291779\nVal Jaccard Similarity (max_con): 0.6695092678953393\nEpoch 190/200\n21561/21561 [==============================] - 14s 662us/step - loss: 0.1419 - val_loss: 0.1303\nVal Jaccard Similarity (min_max): 0.6746020954244805\nVal Jaccard Similarity (max_con): 0.6625735612737782\nEpoch 191/200\n21561/21561 [==============================] - 12s 548us/step - loss: 0.1410 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.6756935346214616\nVal Jaccard Similarity (max_con): 0.6678911335112232\nEpoch 192/200\n21561/21561 [==============================] - 11s 522us/step - loss: 0.1419 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6783187785057289\nVal Jaccard Similarity (max_con): 0.6668691609361004\nEpoch 193/200\n21561/21561 [==============================] - 11s 533us/step - loss: 0.1411 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6731703086483455\nVal Jaccard Similarity (max_con): 0.6636541684058576\nEpoch 194/200\n21561/21561 [==============================] - 11s 506us/step - loss: 0.1417 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6748901285058252\nVal Jaccard Similarity (max_con): 0.6674659819269774\nEpoch 195/200\n21561/21561 [==============================] - 11s 518us/step - loss: 0.1422 - val_loss: 0.1323\nVal Jaccard Similarity (min_max): 0.6790315839463749\nVal Jaccard Similarity (max_con): 0.6676802502386913\nEpoch 196/200\n21561/21561 [==============================] - 11s 503us/step - loss: 0.1413 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6796840107399181\nVal Jaccard Similarity (max_con): 0.6683469414659301\nEpoch 197/200\n21561/21561 [==============================] - 11s 530us/step - loss: 0.1423 - val_loss: 0.1302\nVal Jaccard Similarity (min_max): 0.6778669205640576\nVal Jaccard Similarity (max_con): 0.6671834487489766\nEpoch 198/200\n21561/21561 [==============================] - 11s 515us/step - loss: 0.1428 - val_loss: 0.1321\nVal Jaccard Similarity (min_max): 0.676828250285998\nVal Jaccard Similarity (max_con): 0.6685735049228231\nEpoch 199/200\n21561/21561 [==============================] - 11s 513us/step - loss: 0.1422 - val_loss: 0.1300\nVal Jaccard Similarity (min_max): 0.6746329801122334\nVal Jaccard Similarity (max_con): 0.6660753889169775\nEpoch 200/200\n21561/21561 [==============================] - 11s 512us/step - loss: 0.1429 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6780120156801807\nVal Jaccard Similarity (max_con): 0.6670532154948031\n","name":"stdout"},{"output_type":"execute_result","execution_count":198,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f2199128950>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Epoch 98/100\n21561/21561 [==============================] - 8s 357us/step - loss: 26.4123 - val_loss: 32.8301\nVal Jaccard Similarity: 0.5266293328859964\n\nEpoch 99/100\n21561/21561 [==============================] - 8s 356us/step - loss: 26.0379 - val_loss: 32.5830\nVal Jaccard Similarity: 0.5592650588400309\n\nEpoch 100/100\n21561/21561 [==============================] - 8s 352us/step - loss: 25.8462 - val_loss: 32.7917\nVal Jaccard Similarity: 0.5504962474510444"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2.fit([train_manip_X, train_numerics_repeated], train_manip_Y, validation_data=([test_manip_X, test_numerics_repeated], test_manip_Y), batch_size=1000, epochs=200, callbacks=[metrics_continuous_span])","execution_count":null,"outputs":[{"output_type":"stream","text":"Train on 21561 samples, validate on 3804 samples\nEpoch 1/200\n21561/21561 [==============================] - 9s 396us/step - loss: 0.1415 - val_loss: 0.1301\nVal Jaccard Similarity (min_max): 0.6743554051905798\nVal Jaccard Similarity (max_con): 0.6643664700180647\nEpoch 2/200\n21561/21561 [==============================] - 8s 394us/step - loss: 0.1402 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.67802883279624\nVal Jaccard Similarity (max_con): 0.6684265132922482\nEpoch 3/200\n21561/21561 [==============================] - 8s 373us/step - loss: 0.1409 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6783518547600192\nVal Jaccard Similarity (max_con): 0.6689032508290057\nEpoch 4/200\n21561/21561 [==============================] - 8s 355us/step - loss: 0.1406 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6773932001678056\nVal Jaccard Similarity (max_con): 0.6685068957267316\nEpoch 5/200\n21561/21561 [==============================] - 8s 354us/step - loss: 0.1402 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6780128794666795\nVal Jaccard Similarity (max_con): 0.6690194271627652\nEpoch 6/200\n21561/21561 [==============================] - 8s 359us/step - loss: 0.1405 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6767660796453511\nVal Jaccard Similarity (max_con): 0.6683739212381616\nEpoch 7/200\n21561/21561 [==============================] - 8s 354us/step - loss: 0.1404 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.675993567457224\nVal Jaccard Similarity (max_con): 0.6680052427149089\nEpoch 8/200\n21561/21561 [==============================] - 8s 374us/step - loss: 0.1396 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6758826160421681\nVal Jaccard Similarity (max_con): 0.6681932590319939\nEpoch 9/200\n21561/21561 [==============================] - 8s 357us/step - loss: 0.1409 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6781435653038425\nVal Jaccard Similarity (max_con): 0.6689822554638384\nEpoch 10/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1408 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6795054799293286\nVal Jaccard Similarity (max_con): 0.6686638284214442\nEpoch 11/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1405 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6787050811328293\nVal Jaccard Similarity (max_con): 0.6689020116064925\nEpoch 12/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1395 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.676107739654701\nVal Jaccard Similarity (max_con): 0.6695922596719032\nEpoch 13/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1405 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6790153561764629\nVal Jaccard Similarity (max_con): 0.6689510267000428\nEpoch 14/200\n21561/21561 [==============================] - 8s 362us/step - loss: 0.1405 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6791601003530172\nVal Jaccard Similarity (max_con): 0.6692201006682559\nEpoch 15/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1406 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6796044454432233\nVal Jaccard Similarity (max_con): 0.6697946013965785\nEpoch 16/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1400 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6793235149874607\nVal Jaccard Similarity (max_con): 0.6688606021655488\nEpoch 17/200\n21561/21561 [==============================] - 7s 343us/step - loss: 0.1405 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6797119273373129\nVal Jaccard Similarity (max_con): 0.669364239994088\nEpoch 18/200\n21561/21561 [==============================] - 7s 342us/step - loss: 0.1400 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6754928502810948\nVal Jaccard Similarity (max_con): 0.6673171029200343\nEpoch 19/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1405 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.681239399100779\nVal Jaccard Similarity (max_con): 0.6717241808667499\nEpoch 20/200\n21561/21561 [==============================] - 8s 368us/step - loss: 0.1403 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6792186471015647\nVal Jaccard Similarity (max_con): 0.670509439285975\nEpoch 21/200\n21561/21561 [==============================] - 8s 369us/step - loss: 0.1411 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6792624244551922\nVal Jaccard Similarity (max_con): 0.6707180799790089\nEpoch 22/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1405 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6780929847706513\nVal Jaccard Similarity (max_con): 0.6705401998522871\nEpoch 23/200\n21561/21561 [==============================] - 8s 355us/step - loss: 0.1390 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6813194609375809\nVal Jaccard Similarity (max_con): 0.6721305300370894\nEpoch 24/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1404 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6791302478414548\nVal Jaccard Similarity (max_con): 0.6710846994038223\nEpoch 25/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1402 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6802641144277083\nVal Jaccard Similarity (max_con): 0.6716025782225254\nEpoch 26/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1406 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6790412918279767\nVal Jaccard Similarity (max_con): 0.671960855488346\nEpoch 27/200\n21561/21561 [==============================] - 8s 362us/step - loss: 0.1410 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6780593253446033\nVal Jaccard Similarity (max_con): 0.670303021724482\nEpoch 28/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1392 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6794381505053342\nVal Jaccard Similarity (max_con): 0.670846122011273\nEpoch 29/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1392 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6788726064249794\nVal Jaccard Similarity (max_con): 0.6710303810554417\nEpoch 30/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1390 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6788553868578887\nVal Jaccard Similarity (max_con): 0.6710328167157089\nEpoch 31/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1382 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6813966644366712\nVal Jaccard Similarity (max_con): 0.6708337305607116\nEpoch 32/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1393 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6792163087533529\nVal Jaccard Similarity (max_con): 0.6703696835174022\nEpoch 33/200\n21561/21561 [==============================] - 8s 371us/step - loss: 0.1408 - val_loss: 0.1323\nVal Jaccard Similarity (min_max): 0.6810517668464959\nVal Jaccard Similarity (max_con): 0.6718401929400853\nEpoch 34/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1403 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6801581560796176\nVal Jaccard Similarity (max_con): 0.671569719671621\nEpoch 35/200\n21561/21561 [==============================] - 7s 343us/step - loss: 0.1398 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6779642726304238\nVal Jaccard Similarity (max_con): 0.6687829424638664\nEpoch 36/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1396 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.680172945606673\nVal Jaccard Similarity (max_con): 0.6703449459629642\nEpoch 37/200\n21561/21561 [==============================] - 8s 348us/step - loss: 0.1389 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6787208237035921\nVal Jaccard Similarity (max_con): 0.671314543578229\nEpoch 38/200\n21561/21561 [==============================] - 7s 343us/step - loss: 0.1400 - val_loss: 0.1316\nVal Jaccard Similarity (min_max): 0.6823003001821452\nVal Jaccard Similarity (max_con): 0.6712053038509519\nEpoch 39/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 8s 374us/step - loss: 0.1395 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6805614149705306\nVal Jaccard Similarity (max_con): 0.6721356002920762\nEpoch 40/200\n21561/21561 [==============================] - 8s 351us/step - loss: 0.1406 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.680963378066963\nVal Jaccard Similarity (max_con): 0.6723138288575291\nEpoch 41/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1392 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6797231404075753\nVal Jaccard Similarity (max_con): 0.6714707208614238\nEpoch 42/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1387 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6796524548810761\nVal Jaccard Similarity (max_con): 0.6710513459370263\nEpoch 43/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1392 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6802091464480593\nVal Jaccard Similarity (max_con): 0.6710572769506569\nEpoch 44/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1393 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6796175180349204\nVal Jaccard Similarity (max_con): 0.6710260889042565\nEpoch 45/200\n21561/21561 [==============================] - 8s 358us/step - loss: 0.1402 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6746046346668455\nVal Jaccard Similarity (max_con): 0.6661745476024823\nEpoch 46/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1390 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6793015203369429\nVal Jaccard Similarity (max_con): 0.6716655080039082\nEpoch 47/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1394 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6793880550973904\nVal Jaccard Similarity (max_con): 0.6694262561653078\nEpoch 48/200\n21561/21561 [==============================] - 7s 348us/step - loss: 0.1396 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6781839529655975\nVal Jaccard Similarity (max_con): 0.6713884565968141\nEpoch 49/200\n21561/21561 [==============================] - 10s 487us/step - loss: 0.1410 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6804022856620805\nVal Jaccard Similarity (max_con): 0.6715877215377389\nEpoch 50/200\n21561/21561 [==============================] - 8s 361us/step - loss: 0.1396 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6803409936138831\nVal Jaccard Similarity (max_con): 0.6717451509206375\nEpoch 51/200\n21561/21561 [==============================] - 8s 360us/step - loss: 0.1406 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6804044410381823\nVal Jaccard Similarity (max_con): 0.6724757912400228\nEpoch 52/200\n21561/21561 [==============================] - 8s 359us/step - loss: 0.1401 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6822884032903958\nVal Jaccard Similarity (max_con): 0.6716851491182335\nEpoch 53/200\n21561/21561 [==============================] - 8s 351us/step - loss: 0.1394 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6779633099362372\nVal Jaccard Similarity (max_con): 0.6693757107431254\nEpoch 54/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1402 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.678881753540287\nVal Jaccard Similarity (max_con): 0.6698095130634459\nEpoch 55/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1406 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6783746340961617\nVal Jaccard Similarity (max_con): 0.668828412745177\nEpoch 56/200\n21561/21561 [==============================] - 7s 348us/step - loss: 0.1404 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6787384594477569\nVal Jaccard Similarity (max_con): 0.6696577954038423\nEpoch 57/200\n21561/21561 [==============================] - 8s 371us/step - loss: 0.1406 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6811545970693811\nVal Jaccard Similarity (max_con): 0.6715959864970251\nEpoch 58/200\n21561/21561 [==============================] - 8s 355us/step - loss: 0.1381 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6796875923905772\nVal Jaccard Similarity (max_con): 0.670924412009576\nEpoch 59/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1394 - val_loss: 0.1307\nVal Jaccard Similarity (min_max): 0.6778934890597701\nVal Jaccard Similarity (max_con): 0.6678652827266364\nEpoch 60/200\n21561/21561 [==============================] - 8s 352us/step - loss: 0.1398 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6790980936796307\nVal Jaccard Similarity (max_con): 0.6704774986880565\nEpoch 61/200\n21561/21561 [==============================] - 7s 341us/step - loss: 0.1404 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.6807893800486783\nVal Jaccard Similarity (max_con): 0.6712577908584001\nEpoch 62/200\n21561/21561 [==============================] - 7s 343us/step - loss: 0.1393 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6788043562711806\nVal Jaccard Similarity (max_con): 0.6707534808843856\nEpoch 63/200\n21561/21561 [==============================] - 8s 352us/step - loss: 0.1398 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6792413949530662\nVal Jaccard Similarity (max_con): 0.6709049585172215\nEpoch 64/200\n21561/21561 [==============================] - 8s 363us/step - loss: 0.1391 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6767298396208289\nVal Jaccard Similarity (max_con): 0.6691567082505466\nEpoch 65/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1391 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6772748932065551\nVal Jaccard Similarity (max_con): 0.6685486692441134\nEpoch 66/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1395 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6757561190639658\nVal Jaccard Similarity (max_con): 0.6673052046257548\nEpoch 67/200\n21561/21561 [==============================] - 8s 351us/step - loss: 0.1388 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6802456718060201\nVal Jaccard Similarity (max_con): 0.6717263109570023\nEpoch 68/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1394 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6765456694046551\nVal Jaccard Similarity (max_con): 0.6680525526461786\nEpoch 69/200\n21561/21561 [==============================] - 7s 348us/step - loss: 0.1395 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6780658835068464\nVal Jaccard Similarity (max_con): 0.6676855402917846\nEpoch 70/200\n21561/21561 [==============================] - 8s 366us/step - loss: 0.1399 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.677195526784962\nVal Jaccard Similarity (max_con): 0.6697859260660893\nEpoch 71/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1393 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6788265906214751\nVal Jaccard Similarity (max_con): 0.6706105636341295\nEpoch 72/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1383 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6797722570354504\nVal Jaccard Similarity (max_con): 0.6720302770918276\nEpoch 73/200\n21561/21561 [==============================] - 7s 348us/step - loss: 0.1393 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6775653117402148\nVal Jaccard Similarity (max_con): 0.668897100836319\nEpoch 74/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1404 - val_loss: 0.1305\nVal Jaccard Similarity (min_max): 0.6770267785805592\nVal Jaccard Similarity (max_con): 0.6682617020508752\nEpoch 75/200\n21561/21561 [==============================] - 7s 343us/step - loss: 0.1403 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6775648470512643\nVal Jaccard Similarity (max_con): 0.6682866666483489\nEpoch 76/200\n21561/21561 [==============================] - 8s 362us/step - loss: 0.1401 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6800171811395581\nVal Jaccard Similarity (max_con): 0.6703660348750716\nEpoch 77/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 8s 352us/step - loss: 0.1396 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6800638644691653\nVal Jaccard Similarity (max_con): 0.6710767765288648\nEpoch 78/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1394 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6816998815101002\nVal Jaccard Similarity (max_con): 0.671495506529113\nEpoch 79/200\n21561/21561 [==============================] - 7s 343us/step - loss: 0.1399 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6813746594439845\nVal Jaccard Similarity (max_con): 0.6710469577848462\nEpoch 80/200\n21561/21561 [==============================] - 7s 348us/step - loss: 0.1403 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.67737544241109\nVal Jaccard Similarity (max_con): 0.6686268503013361\nEpoch 81/200\n21561/21561 [==============================] - 8s 348us/step - loss: 0.1402 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6799629323334349\nVal Jaccard Similarity (max_con): 0.670497827607227\nEpoch 82/200\n21561/21561 [==============================] - 8s 371us/step - loss: 0.1395 - val_loss: 0.1304\nVal Jaccard Similarity (min_max): 0.6762398342457507\nVal Jaccard Similarity (max_con): 0.6675733399295016\nEpoch 83/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1391 - val_loss: 0.1310\nVal Jaccard Similarity (min_max): 0.6799921162747787\nVal Jaccard Similarity (max_con): 0.6707336544276048\nEpoch 84/200\n21561/21561 [==============================] - 8s 361us/step - loss: 0.1387 - val_loss: 0.1306\nVal Jaccard Similarity (min_max): 0.6768597376486137\nVal Jaccard Similarity (max_con): 0.6661162779502747\nEpoch 85/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1405 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6787809438898705\nVal Jaccard Similarity (max_con): 0.6707088566543473\nEpoch 86/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1399 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6816263384551793\nVal Jaccard Similarity (max_con): 0.67276258147289\nEpoch 87/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1393 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6815581792060682\nVal Jaccard Similarity (max_con): 0.6706929476294683\nEpoch 88/200\n21561/21561 [==============================] - 8s 360us/step - loss: 0.1389 - val_loss: 0.1317\nVal Jaccard Similarity (min_max): 0.6807541886727859\nVal Jaccard Similarity (max_con): 0.6716715701306728\nEpoch 89/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1391 - val_loss: 0.1317\nVal Jaccard Similarity (min_max): 0.680717320743053\nVal Jaccard Similarity (max_con): 0.6727025968294524\nEpoch 90/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1397 - val_loss: 0.1317\nVal Jaccard Similarity (min_max): 0.6830601957482861\nVal Jaccard Similarity (max_con): 0.6727500372787898\nEpoch 91/200\n21561/21561 [==============================] - 8s 348us/step - loss: 0.1390 - val_loss: 0.1323\nVal Jaccard Similarity (min_max): 0.682979902294305\nVal Jaccard Similarity (max_con): 0.6747990404371979\nEpoch 92/200\n21561/21561 [==============================] - 8s 351us/step - loss: 0.1394 - val_loss: 0.1316\nVal Jaccard Similarity (min_max): 0.6816222953489599\nVal Jaccard Similarity (max_con): 0.6724605487215337\nEpoch 93/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1399 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6812326728533298\nVal Jaccard Similarity (max_con): 0.6716050923506386\nEpoch 94/200\n21561/21561 [==============================] - 8s 363us/step - loss: 0.1388 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.6821515432260883\nVal Jaccard Similarity (max_con): 0.6727636992301465\nEpoch 95/200\n21561/21561 [==============================] - 8s 363us/step - loss: 0.1384 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6819576866186438\nVal Jaccard Similarity (max_con): 0.672617541254901\nEpoch 96/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1393 - val_loss: 0.1318\nVal Jaccard Similarity (min_max): 0.6814695006054536\nVal Jaccard Similarity (max_con): 0.6732206478170574\nEpoch 97/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1388 - val_loss: 0.1318\nVal Jaccard Similarity (min_max): 0.681733854780861\nVal Jaccard Similarity (max_con): 0.6744472615766068\nEpoch 98/200\n21561/21561 [==============================] - 8s 348us/step - loss: 0.1384 - val_loss: 0.1320\nVal Jaccard Similarity (min_max): 0.6803201873542741\nVal Jaccard Similarity (max_con): 0.6736538714046069\nEpoch 99/200\n21561/21561 [==============================] - 7s 342us/step - loss: 0.1378 - val_loss: 0.1322\nVal Jaccard Similarity (min_max): 0.6799422656109253\nVal Jaccard Similarity (max_con): 0.6725259735809809\nEpoch 100/200\n21561/21561 [==============================] - 8s 355us/step - loss: 0.1391 - val_loss: 0.1317\nVal Jaccard Similarity (min_max): 0.6812133825915814\nVal Jaccard Similarity (max_con): 0.6722293262439382\nEpoch 101/200\n21561/21561 [==============================] - 8s 367us/step - loss: 0.1396 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.6822437816135927\nVal Jaccard Similarity (max_con): 0.67282927194279\nEpoch 102/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1396 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6807125641024019\nVal Jaccard Similarity (max_con): 0.671977248277028\nEpoch 103/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1396 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6769709219241626\nVal Jaccard Similarity (max_con): 0.6661843135306169\nEpoch 104/200\n21561/21561 [==============================] - 7s 348us/step - loss: 0.1399 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6816380993264152\nVal Jaccard Similarity (max_con): 0.6714789996043449\nEpoch 105/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1397 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6802324626526032\nVal Jaccard Similarity (max_con): 0.6712897909840366\nEpoch 106/200\n21561/21561 [==============================] - 8s 350us/step - loss: 0.1385 - val_loss: 0.1320\nVal Jaccard Similarity (min_max): 0.682224232002703\nVal Jaccard Similarity (max_con): 0.6725767384037729\nEpoch 107/200\n21561/21561 [==============================] - 8s 362us/step - loss: 0.1394 - val_loss: 0.1323\nVal Jaccard Similarity (min_max): 0.6809841203953538\nVal Jaccard Similarity (max_con): 0.6732074424229159\nEpoch 108/200\n21561/21561 [==============================] - 8s 354us/step - loss: 0.1393 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6799832529819722\nVal Jaccard Similarity (max_con): 0.6691319860953091\nEpoch 109/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1391 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.682203666062585\nVal Jaccard Similarity (max_con): 0.6737997694022675\nEpoch 110/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1392 - val_loss: 0.1325\nVal Jaccard Similarity (min_max): 0.6800651841537404\nVal Jaccard Similarity (max_con): 0.6732285075330189\nEpoch 111/200\n21561/21561 [==============================] - 8s 351us/step - loss: 0.1397 - val_loss: 0.1321\nVal Jaccard Similarity (min_max): 0.6811681664955751\nVal Jaccard Similarity (max_con): 0.672062740428225\nEpoch 112/200\n21561/21561 [==============================] - 11s 514us/step - loss: 0.1391 - val_loss: 0.1314\nVal Jaccard Similarity (min_max): 0.6824525763021024\nVal Jaccard Similarity (max_con): 0.672106829261954\nEpoch 113/200\n21561/21561 [==============================] - 8s 363us/step - loss: 0.1397 - val_loss: 0.1320\nVal Jaccard Similarity (min_max): 0.6813687866217748\nVal Jaccard Similarity (max_con): 0.6720877557403502\nEpoch 114/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1386 - val_loss: 0.1324\nVal Jaccard Similarity (min_max): 0.6816575976044027\nVal Jaccard Similarity (max_con): 0.6729198935182767\nEpoch 115/200\n","name":"stdout"},{"output_type":"stream","text":"21561/21561 [==============================] - 7s 348us/step - loss: 0.1393 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6806186487749013\nVal Jaccard Similarity (max_con): 0.6695265927266697\nEpoch 116/200\n21561/21561 [==============================] - 8s 358us/step - loss: 0.1391 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.675686189500844\nVal Jaccard Similarity (max_con): 0.6658109757006183\nEpoch 117/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1387 - val_loss: 0.1320\nVal Jaccard Similarity (min_max): 0.6826571535373332\nVal Jaccard Similarity (max_con): 0.6743402804552623\nEpoch 118/200\n21561/21561 [==============================] - 8s 352us/step - loss: 0.1402 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6820062261361359\nVal Jaccard Similarity (max_con): 0.6729870734420484\nEpoch 119/200\n21561/21561 [==============================] - 8s 361us/step - loss: 0.1400 - val_loss: 0.1316\nVal Jaccard Similarity (min_max): 0.6803651283920421\nVal Jaccard Similarity (max_con): 0.6720412151360708\nEpoch 120/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1388 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.6811117314471469\nVal Jaccard Similarity (max_con): 0.673636883846653\nEpoch 121/200\n21561/21561 [==============================] - 8s 351us/step - loss: 0.1390 - val_loss: 0.1308\nVal Jaccard Similarity (min_max): 0.6748200535757345\nVal Jaccard Similarity (max_con): 0.6643047071814405\nEpoch 122/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1386 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6775223555039046\nVal Jaccard Similarity (max_con): 0.6672476331776626\nEpoch 123/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1400 - val_loss: 0.1309\nVal Jaccard Similarity (min_max): 0.6720308181216165\nVal Jaccard Similarity (max_con): 0.6628692509389132\nEpoch 124/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1384 - val_loss: 0.1312\nVal Jaccard Similarity (min_max): 0.6767590017826471\nVal Jaccard Similarity (max_con): 0.6678988719549609\nEpoch 125/200\n21561/21561 [==============================] - 8s 369us/step - loss: 0.1388 - val_loss: 0.1327\nVal Jaccard Similarity (min_max): 0.6811521055571655\nVal Jaccard Similarity (max_con): 0.6722759880368673\nEpoch 126/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1386 - val_loss: 0.1322\nVal Jaccard Similarity (min_max): 0.6814158612097312\nVal Jaccard Similarity (max_con): 0.6712756465517052\nEpoch 127/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1402 - val_loss: 0.1313\nVal Jaccard Similarity (min_max): 0.6789564103255556\nVal Jaccard Similarity (max_con): 0.6696995568996135\nEpoch 128/200\n21561/21561 [==============================] - 8s 351us/step - loss: 0.1384 - val_loss: 0.1322\nVal Jaccard Similarity (min_max): 0.6807965351629752\nVal Jaccard Similarity (max_con): 0.6711016811525656\nEpoch 129/200\n21561/21561 [==============================] - 8s 349us/step - loss: 0.1387 - val_loss: 0.1317\nVal Jaccard Similarity (min_max): 0.6801142928656768\nVal Jaccard Similarity (max_con): 0.6705767584862833\nEpoch 130/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1388 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.676792795081384\nVal Jaccard Similarity (max_con): 0.6681163397716952\nEpoch 131/200\n21561/21561 [==============================] - 8s 370us/step - loss: 0.1385 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.676875810027522\nVal Jaccard Similarity (max_con): 0.6689660223850338\nEpoch 132/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1388 - val_loss: 0.1311\nVal Jaccard Similarity (min_max): 0.6772837044341253\nVal Jaccard Similarity (max_con): 0.6665385736456529\nEpoch 133/200\n21561/21561 [==============================] - 7s 345us/step - loss: 0.1388 - val_loss: 0.1318\nVal Jaccard Similarity (min_max): 0.6801689313047045\nVal Jaccard Similarity (max_con): 0.6700979429334841\nEpoch 134/200\n21561/21561 [==============================] - 7s 344us/step - loss: 0.1392 - val_loss: 0.1315\nVal Jaccard Similarity (min_max): 0.6802376004179914\nVal Jaccard Similarity (max_con): 0.6701001331837857\nEpoch 135/200\n21561/21561 [==============================] - 7s 346us/step - loss: 0.1387 - val_loss: 0.1316\nVal Jaccard Similarity (min_max): 0.6806959472707601\nVal Jaccard Similarity (max_con): 0.6698016970435091\nEpoch 136/200\n21561/21561 [==============================] - 7s 347us/step - loss: 0.1391 - val_loss: 0.1317\nVal Jaccard Similarity (min_max): 0.6800331248735407\nVal Jaccard Similarity (max_con): 0.6714643764961634\nEpoch 137/200\n21561/21561 [==============================] - 8s 359us/step - loss: 0.1385 - val_loss: 0.1323\nVal Jaccard Similarity (min_max): 0.6809552062399825\nVal Jaccard Similarity (max_con): 0.6714737498933426\nEpoch 138/200\n21561/21561 [==============================] - 8s 356us/step - loss: 0.1393 - val_loss: 0.1319\nVal Jaccard Similarity (min_max): 0.6826867491839177\nVal Jaccard Similarity (max_con): 0.6727007998685699\nEpoch 139/200\n19000/21561 [=========================>....] - ETA: 0s - loss: 0.1377","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_2.fit([train_manip_X, train_manip_X_sentiment_one_hot_repeated], train_manip_Y, validation_data=([test_manip_X, test_manip_X_sentiment_one_hot_repeated], test_manip_Y), batch_size=5000, epochs=200, callbacks=[metrics])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_2.fit([train_manip_X, train_manip_X_sentiment_one_hot_repeated], train_manip_Y, validation_data=([test_manip_X, test_manip_X_sentiment_one_hot_repeated], test_manip_Y), batch_size=2000, epochs=200, callbacks=[metrics])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model_2.predict([test_manip_X])\n\n# # test_X_df = df_objs.df_objects['train'].iloc[idx2].reset_index(drop=True)\n# test_df_1 = pd.concat([test_df, pd.DataFrame(preds, columns=['pred_start', 'pred_end'])], axis=1)\n# test_df_1['tokens_indices'] = test_df_1['trans_text'].apply(handlers.get_token_indices)\n# print((test_df_1['tokens_indices'].apply(len) == test_df_1['tokens'].apply(len)).value_counts())\n\n# # Get the predictions\n# test_df_1['out_pred_span'] = test_df_1.apply(handlers.get_pred_text_span, axis = 1)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_manip_Y[0]\n# test_manip_Y[1].shape\n# preds.reshape((preds.shape[0], preds.shape[1]))\n# [:,1].shape\n\n# preds","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"array([ True, False,  True,  True,  True,  True,  True, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 100\nprint(preds.shape)\nprint(test_df.iloc[i]['start_ind'])\nprint(test_df.iloc[i]['end_ind'])\nprint(preds[:,:,1][i]>0.5)\nprint(len(test_df.iloc[i]['tokens']))\n# print(np.around(preds[i], decimals=2))\n\n# Two Ways:\n# 1) Min to Max\n# 2) Max Continuous span","execution_count":65,"outputs":[{"output_type":"stream","text":"(3804, 40, 2)\n0\n9\n[False  True False  True  True  True  True  True  True  True False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False]\n10\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# get_pos_indices_continuous_span(preds, 'get_min_max_span_indices')).sum()\n    \n        \n# #         i+=1\n      \n    \n# # get_min_max_span_indices(preds[0])    \n# # print(test_df.iloc[22])\n# # print(test_manip_Y[22])\n# # print(get_max_continuous_span_indices(preds[22]))\n# for i in range(30, 50):\n#     print(get_max_continuous_span_indices(preds[i]))\n#     print()\n# vstack([np.array([1,2]), None])","execution_count":164,"outputs":[{"output_type":"execute_result","execution_count":164,"data":{"text/plain":"array([[ 0,  6],\n       [ 0, 17],\n       [ 7, 10],\n       ...,\n       [-1, -1],\n       [ 1,  7],\n       [ 3,  3]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.5\ntemp_out = pd.concat([\n    handlers.get_preds_out(\n        handlers.get_pos_indices_continuous_span(preds, 'get_min_max_span_indices'), \n#         handlers.get_pos_indices_continuous_span(preds, 'get_max_continuous_span_indices'), \n        test_df), \n    test_df['selected_text']],\n    axis=1\n)\ntemp_out.columns = ['predicted_text', 'selected_text']\n# temp_out['predicted_text']\nprint(temp_out.apply(lambda x: handlers.jaccard(x['predicted_text'], x['selected_text']), axis=1).mean())\n\ntemp_out = pd.concat([\n    handlers.get_preds_out(\n#         handlers.get_pos_indices_continuous_span(preds, 'get_min_max_span_indices'), \n        handlers.get_pos_indices_continuous_span(preds, 'get_max_continuous_span_indices'), \n        test_df), \n    test_df['selected_text']],\n    axis=1\n)\ntemp_out.columns = ['predicted_text', 'selected_text']\n# temp_out['predicted_text']\nprint(temp_out.apply(lambda x: handlers.jaccard(x['predicted_text'], x['selected_text']), axis=1).mean())\n","execution_count":189,"outputs":[{"output_type":"stream","text":"0.568592198881803\n0.5277566266345529\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- Check if embedding matrix is correctly created"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Overall Jaccard from Model:')\n# print(test_df_1[['text', 'selected_text', 'out_pred_span']].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\n# print('Overall Jaccard baseline from predicting the complete text:')\n# print(test_df_1[['text', 'selected_text', 'out_pred_span']].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\n# print('Overall Jaccard using model for NOT neutral and baseline for neutral:')\n# print(pd.concat([\n#     test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1),\n#     test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1)\n# ]).mean())\n\n\n# print('Model Jaccard for neutral')\n# print(test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\n# print('Baseline Jaccard for neutral from predicting complete text')\n# print(test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\n# print('Model Jaccard for NOT neutral')\n# print(test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\n# print('Baseline Jaccard for NOT neutral from predicting complete text')\n# print(test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df_1.apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df_1[['text', 'selected_text', 'out_pred_span']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df_1['selected_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}