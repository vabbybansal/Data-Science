{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n/kaggle/input/tweet-sentiment-extraction/train.csv\n/kaggle/input/glove-twitter/glove.twitter.27B.100d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.200d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.50d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.25d.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load the competetion datasets\n\nclass KaggleReader(object):\n    def __init__(self):\n        self.df_objects = {}        \n    def read_kaggle_df(self, df_name, df_path):\n        self.df_objects[df_name] = pd.read_csv(df_path)\n        print()\n        print(df_name + ' loaded')\n        print(\"Shape => \" + str(self.df_objects[df_name].shape))\n    \ndf_objs = KaggleReader()\ndf_objs.read_kaggle_df('train', \"../input/tweet-sentiment-extraction/train.csv\")\ndf_objs.read_kaggle_df('test', \"../input/tweet-sentiment-extraction/test.csv\")\ndf_objs.read_kaggle_df('submission', \"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":2,"outputs":[{"output_type":"stream","text":"\ntrain loaded\nShape => (27481, 4)\n\ntest loaded\nShape => (3534, 3)\n\nsubmission loaded\nShape => (3534, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom numpy import hstack, vstack\n# Tokenize text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.utils import to_categorical\nfrom nltk.tokenize import WordPunctTokenizer\n\n# Model\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, LSTM, Embedding, Dropout, Input, Bidirectional, TimeDistributed, BatchNormalization\nfrom keras.initializers import Constant\nfrom keras.layers.merge import concatenate\nimport keras\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Handlers(object):\n    def __init__(self):\n        self.run_tests()\n        pass\n    \n    def draw_pr_curve_plt(self, Y_valid, y_pred, x_range=1.0):\n    #     (precision, recall, x_range=1.0):\n\n            precision, recall, thresholds_pr = precision_recall_curve(Y_valid, y_pred)\n\n            # import dependencies\n            import matplotlib.pyplot as plt\n\n            plt.step(recall, precision, color='b', alpha=0.2,\n                     where='post')\n            plt.fill_between(recall, precision, alpha=0.2, color='b')\n            plt.xlabel('Recall')\n            plt.ylabel('Precision')\n            plt.ylim([0.0, 1.05])\n            plt.xlim([0.0, x_range])\n            plt.show()\n     \n    def draw_roc_curve_plt(self, Y_valid, y_pred):\n#     (fpr, tpr, auc):\n        # import dependencies\n        import matplotlib.pyplot as plt\n        fpr, tpr, thresholds_roc = roc_curve(Y_valid, y_pred)\n        auc_roc = auc(fpr, tpr)\n        \n        plt.figure(1)\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.plot(fpr, tpr, label='(area = {:.3f})'.format(auc_roc))\n        # plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n        plt.xlabel('False positive rate')\n        plt.ylabel('True positive rate')\n        plt.title('ROC curve')\n        plt.legend(loc='best')\n        plt.show()\n\n    # Optimize using KMP\n    def return_start_end_indices(self, big, small):\n        if len(small) <= 0 or len(big) <= 0:\n            return (-1,-1)\n        i = 0\n        j = 0\n        started = False\n        while i < len(big):\n            if big[i] == small[j]:\n                if started == False:\n                    started = True\n                    i_start = i\n\n                if j == len(small) - 1:\n                    return (i-len(small)+1, i)\n                j += 1\n\n            else:\n                if started == True:\n                    started = False\n                    i = i_start + 1\n                j = 0\n            i += 1\n        return (-1,-1)\n    \n    def run_tests(self):\n        \n        assert self.return_start_end_indices(['i', '`', 'd', 'have', 'responded', ',', 'if', 'i', 'were', 'going'], ['have', 'responded', ',']) == (3,5)\n        assert self.return_start_end_indices('abc', '') == (-1,-1)\n        assert self.return_start_end_indices('abc', 'a') == (0,0)\n        assert self.return_start_end_indices('abc', 'b') == (1,1)\n        assert self.return_start_end_indices('abc', 'c') == (2,2)\n        assert self.return_start_end_indices('abc', 'ab') == (0,1)\n        assert self.return_start_end_indices('abc', 'bc') == (1,2)\n        assert self.return_start_end_indices('abc', 'ac') == (-1,-1)\n        assert self.return_start_end_indices('abc', 'abc') == (0,2)\n        assert self.return_start_end_indices('abcabcabc', 'abc') == (0,2)\n        assert self.return_start_end_indices('ababcc', 'abc') == (2,4)\n    \n    def jaccard(self, str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    \n    def load_gloVe_embeddings(self):\n        # Boilerplate taken from here - https://www.kaggle.com/stacykurnikova/using-glove-embedding\n        embeddings_index = {}\n        f = open('/kaggle/input/glove-twitter/glove.twitter.27B.25d.txt')\n        for line in f:\n            values = line.split(' ')\n            word = values[0] ## The first entry is the word\n            coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n            embeddings_index[word] = coefs\n        f.close()\n        print('GloVe data loaded')\n        return embeddings_index\n    \n    def load_embeddings_matrix(self, embeddings_index, index_tokenizer):\n        # https://www.kaggle.com/stacykurnikova/using-glove-embedding\n        # Create an embedding matrix with embedding vectors for the tokens recognized in the vocab of tweets\n        \n        EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n        # num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n        num_words = len(index_tokenizer.word_index) + 1\n\n        # To Do: constrain the vocab size\n        embedding_matrix = np.random.uniform(-1,+1,(num_words, EMBEDDING_DIM))\n        count_in_embedding_vocab = 0\n        for word, i in index_tokenizer.word_index.items():\n            embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n            if embedding_vector is not None:\n                # words not found in embedding index will be all-zeros.\n                embedding_matrix[i] = embedding_vector\n                count_in_embedding_vocab += 1\n#         TO DO:\n#             else:\n#                 PASS EMBEDDINGS OF SYNONYMS (!!! == !)\n        return embedding_matrix\n        # print(count_in_embedding_vocab)\n        # print(len(embedding_matrix))\n    \n    def get_token_indices(self, x):\n        span_generator = WordPunctTokenizer().span_tokenize(x)\n        spans = [span for span in span_generator]\n        return spans\n    \n    # TO DO: compensate for end and start on top of padding\n    def get_pred_text_span(self, x):\n        start_token_index = int(x['pred_start'])\n        end_token_index = int(x['pred_end'])\n        token_indices_list = x['tokens_indices']\n        text = x['text']\n\n        # start token ind > end token ind (could change logic to also have either 0:end)\n        if start_token_index > end_token_index:\n            return text\n\n        # start token ind and end token ind within bounds\n        elif start_token_index < len(token_indices_list) and end_token_index < len(token_indices_list):\n            return text[token_indices_list[start_token_index][0]: token_indices_list[end_token_index][1]]\n\n\n        # start token ind after bounds (could change logic to also have either 0:end or sth similar)\n        elif start_token_index >= len(token_indices_list):\n            return text\n\n        # only end token ind out of bounds\n        elif start_token_index < len(token_indices_list) and end_token_index >= len(token_indices_list):\n            return text[token_indices_list[start_token_index][0]: len(text)-1]\n        \n    def get_preds_out(self, preds_indexes, test_df):\n        temp_df = pd.concat([test_df, pd.DataFrame(preds_indexes, columns=['pred_start', 'pred_end'])], axis=1)\n        temp_df['tokens_indices'] = temp_df['trans_text'].apply(handlers.get_token_indices)\n        # print((temp_df['tokens_indices'].apply(len) == temp_df['tokens'].apply(len)).value_counts())\n\n        # Get the predictions\n        return temp_df.apply(handlers.get_pred_text_span, axis = 1)\n    \n    def get_indexes_from_argmax(self, preds, test_df):\n        start_preds = preds[0]\n        end_preds = preds[1]\n        start_preds = start_preds.reshape((start_preds.shape[0], start_preds.shape[1]))\n        end_preds = end_preds.reshape((end_preds.shape[0], end_preds.shape[1]))\n        start_inds = start_preds.argmax(axis=1)\n        end_inds = end_preds.argmax(axis=1)\n        return vstack([start_inds, end_inds]).transpose()\n        \n        \n    def batch_jaccard(self, preds, test_df):\n        temp_df = pd.concat([test_df, pd.DataFrame(preds, columns=['pred_start', 'pred_end'])], axis=1)\n        temp_df['tokens_indices'] = temp_df['trans_text'].apply(handlers.get_token_indices)\n        # print((temp_df['tokens_indices'].apply(len) == temp_df['tokens'].apply(len)).value_counts())\n\n        # Get the predictions\n        temp_df['out_pred_span'] = temp_df.apply(handlers.get_pred_text_span, axis = 1)\n        return temp_df.apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean()\n    \n    \n    def batch_jaccard_from_argmax(self, preds, test_df):\n        start_preds = preds[0]\n        end_preds = preds[1]\n        start_preds = start_preds.reshape((start_preds.shape[0], start_preds.shape[1]))\n        end_preds = end_preds.reshape((end_preds.shape[0], end_preds.shape[1]))\n        start_inds = start_preds.argmax(axis=1)\n        end_inds = end_preds.argmax(axis=1)\n        return self.batch_jaccard(vstack([start_inds, end_inds]).transpose(), test_df)    \n\nhandlers = Handlers()\n\n\n# To add Jaccard Similarity Value on Validation set after each epoch\nclass Metrics(keras.callbacks.Callback):\n    # https://stackoverflow.com/questions/37657260/how-to-implement-custom-metric-in-keras\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, X_sentiment, y_val = self.validation_data[0], self.validation_data[1], self.validation_data[2]\n        y_predict = np.asarray(self.model.predict([X_val, X_sentiment]))\n        print('Val Jaccard Similarity: {}'.format(handlers.batch_jaccard(y_predict, test_df))) \n        return\n\n    def get_data(self):\n        return self._data\n    \nclass MetricsCategorical(keras.callbacks.Callback):\n    # https://stackoverflow.com/questions/37657260/how-to-implement-custom-metric-in-keras\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, X_numerics, y_val = self.validation_data[0], self.validation_data[1], self.validation_data[2]\n        y_predict = np.asarray(self.model.predict([X_val, X_numerics]))\n        print('Val Jaccard Similarity: {}'.format(handlers.batch_jaccard_from_argmax(y_predict, test_df))) \n        return\n\n    def get_data(self):\n        return self._data    \n    \nmetrics = Metrics()\nmetrics_categorical = MetricsCategorical()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 40\n\nclass DataManipulationPipeline(object):\n    def __init__(self):\n        # Load dependencies\n        try:\n            self.handlers = handlers\n        except NameError:\n            self.handlers = Handlers()\n            \n        # init vars\n        self.vars = {\n            'MAX_SEQUENCE_LENGTH' : MAX_SEQUENCE_LENGTH\n        }\n\n    def pre_fit(self, X):\n        \n        # Strip the text\n        X['text'] = X['text'].str.strip()\n        # Lower case the text\n        X['trans_text'] = X['text'].apply(str.lower)\n        return X\n\n    def create_sequences(self, X):\n        # Tokenize the word tokens to word_indexes\n        sequences = self.vars['keras_index_tokenizer'].texts_to_sequences(X['tokens'])\n        # Pad the sequences to be fed to NN [Note that this will effectively change the start, end index if padded on post]\n        sequences_padded = pad_sequences(sequences, maxlen=self.vars['MAX_SEQUENCE_LENGTH'], padding='post', truncating='post')        \n\n        return sequences_padded\n    \n    def create_Y(self, X):\n        X['trans_selected_text'] = X['selected_text'].apply(str.lower)\n        X = self.punct_tokenize(X, 'trans_selected_text', 'tokens_selected_text', self.vars['punct_tokenizer'])\n        X['start_end_indices'] = X.apply(lambda x: handlers.return_start_end_indices(x['tokens'], x['tokens_selected_text']), axis=1)\n        # truncate the start end indices to end of MAX LEN of sequence\n        X['start_end_indices'] = X['start_end_indices'].apply(lambda x: (x[0], (MAX_SEQUENCE_LENGTH-1) if x[1]>= MAX_SEQUENCE_LENGTH else x[1]))\n        # if start is ahead of end, put stamp\n        X['start_end_indices'] = X['start_end_indices'].apply(lambda x: (x[0],x[1]) if x[0] <= x[1] else (-1,-1))\n        \n        X = X[(X['start_end_indices'] != (-1,-1))]\n        X = X.reset_index(drop=True)\n        X['start_ind'] = X['start_end_indices'].apply(lambda x: x[0])\n        X['end_ind'] = X['start_end_indices'].apply(lambda x: x[1])\n        Y = hstack(\n            (\n                X['start_ind'].values.reshape(X.shape[0],1),\n                X['end_ind'].values.reshape(X.shape[0],1)\n            )\n        )\n        return Y,X\n    \n    def create_Y_Categorical(self, Y):\n        Y_categorical = np.array([[0]*MAX_SEQUENCE_LENGTH for j in range(Y.shape[0])])\n        for i in range(Y.shape[0]):\n            for j in range(Y[i][0], Y[i][1]+1):\n                Y_categorical[i][j] = 1\n        \n        return keras.utils.to_categorical(Y_categorical, 2)\n    \n    def create_Y_Categorical_two_outs(self, Y):\n        Y_categorical_start = np.array([[0]*MAX_SEQUENCE_LENGTH for j in range(Y.shape[0])])\n        for i in range(Y.shape[0]): # for each example\n            # print(Y[i])\n            Y_categorical_start[i][Y[i][0]] = 1\n        Y_categorical_start = Y_categorical_start.reshape(Y_categorical_start.shape[0], Y_categorical_start.shape[1], 1)\n\n        Y_categorical_end = np.array([[0]*MAX_SEQUENCE_LENGTH for j in range(Y.shape[0])])\n        for i in range(Y.shape[0]): # for each example\n            Y_categorical_end[i][Y[i][1]] = 1\n        Y_categorical_end = Y_categorical_end.reshape(Y_categorical_end.shape[0], Y_categorical_end.shape[1], 1)            \n\n#         return (keras.utils.to_categorical(Y_categorical_start, 2), keras.utils.to_categorical(Y_categorical_end, 2))\n        return (Y_categorical_start, Y_categorical_end)\n        \n    \n    def handle_sentiment_feature_eng(self, X):\n        sentiment_transform = self.vars['sentiment_one_hot'].transform(X['sentiment'].values.reshape((X['sentiment'].shape[0],1)))\n        # Copy over each data row for MAX_SEQUENCE_LENGTH times to send it inside each LSTM sequence\n#         sentiment_transform_repeated = np.array([([sentiment_transform[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n        \n        # To Do: Scale?\n        return sentiment_transform\n    \n    def text_stats(self, X):\n        X['char_len'] = X['trans_text'].apply(len)\n        X['word_len'] = X['tokens'].apply(len)\n        X['char_word_ratio'] = X['char_len']/X['word_len']\n            \n        text_stats_singular = X[['char_len', 'word_len', 'char_word_ratio']].values\n        \n        # Standardize\n#         text_stats_singular        \n\n#         text_stats_repeated = np.array([([text_stats_singular[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n        return text_stats_singular\n\n    def repeater(self, X):\n        return np.array([([X[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n    \n        \n    \n    # Fits the parameters on train\n    def fit_transform(self, X):\n        \n        # {x}\n        # Remove nulls from train\n        X = X.dropna()\n        \n        X = self.pre_fit(X)\n        \n        # Create word tokens from sentences using NTLK\n        self.vars['punct_tokenizer'] = WordPunctTokenizer()\n        # put sth else inseatd of X foe expanded vocab?\n        X = self.punct_tokenize(X, 'trans_text', 'tokens', self.vars['punct_tokenizer'])\n\n        # Fit Index-Tokenizer the vocab using Keras\n        self.vars['keras_index_tokenizer'] = Tokenizer()\n        # Use all words for extended voacb\n        self.vars['keras_index_tokenizer'].fit_on_texts(\n            pd.concat(\n            [\n                df_objs.df_objects['train']['text'].dropna().str.strip().apply(str.lower),\n                # To Do: Make an option statement on test in case not available\n                # Increase Vocab\n                df_objs.df_objects['test']['text'].dropna().str.strip().apply(str.lower)\n            ]\n        ).reset_index(drop=True).apply(WordPunctTokenizer().tokenize)\n        )\n        # len(keras_tokenizer.word_index)\n        \n        # load glove embeddings\n        self.vars['embeddings_index'] = self.handlers.load_gloVe_embeddings() \n        self.vars['embeddings_matrix'] = self.handlers.load_embeddings_matrix(self.vars['embeddings_index'], self.vars['keras_index_tokenizer'])\n\n        # {y}\n        # Create label column - Word Tokenize selected_text, and create label indices\n        Y,X = self.create_Y(X)\n        Y_categorical = self.create_Y_Categorical_two_outs(Y)\n        # Run final X transform after Y since Y transform filters out some X\n        sequences_padded = self.create_sequences(X)\n        \n        # Fit one hot encoder for sentiment (Try standard scalar downstream as well?)\n        self.vars['sentiment_one_hot'] = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        self.vars['sentiment_one_hot'].fit(X['sentiment'].values.reshape((X['sentiment'].shape[0],1)))\n        # print(enc.get_feature_names())\n        X_sentiment_one_hot = self.handle_sentiment_feature_eng(X)\n        \n        # Create text stats\n        X_text_stats = self.text_stats(X)\n        self.vars['X_text_stats_standard_scaler'] = StandardScaler()\n        self.vars['X_text_stats_standard_scaler'].fit(X_text_stats)\n        X_text_stats_scaled = self.vars['X_text_stats_standard_scaler'].transform(X_text_stats)\n        X_agg_numerics_all = hstack([X_sentiment_one_hot, X_text_stats_scaled])\n        \n        X_numerics_repeated = self.repeater(X_agg_numerics_all)\n        \n        return (sequences_padded, X_numerics_repeated, Y, Y_categorical, X)\n\n\n    # Transforms using the parameters on train\n    def transform(self, X):\n        \n        X = self.pre_fit(X)\n        \n        # Create word tokens\n        X = self.punct_tokenize(X, 'trans_text', 'tokens', self.vars['punct_tokenizer'])\n        \n        Y = None\n        Y_categorical = None\n        if 'selected_text' in X.columns:\n            Y,X = self.create_Y(X)\n            Y_categorical = self.create_Y_Categorical_two_outs(Y)\n        sequences_padded = self.create_sequences(X)\n        X_sentiment_one_hot = self.handle_sentiment_feature_eng(X)        \n        # Create text stats\n        X_text_stats = self.text_stats(X)\n        X_text_stats_scaled = self.vars['X_text_stats_standard_scaler'].transform(X_text_stats)\n        \n        X_agg_numerics_all = hstack([X_sentiment_one_hot, X_text_stats_scaled])\n        X_numerics_repeated = self.repeater(X_agg_numerics_all)\n    \n        \n        return (sequences_padded, X_numerics_repeated, Y, Y_categorical, X)\n        \n        \n    def punct_tokenize(self, X,old_col, new_col, punct_tokenizer):\n        X[new_col] = X[old_col].apply(punct_tokenizer.tokenize)\n        return X","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_objs.df_objects['train'], test_size=0.15, random_state=42)\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n# X_train, X_test, Y_train, Y_test, idx1, idx2 = train_test_split(df_objs.df_objects['train'], Y, np.arange(Y.shape[0]), test_size=0.15, random_state=42)\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mainpulation_pipeline = DataManipulationPipeline()\n(train_manip_X, train_numerics_repeated, train_manip_Y, train_manip_Y_categorical, train_df) = data_mainpulation_pipeline.fit_transform(train)\n(test_manip_X, test_numerics_repeated, test_manip_Y, test_manip_Y_categorical, test_df) = data_mainpulation_pipeline.transform(test)","execution_count":8,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","name":"stderr"},{"output_type":"stream","text":"GloVe data loaded\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = len(data_mainpulation_pipeline.vars['keras_index_tokenizer'].word_index)+1\nEMBEDDING_DIM = len(data_mainpulation_pipeline.vars['embeddings_index']['a'])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\n# ----------------------------------------------------\ninputLayer_words = Input(shape=(MAX_SEQUENCE_LENGTH,))\ninputLayer_agg_numerics = Input(shape=(\n    MAX_SEQUENCE_LENGTH,\n    train_numerics_repeated.shape[2]\n))\n\n# Embedding layer for the tap names\nwordEmbeddings = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputLayer_words)\nwordEmbeddings = Embedding(\n    input_dim=VOCAB_SIZE, \n    output_dim=EMBEDDING_DIM, \n    input_length=MAX_SEQUENCE_LENGTH, \n    weights=[data_mainpulation_pipeline.vars['embeddings_matrix']],\n    trainable=False)(inputLayer_words)\n# \nmerged_Input = concatenate([wordEmbeddings, inputLayer_agg_numerics])\n# inputLayer_agg_indices\n# # LSTM\nlstm_1 = Bidirectional(LSTM(50, \n                            return_sequences = True,\n                            dropout=0.2,\n                            recurrent_dropout=0.2,\n                           ))(merged_Input)\n# b_norm_1 = BatchNormalization(5)(lstm_1)\n# input_shape=(25,(EMBEDDING_DIM))\ntd = TimeDistributed(Dense(25))(lstm_1)\n\n\n# drp1 = Dropout(0.2)(lstm_1)\n\n# # Dense\n# dense_0 = Dense(75, activation='relu')(drp1)\n# drp2 = Dropout(0.2)(dense_0)\n# dense_1 = Dense(30, activation='relu')(drp2)\noutputLayer_start = Dense(1, activation='sigmoid')(td)\noutputLayer_end = Dense(1, activation='sigmoid')(td)\n\nmodel_2 = Model(inputs=[inputLayer_words, inputLayer_agg_numerics], outputs=[outputLayer_start, outputLayer_end])\n\nmodel_2.compile(loss='binary_crossentropy', optimizer='adam')\n# model.compile(loss='mse', optimizer='adam', metrics=[jaccard])\nmodel_2.summary()\n# ----------------------------------------------------\n\nmodel_2.fit(\n    [train_manip_X, train_numerics_repeated], \n    [train_manip_Y_categorical[0], train_manip_Y_categorical[1]], \n    validation_data=(\n        [test_manip_X, test_numerics_repeated], \n        [test_manip_Y_categorical[0], test_manip_Y_categorical[1]]), \n    batch_size=100, \n    epochs=200,\n    callbacks=[metrics_categorical]\n)","execution_count":14,"outputs":[{"output_type":"stream","text":"Model: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            (None, 40)           0                                            \n__________________________________________________________________________________________________\nembedding_6 (Embedding)         (None, 40, 25)       728175      input_5[0][0]                    \n__________________________________________________________________________________________________\ninput_6 (InputLayer)            (None, 40, 6)        0                                            \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 40, 31)       0           embedding_6[0][0]                \n                                                                 input_6[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_3 (Bidirectional) (None, 40, 100)      32800       concatenate_3[0][0]              \n__________________________________________________________________________________________________\ntime_distributed_3 (TimeDistrib (None, 40, 25)       2525        bidirectional_3[0][0]            \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 40, 1)        26          time_distributed_3[0][0]         \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 40, 1)        26          time_distributed_3[0][0]         \n==================================================================================================\nTotal params: 763,552\nTrainable params: 35,377\nNon-trainable params: 728,175\n__________________________________________________________________________________________________\nTrain on 21560 samples, validate on 3804 samples\nEpoch 1/200\n21560/21560 [==============================] - 14s 665us/step - loss: 0.1956 - dense_8_loss: 0.0892 - dense_9_loss: 0.1063 - val_loss: 0.1212 - val_dense_8_loss: 0.0615 - val_dense_9_loss: 0.0606\nVal Jaccard Similarity: 0.6144262821610191\nEpoch 2/200\n21560/21560 [==============================] - 13s 604us/step - loss: 0.1151 - dense_8_loss: 0.0586 - dense_9_loss: 0.0565 - val_loss: 0.0982 - val_dense_8_loss: 0.0510 - val_dense_9_loss: 0.0483\nVal Jaccard Similarity: 0.622708368768121\nEpoch 3/200\n21560/21560 [==============================] - 13s 601us/step - loss: 0.1017 - dense_8_loss: 0.0520 - dense_9_loss: 0.0496 - val_loss: 0.0896 - val_dense_8_loss: 0.0464 - val_dense_9_loss: 0.0444\nVal Jaccard Similarity: 0.6310992594558728\nEpoch 4/200\n21560/21560 [==============================] - 13s 598us/step - loss: 0.0943 - dense_8_loss: 0.0478 - dense_9_loss: 0.0465 - val_loss: 0.0839 - val_dense_8_loss: 0.0428 - val_dense_9_loss: 0.0422\nVal Jaccard Similarity: 0.6458187157475217\nEpoch 5/200\n21560/21560 [==============================] - 13s 588us/step - loss: 0.0898 - dense_8_loss: 0.0450 - dense_9_loss: 0.0447 - val_loss: 0.0807 - val_dense_8_loss: 0.0407 - val_dense_9_loss: 0.0411\nVal Jaccard Similarity: 0.6558439716903287\nEpoch 6/200\n21560/21560 [==============================] - 13s 606us/step - loss: 0.0868 - dense_8_loss: 0.0434 - dense_9_loss: 0.0434 - val_loss: 0.0784 - val_dense_8_loss: 0.0395 - val_dense_9_loss: 0.0399\nVal Jaccard Similarity: 0.6649692776832862\nEpoch 7/200\n21560/21560 [==============================] - 13s 608us/step - loss: 0.0848 - dense_8_loss: 0.0424 - dense_9_loss: 0.0424 - val_loss: 0.0769 - val_dense_8_loss: 0.0387 - val_dense_9_loss: 0.0391\nVal Jaccard Similarity: 0.6660396728582124\nEpoch 8/200\n21560/21560 [==============================] - 13s 592us/step - loss: 0.0836 - dense_8_loss: 0.0419 - dense_9_loss: 0.0417 - val_loss: 0.0757 - val_dense_8_loss: 0.0383 - val_dense_9_loss: 0.0385\nVal Jaccard Similarity: 0.6751660807949234\nEpoch 9/200\n21560/21560 [==============================] - 13s 586us/step - loss: 0.0825 - dense_8_loss: 0.0413 - dense_9_loss: 0.0412 - val_loss: 0.0751 - val_dense_8_loss: 0.0380 - val_dense_9_loss: 0.0380\nVal Jaccard Similarity: 0.6769429228043267\nEpoch 10/200\n21560/21560 [==============================] - 13s 594us/step - loss: 0.0816 - dense_8_loss: 0.0409 - dense_9_loss: 0.0407 - val_loss: 0.0741 - val_dense_8_loss: 0.0375 - val_dense_9_loss: 0.0375\nVal Jaccard Similarity: 0.6858406411726703\nEpoch 11/200\n21560/21560 [==============================] - 13s 619us/step - loss: 0.0808 - dense_8_loss: 0.0407 - dense_9_loss: 0.0401 - val_loss: 0.0736 - val_dense_8_loss: 0.0374 - val_dense_9_loss: 0.0372\nVal Jaccard Similarity: 0.6861321171298088\nEpoch 12/200\n21560/21560 [==============================] - 13s 585us/step - loss: 0.0802 - dense_8_loss: 0.0404 - dense_9_loss: 0.0398 - val_loss: 0.0732 - val_dense_8_loss: 0.0371 - val_dense_9_loss: 0.0369\nVal Jaccard Similarity: 0.6850866857070184\nEpoch 13/200\n21560/21560 [==============================] - 13s 612us/step - loss: 0.0797 - dense_8_loss: 0.0401 - dense_9_loss: 0.0395 - val_loss: 0.0727 - val_dense_8_loss: 0.0370 - val_dense_9_loss: 0.0367\nVal Jaccard Similarity: 0.6878800917514402\nEpoch 14/200\n21560/21560 [==============================] - 13s 582us/step - loss: 0.0792 - dense_8_loss: 0.0399 - dense_9_loss: 0.0393 - val_loss: 0.0726 - val_dense_8_loss: 0.0372 - val_dense_9_loss: 0.0365\nVal Jaccard Similarity: 0.6911659645508667\nEpoch 15/200\n21560/21560 [==============================] - 14s 638us/step - loss: 0.0790 - dense_8_loss: 0.0398 - dense_9_loss: 0.0392 - val_loss: 0.0722 - val_dense_8_loss: 0.0368 - val_dense_9_loss: 0.0364\nVal Jaccard Similarity: 0.685165767336807\nEpoch 16/200\n21560/21560 [==============================] - 13s 599us/step - loss: 0.0785 - dense_8_loss: 0.0396 - dense_9_loss: 0.0389 - val_loss: 0.0717 - val_dense_8_loss: 0.0367 - val_dense_9_loss: 0.0362\nVal Jaccard Similarity: 0.6871301887611209\nEpoch 17/200\n21560/21560 [==============================] - 13s 585us/step - loss: 0.0778 - dense_8_loss: 0.0392 - dense_9_loss: 0.0385 - val_loss: 0.0713 - val_dense_8_loss: 0.0364 - val_dense_9_loss: 0.0359\nVal Jaccard Similarity: 0.69128307217485\nEpoch 18/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0776 - dense_8_loss: 0.0392 - dense_9_loss: 0.0383 - val_loss: 0.0715 - val_dense_8_loss: 0.0365 - val_dense_9_loss: 0.0359\nVal Jaccard Similarity: 0.6879055170694874\nEpoch 19/200\n21560/21560 [==============================] - 13s 617us/step - loss: 0.0770 - dense_8_loss: 0.0390 - dense_9_loss: 0.0381 - val_loss: 0.0708 - val_dense_8_loss: 0.0361 - val_dense_9_loss: 0.0355\nVal Jaccard Similarity: 0.6924515821755609\nEpoch 20/200\n21560/21560 [==============================] - 13s 584us/step - loss: 0.0769 - dense_8_loss: 0.0389 - dense_9_loss: 0.0380 - val_loss: 0.0710 - val_dense_8_loss: 0.0362 - val_dense_9_loss: 0.0357\nVal Jaccard Similarity: 0.6891057666477847\nEpoch 21/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0766 - dense_8_loss: 0.0388 - dense_9_loss: 0.0378 - val_loss: 0.0700 - val_dense_8_loss: 0.0359 - val_dense_9_loss: 0.0352\nVal Jaccard Similarity: 0.6985751230822301\nEpoch 22/200\n21560/21560 [==============================] - 13s 585us/step - loss: 0.0765 - dense_8_loss: 0.0387 - dense_9_loss: 0.0378 - val_loss: 0.0702 - val_dense_8_loss: 0.0358 - val_dense_9_loss: 0.0352\nVal Jaccard Similarity: 0.6950823006255548\nEpoch 23/200\n21560/21560 [==============================] - 13s 602us/step - loss: 0.0762 - dense_8_loss: 0.0385 - dense_9_loss: 0.0376 - val_loss: 0.0703 - val_dense_8_loss: 0.0360 - val_dense_9_loss: 0.0354\nVal Jaccard Similarity: 0.69510941672123\nEpoch 24/200\n21560/21560 [==============================] - 13s 596us/step - loss: 0.0757 - dense_8_loss: 0.0383 - dense_9_loss: 0.0374 - val_loss: 0.0700 - val_dense_8_loss: 0.0358 - val_dense_9_loss: 0.0351\n","name":"stdout"},{"output_type":"stream","text":"Val Jaccard Similarity: 0.6999335772331678\nEpoch 25/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0755 - dense_8_loss: 0.0382 - dense_9_loss: 0.0373 - val_loss: 0.0699 - val_dense_8_loss: 0.0357 - val_dense_9_loss: 0.0351\nVal Jaccard Similarity: 0.6985261175492206\nEpoch 26/200\n21560/21560 [==============================] - 13s 587us/step - loss: 0.0751 - dense_8_loss: 0.0380 - dense_9_loss: 0.0371 - val_loss: 0.0697 - val_dense_8_loss: 0.0358 - val_dense_9_loss: 0.0348\nVal Jaccard Similarity: 0.7040279113986752\nEpoch 27/200\n21560/21560 [==============================] - 13s 596us/step - loss: 0.0749 - dense_8_loss: 0.0379 - dense_9_loss: 0.0369 - val_loss: 0.0694 - val_dense_8_loss: 0.0355 - val_dense_9_loss: 0.0349\nVal Jaccard Similarity: 0.7022302134495331\nEpoch 28/200\n21560/21560 [==============================] - 13s 597us/step - loss: 0.0748 - dense_8_loss: 0.0378 - dense_9_loss: 0.0369 - val_loss: 0.0695 - val_dense_8_loss: 0.0356 - val_dense_9_loss: 0.0347\nVal Jaccard Similarity: 0.7056393732950444\nEpoch 29/200\n21560/21560 [==============================] - 13s 580us/step - loss: 0.0748 - dense_8_loss: 0.0379 - dense_9_loss: 0.0369 - val_loss: 0.0691 - val_dense_8_loss: 0.0352 - val_dense_9_loss: 0.0347\nVal Jaccard Similarity: 0.707658616135887\nEpoch 30/200\n21560/21560 [==============================] - 12s 572us/step - loss: 0.0742 - dense_8_loss: 0.0376 - dense_9_loss: 0.0366 - val_loss: 0.0694 - val_dense_8_loss: 0.0354 - val_dense_9_loss: 0.0349\nVal Jaccard Similarity: 0.706937412732956\nEpoch 31/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0744 - dense_8_loss: 0.0377 - dense_9_loss: 0.0367 - val_loss: 0.0694 - val_dense_8_loss: 0.0356 - val_dense_9_loss: 0.0346\nVal Jaccard Similarity: 0.7073677204319032\nEpoch 32/200\n21560/21560 [==============================] - 13s 612us/step - loss: 0.0740 - dense_8_loss: 0.0374 - dense_9_loss: 0.0366 - val_loss: 0.0690 - val_dense_8_loss: 0.0353 - val_dense_9_loss: 0.0346\nVal Jaccard Similarity: 0.7054536817665295\nEpoch 33/200\n21560/21560 [==============================] - 13s 582us/step - loss: 0.0735 - dense_8_loss: 0.0373 - dense_9_loss: 0.0363 - val_loss: 0.0688 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0347\nVal Jaccard Similarity: 0.7017417986156712\nEpoch 34/200\n21560/21560 [==============================] - 12s 579us/step - loss: 0.0736 - dense_8_loss: 0.0374 - dense_9_loss: 0.0363 - val_loss: 0.0686 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0344\nVal Jaccard Similarity: 0.7063670887950548\nEpoch 35/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0736 - dense_8_loss: 0.0373 - dense_9_loss: 0.0363 - val_loss: 0.0692 - val_dense_8_loss: 0.0355 - val_dense_9_loss: 0.0346\nVal Jaccard Similarity: 0.7062747622132618\nEpoch 36/200\n21560/21560 [==============================] - 13s 615us/step - loss: 0.0733 - dense_8_loss: 0.0370 - dense_9_loss: 0.0363 - val_loss: 0.0691 - val_dense_8_loss: 0.0354 - val_dense_9_loss: 0.0345\nVal Jaccard Similarity: 0.7017553392328134\nEpoch 37/200\n21560/21560 [==============================] - 13s 583us/step - loss: 0.0733 - dense_8_loss: 0.0372 - dense_9_loss: 0.0362 - val_loss: 0.0686 - val_dense_8_loss: 0.0350 - val_dense_9_loss: 0.0344\nVal Jaccard Similarity: 0.7062191606711566\nEpoch 38/200\n21560/21560 [==============================] - 13s 590us/step - loss: 0.0732 - dense_8_loss: 0.0371 - dense_9_loss: 0.0361 - val_loss: 0.0688 - val_dense_8_loss: 0.0352 - val_dense_9_loss: 0.0346\nVal Jaccard Similarity: 0.7088347486749469\nEpoch 39/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0729 - dense_8_loss: 0.0368 - dense_9_loss: 0.0361 - val_loss: 0.0685 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.708928938290315\nEpoch 40/200\n21560/21560 [==============================] - 13s 610us/step - loss: 0.0727 - dense_8_loss: 0.0369 - dense_9_loss: 0.0358 - val_loss: 0.0687 - val_dense_8_loss: 0.0352 - val_dense_9_loss: 0.0344\nVal Jaccard Similarity: 0.708291013838653\nEpoch 41/200\n21560/21560 [==============================] - 13s 587us/step - loss: 0.0727 - dense_8_loss: 0.0369 - dense_9_loss: 0.0359 - val_loss: 0.0685 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0344\nVal Jaccard Similarity: 0.7094375911815918\nEpoch 42/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0725 - dense_8_loss: 0.0366 - dense_9_loss: 0.0358 - val_loss: 0.0682 - val_dense_8_loss: 0.0350 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7113461480042187\nEpoch 43/200\n21560/21560 [==============================] - 13s 587us/step - loss: 0.0724 - dense_8_loss: 0.0366 - dense_9_loss: 0.0358 - val_loss: 0.0679 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7127565825093312\nEpoch 44/200\n21560/21560 [==============================] - 13s 596us/step - loss: 0.0725 - dense_8_loss: 0.0367 - dense_9_loss: 0.0358 - val_loss: 0.0686 - val_dense_8_loss: 0.0353 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7122671774907922\nEpoch 45/200\n21560/21560 [==============================] - 13s 614us/step - loss: 0.0722 - dense_8_loss: 0.0366 - dense_9_loss: 0.0355 - val_loss: 0.0692 - val_dense_8_loss: 0.0355 - val_dense_9_loss: 0.0346\nVal Jaccard Similarity: 0.7084391278731619\nEpoch 46/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0722 - dense_8_loss: 0.0366 - dense_9_loss: 0.0356 - val_loss: 0.0679 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7123214163558613\nEpoch 47/200\n21560/21560 [==============================] - 13s 580us/step - loss: 0.0725 - dense_8_loss: 0.0368 - dense_9_loss: 0.0357 - val_loss: 0.0680 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7138393876621322\nEpoch 48/200\n21560/21560 [==============================] - 13s 582us/step - loss: 0.0717 - dense_8_loss: 0.0364 - dense_9_loss: 0.0353 - val_loss: 0.0688 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0345\nVal Jaccard Similarity: 0.7083669741082665\nEpoch 49/200\n21560/21560 [==============================] - 13s 611us/step - loss: 0.0716 - dense_8_loss: 0.0363 - dense_9_loss: 0.0352 - val_loss: 0.0680 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0344\nVal Jaccard Similarity: 0.7097373846976209\nEpoch 50/200\n21560/21560 [==============================] - 12s 576us/step - loss: 0.0716 - dense_8_loss: 0.0363 - dense_9_loss: 0.0353 - val_loss: 0.0677 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7136240198186994\nEpoch 51/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0716 - dense_8_loss: 0.0363 - dense_9_loss: 0.0354 - val_loss: 0.0681 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7081269047416063\nEpoch 52/200\n21560/21560 [==============================] - 13s 580us/step - loss: 0.0713 - dense_8_loss: 0.0362 - dense_9_loss: 0.0352 - val_loss: 0.0686 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7117564471635817\nEpoch 53/200\n21560/21560 [==============================] - 13s 606us/step - loss: 0.0717 - dense_8_loss: 0.0363 - dense_9_loss: 0.0354 - val_loss: 0.0681 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7118164518447926\nEpoch 54/200\n21560/21560 [==============================] - 13s 589us/step - loss: 0.0712 - dense_8_loss: 0.0360 - dense_9_loss: 0.0352 - val_loss: 0.0684 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7149131593305076\nEpoch 55/200\n21560/21560 [==============================] - 13s 597us/step - loss: 0.0714 - dense_8_loss: 0.0363 - dense_9_loss: 0.0352 - val_loss: 0.0678 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7132099261492096\nEpoch 56/200\n21560/21560 [==============================] - 13s 580us/step - loss: 0.0711 - dense_8_loss: 0.0359 - dense_9_loss: 0.0352 - val_loss: 0.0677 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7117875220904385\nEpoch 57/200\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 13s 620us/step - loss: 0.0715 - dense_8_loss: 0.0361 - dense_9_loss: 0.0353 - val_loss: 0.0678 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7137328283836303\nEpoch 58/200\n21560/21560 [==============================] - 13s 596us/step - loss: 0.0712 - dense_8_loss: 0.0361 - dense_9_loss: 0.0351 - val_loss: 0.0682 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7122871008207629\nEpoch 59/200\n21560/21560 [==============================] - 13s 603us/step - loss: 0.0709 - dense_8_loss: 0.0359 - dense_9_loss: 0.0351 - val_loss: 0.0681 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7147027245898943\nEpoch 60/200\n21560/21560 [==============================] - 13s 594us/step - loss: 0.0710 - dense_8_loss: 0.0360 - dense_9_loss: 0.0350 - val_loss: 0.0678 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7138314684981322\nEpoch 61/200\n21560/21560 [==============================] - 13s 609us/step - loss: 0.0708 - dense_8_loss: 0.0358 - dense_9_loss: 0.0350 - val_loss: 0.0680 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7182355495386693\nEpoch 62/200\n21560/21560 [==============================] - 13s 590us/step - loss: 0.0707 - dense_8_loss: 0.0358 - dense_9_loss: 0.0348 - val_loss: 0.0676 - val_dense_8_loss: 0.0345 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7115314944612122\nEpoch 63/200\n21560/21560 [==============================] - 13s 583us/step - loss: 0.0704 - dense_8_loss: 0.0357 - dense_9_loss: 0.0347 - val_loss: 0.0675 - val_dense_8_loss: 0.0344 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7160720723268715\nEpoch 64/200\n21560/21560 [==============================] - 13s 584us/step - loss: 0.0705 - dense_8_loss: 0.0356 - dense_9_loss: 0.0349 - val_loss: 0.0682 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7100748763970584\nEpoch 65/200\n21560/21560 [==============================] - 13s 592us/step - loss: 0.0706 - dense_8_loss: 0.0358 - dense_9_loss: 0.0348 - val_loss: 0.0676 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7145249034625285\nEpoch 66/200\n21560/21560 [==============================] - 13s 616us/step - loss: 0.0705 - dense_8_loss: 0.0358 - dense_9_loss: 0.0347 - val_loss: 0.0676 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7143422278791056\nEpoch 67/200\n21560/21560 [==============================] - 13s 586us/step - loss: 0.0704 - dense_8_loss: 0.0356 - dense_9_loss: 0.0348 - val_loss: 0.0679 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7128618486767591\nEpoch 68/200\n21560/21560 [==============================] - 13s 587us/step - loss: 0.0703 - dense_8_loss: 0.0355 - dense_9_loss: 0.0347 - val_loss: 0.0678 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7182555707387595\nEpoch 69/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0701 - dense_8_loss: 0.0355 - dense_9_loss: 0.0347 - val_loss: 0.0683 - val_dense_8_loss: 0.0353 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7114120133684352\nEpoch 70/200\n21560/21560 [==============================] - 13s 609us/step - loss: 0.0705 - dense_8_loss: 0.0357 - dense_9_loss: 0.0347 - val_loss: 0.0688 - val_dense_8_loss: 0.0354 - val_dense_9_loss: 0.0344\nVal Jaccard Similarity: 0.7112487490095745\nEpoch 71/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0701 - dense_8_loss: 0.0356 - dense_9_loss: 0.0346 - val_loss: 0.0678 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7161565542845509\nEpoch 72/200\n21560/21560 [==============================] - 13s 581us/step - loss: 0.0701 - dense_8_loss: 0.0355 - dense_9_loss: 0.0345 - val_loss: 0.0676 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7131063208950769\nEpoch 73/200\n21560/21560 [==============================] - 13s 584us/step - loss: 0.0701 - dense_8_loss: 0.0355 - dense_9_loss: 0.0345 - val_loss: 0.0682 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.713196100617974\nEpoch 74/200\n21560/21560 [==============================] - 13s 610us/step - loss: 0.0700 - dense_8_loss: 0.0355 - dense_9_loss: 0.0345 - val_loss: 0.0681 - val_dense_8_loss: 0.0350 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7130291995595655\nEpoch 75/200\n21560/21560 [==============================] - 13s 595us/step - loss: 0.0699 - dense_8_loss: 0.0355 - dense_9_loss: 0.0345 - val_loss: 0.0679 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.716300756748211\nEpoch 76/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0700 - dense_8_loss: 0.0354 - dense_9_loss: 0.0346 - val_loss: 0.0678 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7128059347576864\nEpoch 77/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0699 - dense_8_loss: 0.0353 - dense_9_loss: 0.0345 - val_loss: 0.0692 - val_dense_8_loss: 0.0355 - val_dense_9_loss: 0.0346\nVal Jaccard Similarity: 0.7183542391497055\nEpoch 78/200\n21560/21560 [==============================] - 13s 588us/step - loss: 0.0697 - dense_8_loss: 0.0354 - dense_9_loss: 0.0343 - val_loss: 0.0678 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7170522034625183\nEpoch 79/200\n21560/21560 [==============================] - 13s 583us/step - loss: 0.0699 - dense_8_loss: 0.0354 - dense_9_loss: 0.0344 - val_loss: 0.0680 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7158051424603235\nEpoch 80/200\n21560/21560 [==============================] - 13s 590us/step - loss: 0.0697 - dense_8_loss: 0.0352 - dense_9_loss: 0.0345 - val_loss: 0.0681 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7157635622933638\nEpoch 81/200\n21560/21560 [==============================] - 13s 611us/step - loss: 0.0692 - dense_8_loss: 0.0350 - dense_9_loss: 0.0342 - val_loss: 0.0683 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7111888881101927\nEpoch 82/200\n21560/21560 [==============================] - 13s 605us/step - loss: 0.0696 - dense_8_loss: 0.0354 - dense_9_loss: 0.0342 - val_loss: 0.0679 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7146049095885194\nEpoch 83/200\n21560/21560 [==============================] - 12s 579us/step - loss: 0.0696 - dense_8_loss: 0.0352 - dense_9_loss: 0.0344 - val_loss: 0.0682 - val_dense_8_loss: 0.0352 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7150581509363554\nEpoch 84/200\n21560/21560 [==============================] - 12s 571us/step - loss: 0.0695 - dense_8_loss: 0.0352 - dense_9_loss: 0.0342 - val_loss: 0.0692 - val_dense_8_loss: 0.0355 - val_dense_9_loss: 0.0346\nVal Jaccard Similarity: 0.713455898527056\nEpoch 85/200\n21560/21560 [==============================] - 12s 572us/step - loss: 0.0693 - dense_8_loss: 0.0351 - dense_9_loss: 0.0342 - val_loss: 0.0685 - val_dense_8_loss: 0.0355 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.716062757471823\nEpoch 86/200\n21560/21560 [==============================] - 12s 570us/step - loss: 0.0693 - dense_8_loss: 0.0351 - dense_9_loss: 0.0342 - val_loss: 0.0673 - val_dense_8_loss: 0.0345 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.711435163007387\nEpoch 87/200\n21560/21560 [==============================] - 13s 610us/step - loss: 0.0694 - dense_8_loss: 0.0352 - dense_9_loss: 0.0342 - val_loss: 0.0678 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7171705917564557\nEpoch 88/200\n21560/21560 [==============================] - 12s 574us/step - loss: 0.0692 - dense_8_loss: 0.0350 - dense_9_loss: 0.0342 - val_loss: 0.0678 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7163246938334462\nEpoch 89/200\n21560/21560 [==============================] - 13s 580us/step - loss: 0.0693 - dense_8_loss: 0.0351 - dense_9_loss: 0.0342 - val_loss: 0.0676 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0339\n","name":"stdout"},{"output_type":"stream","text":"Val Jaccard Similarity: 0.7153834683385509\nEpoch 90/200\n21560/21560 [==============================] - 12s 573us/step - loss: 0.0692 - dense_8_loss: 0.0350 - dense_9_loss: 0.0341 - val_loss: 0.0677 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7164511385496671\nEpoch 91/200\n21560/21560 [==============================] - 13s 609us/step - loss: 0.0691 - dense_8_loss: 0.0350 - dense_9_loss: 0.0341 - val_loss: 0.0682 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7161386179158155\nEpoch 92/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0690 - dense_8_loss: 0.0348 - dense_9_loss: 0.0342 - val_loss: 0.0678 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7158923272246154\nEpoch 93/200\n21560/21560 [==============================] - 12s 579us/step - loss: 0.0690 - dense_8_loss: 0.0349 - dense_9_loss: 0.0341 - val_loss: 0.0674 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7160930515375298\nEpoch 94/200\n21560/21560 [==============================] - 13s 585us/step - loss: 0.0690 - dense_8_loss: 0.0349 - dense_9_loss: 0.0342 - val_loss: 0.0677 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7143250961010886\nEpoch 95/200\n21560/21560 [==============================] - 13s 594us/step - loss: 0.0691 - dense_8_loss: 0.0351 - dense_9_loss: 0.0340 - val_loss: 0.0673 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7145610719160169\nEpoch 96/200\n21560/21560 [==============================] - 13s 588us/step - loss: 0.0687 - dense_8_loss: 0.0348 - dense_9_loss: 0.0338 - val_loss: 0.0680 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.713577389531533\nEpoch 97/200\n21560/21560 [==============================] - 13s 581us/step - loss: 0.0688 - dense_8_loss: 0.0349 - dense_9_loss: 0.0340 - val_loss: 0.0686 - val_dense_8_loss: 0.0352 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7152241245601766\nEpoch 98/200\n21560/21560 [==============================] - 13s 581us/step - loss: 0.0691 - dense_8_loss: 0.0350 - dense_9_loss: 0.0341 - val_loss: 0.0674 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189710462659988\nEpoch 99/200\n21560/21560 [==============================] - 13s 595us/step - loss: 0.0687 - dense_8_loss: 0.0348 - dense_9_loss: 0.0338 - val_loss: 0.0675 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7129870732248234\nEpoch 100/200\n21560/21560 [==============================] - 13s 592us/step - loss: 0.0689 - dense_8_loss: 0.0348 - dense_9_loss: 0.0340 - val_loss: 0.0681 - val_dense_8_loss: 0.0350 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7147577395850742\nEpoch 101/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0689 - dense_8_loss: 0.0350 - dense_9_loss: 0.0339 - val_loss: 0.0679 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7190000491531402\nEpoch 102/200\n21560/21560 [==============================] - 12s 570us/step - loss: 0.0690 - dense_8_loss: 0.0349 - dense_9_loss: 0.0341 - val_loss: 0.0672 - val_dense_8_loss: 0.0344 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7168247747235433\nEpoch 103/200\n21560/21560 [==============================] - 13s 584us/step - loss: 0.0689 - dense_8_loss: 0.0349 - dense_9_loss: 0.0340 - val_loss: 0.0678 - val_dense_8_loss: 0.0350 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7164463950930882\nEpoch 104/200\n21560/21560 [==============================] - 13s 611us/step - loss: 0.0685 - dense_8_loss: 0.0346 - dense_9_loss: 0.0339 - val_loss: 0.0676 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7159911862666182\nEpoch 105/200\n21560/21560 [==============================] - 13s 610us/step - loss: 0.0687 - dense_8_loss: 0.0348 - dense_9_loss: 0.0339 - val_loss: 0.0677 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7163903358029033\nEpoch 106/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0688 - dense_8_loss: 0.0350 - dense_9_loss: 0.0339 - val_loss: 0.0681 - val_dense_8_loss: 0.0350 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7154717371917556\nEpoch 107/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0687 - dense_8_loss: 0.0348 - dense_9_loss: 0.0339 - val_loss: 0.0678 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7171571180341652\nEpoch 108/200\n21560/21560 [==============================] - 13s 617us/step - loss: 0.0685 - dense_8_loss: 0.0348 - dense_9_loss: 0.0337 - val_loss: 0.0675 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7191184708478234\nEpoch 109/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0686 - dense_8_loss: 0.0349 - dense_9_loss: 0.0337 - val_loss: 0.0678 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7160235959088769\nEpoch 110/200\n21560/21560 [==============================] - 13s 591us/step - loss: 0.0686 - dense_8_loss: 0.0347 - dense_9_loss: 0.0338 - val_loss: 0.0677 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7175278179073573\nEpoch 111/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0686 - dense_8_loss: 0.0346 - dense_9_loss: 0.0339 - val_loss: 0.0676 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.716573578697351\nEpoch 112/200\n21560/21560 [==============================] - 13s 608us/step - loss: 0.0682 - dense_8_loss: 0.0345 - dense_9_loss: 0.0337 - val_loss: 0.0676 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7151859479102146\nEpoch 113/200\n21560/21560 [==============================] - 12s 577us/step - loss: 0.0684 - dense_8_loss: 0.0346 - dense_9_loss: 0.0338 - val_loss: 0.0675 - val_dense_8_loss: 0.0345 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7142383245279038\nEpoch 114/200\n21560/21560 [==============================] - 12s 572us/step - loss: 0.0682 - dense_8_loss: 0.0347 - dense_9_loss: 0.0335 - val_loss: 0.0676 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7191985061770343\nEpoch 115/200\n21560/21560 [==============================] - 13s 582us/step - loss: 0.0683 - dense_8_loss: 0.0346 - dense_9_loss: 0.0338 - val_loss: 0.0678 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7177812206659869\nEpoch 116/200\n21560/21560 [==============================] - 13s 589us/step - loss: 0.0683 - dense_8_loss: 0.0345 - dense_9_loss: 0.0337 - val_loss: 0.0679 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7199528258953667\nEpoch 117/200\n21560/21560 [==============================] - 13s 602us/step - loss: 0.0681 - dense_8_loss: 0.0344 - dense_9_loss: 0.0336 - val_loss: 0.0673 - val_dense_8_loss: 0.0344 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7164242357190943\nEpoch 118/200\n21560/21560 [==============================] - 12s 569us/step - loss: 0.0684 - dense_8_loss: 0.0346 - dense_9_loss: 0.0338 - val_loss: 0.0679 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.717486367046117\nEpoch 119/200\n21560/21560 [==============================] - 12s 573us/step - loss: 0.0683 - dense_8_loss: 0.0345 - dense_9_loss: 0.0338 - val_loss: 0.0683 - val_dense_8_loss: 0.0350 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7198563516776785\nEpoch 120/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0682 - dense_8_loss: 0.0347 - dense_9_loss: 0.0335 - val_loss: 0.0678 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7192709397447622\nEpoch 121/200\n21560/21560 [==============================] - 13s 611us/step - loss: 0.0685 - dense_8_loss: 0.0347 - dense_9_loss: 0.0338 - val_loss: 0.0677 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7181239020922069\nEpoch 122/200\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 12s 578us/step - loss: 0.0677 - dense_8_loss: 0.0344 - dense_9_loss: 0.0334 - val_loss: 0.0679 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7186887668361355\nEpoch 123/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0681 - dense_8_loss: 0.0345 - dense_9_loss: 0.0337 - val_loss: 0.0680 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7192805846165078\nEpoch 124/200\n21560/21560 [==============================] - 12s 579us/step - loss: 0.0677 - dense_8_loss: 0.0344 - dense_9_loss: 0.0333 - val_loss: 0.0674 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201806663750604\nEpoch 125/200\n21560/21560 [==============================] - 13s 619us/step - loss: 0.0680 - dense_8_loss: 0.0346 - dense_9_loss: 0.0334 - val_loss: 0.0677 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7147511168563762\nEpoch 126/200\n21560/21560 [==============================] - 12s 578us/step - loss: 0.0682 - dense_8_loss: 0.0346 - dense_9_loss: 0.0335 - val_loss: 0.0679 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.7150648729210697\nEpoch 127/200\n21560/21560 [==============================] - 13s 582us/step - loss: 0.0682 - dense_8_loss: 0.0346 - dense_9_loss: 0.0336 - val_loss: 0.0677 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7167839289839235\nEpoch 128/200\n21560/21560 [==============================] - 12s 579us/step - loss: 0.0681 - dense_8_loss: 0.0345 - dense_9_loss: 0.0337 - val_loss: 0.0682 - val_dense_8_loss: 0.0352 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7149175198069211\nEpoch 129/200\n21560/21560 [==============================] - 13s 616us/step - loss: 0.0682 - dense_8_loss: 0.0345 - dense_9_loss: 0.0337 - val_loss: 0.0676 - val_dense_8_loss: 0.0345 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7123354089684097\nEpoch 130/200\n21560/21560 [==============================] - 12s 579us/step - loss: 0.0680 - dense_8_loss: 0.0345 - dense_9_loss: 0.0335 - val_loss: 0.0672 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.714800250133067\nEpoch 131/200\n21560/21560 [==============================] - 13s 587us/step - loss: 0.0678 - dense_8_loss: 0.0343 - dense_9_loss: 0.0335 - val_loss: 0.0674 - val_dense_8_loss: 0.0345 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7177292808008535\nEpoch 132/200\n21560/21560 [==============================] - 12s 573us/step - loss: 0.0676 - dense_8_loss: 0.0343 - dense_9_loss: 0.0333 - val_loss: 0.0682 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7159224999128311\nEpoch 133/200\n21560/21560 [==============================] - 13s 585us/step - loss: 0.0677 - dense_8_loss: 0.0343 - dense_9_loss: 0.0334 - val_loss: 0.0676 - val_dense_8_loss: 0.0346 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7186905371828256\nEpoch 134/200\n21560/21560 [==============================] - 13s 601us/step - loss: 0.0676 - dense_8_loss: 0.0342 - dense_9_loss: 0.0334 - val_loss: 0.0680 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.71633970225451\nEpoch 135/200\n21560/21560 [==============================] - 12s 575us/step - loss: 0.0675 - dense_8_loss: 0.0342 - dense_9_loss: 0.0333 - val_loss: 0.0682 - val_dense_8_loss: 0.0349 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7178524048140479\nEpoch 136/200\n21560/21560 [==============================] - 12s 576us/step - loss: 0.0676 - dense_8_loss: 0.0343 - dense_9_loss: 0.0334 - val_loss: 0.0679 - val_dense_8_loss: 0.0348 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7151681072609453\nEpoch 137/200\n21560/21560 [==============================] - 12s 572us/step - loss: 0.0676 - dense_8_loss: 0.0342 - dense_9_loss: 0.0334 - val_loss: 0.0685 - val_dense_8_loss: 0.0351 - val_dense_9_loss: 0.0343\nVal Jaccard Similarity: 0.7174990240771251\nEpoch 138/200\n21560/21560 [==============================] - 13s 617us/step - loss: 0.0675 - dense_8_loss: 0.0343 - dense_9_loss: 0.0332 - val_loss: 0.0679 - val_dense_8_loss: 0.0345 - val_dense_9_loss: 0.0342\nVal Jaccard Similarity: 0.7167793041474525\nEpoch 139/200\n21560/21560 [==============================] - 12s 569us/step - loss: 0.0675 - dense_8_loss: 0.0342 - dense_9_loss: 0.0333 - val_loss: 0.0678 - val_dense_8_loss: 0.0345 - val_dense_9_loss: 0.0340\nVal Jaccard Similarity: 0.7166376644688889\nEpoch 140/200\n21560/21560 [==============================] - 12s 573us/step - loss: 0.0677 - dense_8_loss: 0.0344 - dense_9_loss: 0.0334 - val_loss: 0.0681 - val_dense_8_loss: 0.0347 - val_dense_9_loss: 0.0341\nVal Jaccard Similarity: 0.71639841161004\nEpoch 141/200\n10700/21560 [=============>................] - ETA: 5s - loss: 0.0680 - dense_8_loss: 0.0344 - dense_9_loss: 0.0336","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a4b5a46a700e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics_categorical\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"Epoch 98/100\n21561/21561 [==============================] - 8s 357us/step - loss: 26.4123 - val_loss: 32.8301\nVal Jaccard Similarity: 0.5266293328859964\n\nEpoch 99/100\n21561/21561 [==============================] - 8s 356us/step - loss: 26.0379 - val_loss: 32.5830\nVal Jaccard Similarity: 0.5592650588400309\n\nEpoch 100/100\n21561/21561 [==============================] - 8s 352us/step - loss: 25.8462 - val_loss: 32.7917\nVal Jaccard Similarity: 0.5504962474510444"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2.fit(\n    [train_manip_X, train_numerics_repeated], \n    [train_manip_Y_categorical[0], train_manip_Y_categorical[1]], \n    validation_data=(\n        [test_manip_X, test_numerics_repeated], \n        [test_manip_Y_categorical[0], test_manip_Y_categorical[1]]), \n    batch_size=10000, \n    epochs=1000,\n    callbacks=[metrics_categorical]\n)","execution_count":null,"outputs":[{"output_type":"stream","text":"Train on 21560 samples, validate on 3804 samples\nEpoch 1/1000\n21560/21560 [==============================] - 9s 433us/step - loss: 0.0658 - dense_8_loss: 0.0339 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7218930148985786\nEpoch 2/1000\n21560/21560 [==============================] - 7s 344us/step - loss: 0.0658 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7216997479714511\nEpoch 3/1000\n21560/21560 [==============================] - 8s 379us/step - loss: 0.0662 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7222091212281733\nEpoch 4/1000\n21560/21560 [==============================] - 8s 351us/step - loss: 0.0657 - dense_8_loss: 0.0336 - dense_9_loss: 0.0327 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215352694190896\nEpoch 5/1000\n21560/21560 [==============================] - 7s 330us/step - loss: 0.0657 - dense_8_loss: 0.0327 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0339 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202440199837691\nEpoch 6/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189238558366413\nEpoch 7/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0658 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197147116811191\nEpoch 8/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0658 - dense_8_loss: 0.0327 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7205790734836478\nEpoch 9/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198999451487073\nEpoch 10/1000\n21560/21560 [==============================] - 8s 354us/step - loss: 0.0659 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202045468351949\nEpoch 11/1000\n21560/21560 [==============================] - 7s 333us/step - loss: 0.0658 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7200264286837191\nEpoch 12/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719228378729558\nEpoch 13/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0658 - dense_8_loss: 0.0333 - dense_9_loss: 0.0318 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191460648863222\nEpoch 14/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0322 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7194397831076721\nEpoch 15/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205620071042355\nEpoch 16/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207346198453054\nEpoch 17/1000\n21560/21560 [==============================] - 8s 352us/step - loss: 0.0656 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7220215774095934\nEpoch 18/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0658 - dense_8_loss: 0.0337 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.721287922988908\nEpoch 19/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7203054662296373\nEpoch 20/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0658 - dense_8_loss: 0.0334 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7198064183531411\nEpoch 21/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0658 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7193784332325556\nEpoch 22/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196050231103422\nEpoch 23/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0657 - dense_8_loss: 0.0339 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7198575036136053\nEpoch 24/1000\n21560/21560 [==============================] - 8s 348us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0329 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201340442232621\nEpoch 25/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0656 - dense_8_loss: 0.0328 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205085657821328\nEpoch 26/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0660 - dense_8_loss: 0.0334 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190721815475406\nEpoch 27/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0658 - dense_8_loss: 0.0330 - dense_9_loss: 0.0327 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719532174194774\nEpoch 28/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0658 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7200676621016185\nEpoch 29/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0655 - dense_8_loss: 0.0336 - dense_9_loss: 0.0330 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204736251543028\nEpoch 30/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0660 - dense_8_loss: 0.0335 - dense_9_loss: 0.0328 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7217705766725531\nEpoch 31/1000\n21560/21560 [==============================] - 7s 344us/step - loss: 0.0657 - dense_8_loss: 0.0336 - dense_9_loss: 0.0328 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215921934916757\nEpoch 32/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0315 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7202565933168604\nEpoch 33/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 323us/step - loss: 0.0660 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204436273367022\nEpoch 34/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0659 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204316707178481\nEpoch 35/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0328 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196071438683778\nEpoch 36/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0319 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190405458589565\nEpoch 37/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0659 - dense_8_loss: 0.0337 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7199897992013156\nEpoch 38/1000\n21560/21560 [==============================] - 7s 333us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0328 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201336625695838\nEpoch 39/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0659 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205217450182007\nEpoch 40/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0658 - dense_8_loss: 0.0335 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7210647943671408\nEpoch 41/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0659 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.720315015985189\nEpoch 42/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0662 - dense_8_loss: 0.0338 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7200125628364538\nEpoch 43/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0654 - dense_8_loss: 0.0337 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7194684079326565\nEpoch 44/1000\n21560/21560 [==============================] - 7s 333us/step - loss: 0.0657 - dense_8_loss: 0.0333 - dense_9_loss: 0.0328 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.72024087514577\nEpoch 45/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198373911781909\nEpoch 46/1000\n21560/21560 [==============================] - 7s 330us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0318 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197221726922172\nEpoch 47/1000\n21560/21560 [==============================] - 7s 315us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7204898684673211\nEpoch 48/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7193018259077993\nEpoch 49/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7190857663511361\nEpoch 50/1000\n21560/21560 [==============================] - 7s 315us/step - loss: 0.0656 - dense_8_loss: 0.0338 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198961378728618\nEpoch 51/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0658 - dense_8_loss: 0.0338 - dense_9_loss: 0.0328 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7212919021346592\nEpoch 52/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215588174545846\nEpoch 53/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0655 - dense_8_loss: 0.0336 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215590623574577\nEpoch 54/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0658 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7221633941649979\nEpoch 55/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7209675080322032\nEpoch 56/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0656 - dense_8_loss: 0.0326 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207167713096331\nEpoch 57/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0337 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201083983814882\nEpoch 58/1000\n21560/21560 [==============================] - 8s 360us/step - loss: 0.0657 - dense_8_loss: 0.0336 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195342691585518\nEpoch 59/1000\n21560/21560 [==============================] - 7s 330us/step - loss: 0.0656 - dense_8_loss: 0.0330 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7194607414490689\nEpoch 60/1000\n21560/21560 [==============================] - 7s 332us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7208524119061969\nEpoch 61/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7208268249805384\nEpoch 62/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0660 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7209826762433901\nEpoch 63/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0658 - dense_8_loss: 0.0338 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202556729636271\nEpoch 64/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0658 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7190618396325695\nEpoch 65/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0657 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\n","name":"stdout"},{"output_type":"stream","text":"Val Jaccard Similarity: 0.7191655118886265\nEpoch 66/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0658 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7190854199426114\nEpoch 67/1000\n21560/21560 [==============================] - 7s 342us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7184766584505693\nEpoch 68/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0658 - dense_8_loss: 0.0332 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7188523561376191\nEpoch 69/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719889373169572\nEpoch 70/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203971089113882\nEpoch 71/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0660 - dense_8_loss: 0.0337 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192904568746009\nEpoch 72/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0657 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187149328711753\nEpoch 73/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7179127706829721\nEpoch 74/1000\n21560/21560 [==============================] - 7s 332us/step - loss: 0.0658 - dense_8_loss: 0.0329 - dense_9_loss: 0.0327 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186254323770065\nEpoch 75/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718482086106315\nEpoch 76/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0658 - dense_8_loss: 0.0335 - dense_9_loss: 0.0323 - val_loss: 0.0682 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7190222961466567\nEpoch 77/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0658 - dense_8_loss: 0.0337 - dense_9_loss: 0.0321 - val_loss: 0.0676 - val_dense_8_loss: 0.0339 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7181809270710577\nEpoch 78/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0329 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193144152562193\nEpoch 79/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0657 - dense_8_loss: 0.0336 - dense_9_loss: 0.0327 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193870084508888\nEpoch 80/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0657 - dense_8_loss: 0.0336 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7188872555569676\nEpoch 81/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201667886542836\nEpoch 82/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192692849204364\nEpoch 83/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0329 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192616435620007\nEpoch 84/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0657 - dense_8_loss: 0.0328 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196331071056612\nEpoch 85/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7182842745528156\nEpoch 86/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7199286711313375\nEpoch 87/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.72045600449031\nEpoch 88/1000\n21560/21560 [==============================] - 7s 346us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7212185532221016\nEpoch 89/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215272811691537\nEpoch 90/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189293113407913\nEpoch 91/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0657 - dense_8_loss: 0.0338 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7182180881361734\nEpoch 92/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186487125415614\nEpoch 93/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0656 - dense_8_loss: 0.0340 - dense_9_loss: 0.0318 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201042449415858\nEpoch 94/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7196008612949243\nEpoch 95/1000\n21560/21560 [==============================] - 7s 341us/step - loss: 0.0658 - dense_8_loss: 0.0337 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197584499959092\nEpoch 96/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0657 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7201619262077451\nEpoch 97/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0317 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193436260259597\nEpoch 98/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 317us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7184431227844158\nEpoch 99/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0658 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7180052261602964\nEpoch 100/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0653 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190393207368677\nEpoch 101/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0657 - dense_8_loss: 0.0331 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198568581079412\nEpoch 102/1000\n21560/21560 [==============================] - 7s 333us/step - loss: 0.0656 - dense_8_loss: 0.0337 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201817144944265\nEpoch 103/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0333 - dense_9_loss: 0.0316 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719985739771638\nEpoch 104/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7200454373471084\nEpoch 105/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0659 - dense_8_loss: 0.0329 - dense_9_loss: 0.0318 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.719654869833454\nEpoch 106/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0335 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.718975438934967\nEpoch 107/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0657 - dense_8_loss: 0.0333 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7196136605650237\nEpoch 108/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719244110934241\nEpoch 109/1000\n21560/21560 [==============================] - 7s 339us/step - loss: 0.0654 - dense_8_loss: 0.0336 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195377479635896\nEpoch 110/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0657 - dense_8_loss: 0.0337 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197234653933459\nEpoch 111/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190088400402762\nEpoch 112/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192422466616956\nEpoch 113/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7172541645862823\nEpoch 114/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0658 - dense_8_loss: 0.0335 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7172129795952709\nEpoch 115/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0682 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7188222696106336\nEpoch 116/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0653 - dense_8_loss: 0.0332 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195494881217641\nEpoch 117/1000\n21560/21560 [==============================] - 7s 343us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203155302848855\nEpoch 118/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0660 - dense_8_loss: 0.0338 - dense_9_loss: 0.0328 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7221909951083838\nEpoch 119/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7213582725090999\nEpoch 120/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0658 - dense_8_loss: 0.0335 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195584921571206\nEpoch 121/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197407425958247\nEpoch 122/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0658 - dense_8_loss: 0.0338 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7212046364394504\nEpoch 123/1000\n21560/21560 [==============================] - 7s 333us/step - loss: 0.0652 - dense_8_loss: 0.0328 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215601049028906\nEpoch 124/1000\n21560/21560 [==============================] - 7s 343us/step - loss: 0.0659 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7213936799528384\nEpoch 125/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7216339334317189\nEpoch 126/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.721284795457855\nEpoch 127/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7209802617396948\nEpoch 128/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7202686252046476\nEpoch 129/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205203127578891\nEpoch 130/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 331us/step - loss: 0.0657 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7210539788952826\nEpoch 131/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7206163759041526\nEpoch 132/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7188011727101089\nEpoch 133/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7181742200519107\nEpoch 134/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0657 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718831617469365\nEpoch 135/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0658 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182054401326026\nEpoch 136/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0337 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191411012143343\nEpoch 137/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7199343400964843\nEpoch 138/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0327 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7198985986546377\nEpoch 139/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0659 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7208292155469846\nEpoch 140/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7216592640518208\nEpoch 141/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207073690749342\nEpoch 142/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0652 - dense_8_loss: 0.0328 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203137463483787\nEpoch 143/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0314 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7203855294682324\nEpoch 144/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0337 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7206380410580512\nEpoch 145/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204987507601647\nEpoch 146/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0326 - dense_9_loss: 0.0316 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7196801797113305\nEpoch 147/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7188197782489922\nEpoch 148/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0337 - dense_9_loss: 0.0328 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191647801391355\nEpoch 149/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.71909559473872\nEpoch 150/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7184372632641424\nEpoch 151/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0659 - dense_8_loss: 0.0332 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7183212904009469\nEpoch 152/1000\n21560/21560 [==============================] - 7s 340us/step - loss: 0.0656 - dense_8_loss: 0.0327 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182723280885774\nEpoch 153/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0653 - dense_8_loss: 0.0327 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197334132190342\nEpoch 154/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0658 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7193871850394415\nEpoch 155/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0656 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204719574212456\nEpoch 156/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0656 - dense_8_loss: 0.0338 - dense_9_loss: 0.0326 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7206032148407636\nEpoch 157/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7207212238299521\nEpoch 158/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196913797401712\nEpoch 159/1000\n21560/21560 [==============================] - 8s 350us/step - loss: 0.0656 - dense_8_loss: 0.0330 - dense_9_loss: 0.0318 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195187394250562\nEpoch 160/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207661573444947\nEpoch 161/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7220610708779559\nEpoch 162/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0318 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7217389886107783\nEpoch 163/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197736219091733\nEpoch 164/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7189055033547549\nEpoch 165/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0328 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.720073337201936\nEpoch 166/1000\n21560/21560 [==============================] - 7s 342us/step - loss: 0.0657 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7192297705177779\nEpoch 167/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0658 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192614583042876\nEpoch 168/1000\n21560/21560 [==============================] - 12s 535us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7185886926571527\nEpoch 169/1000\n21560/21560 [==============================] - 10s 484us/step - loss: 0.0658 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7197920430769527\nEpoch 170/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7208538941728951\nEpoch 171/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0340 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7205873866431571\nEpoch 172/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204095114527831\nEpoch 173/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0652 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186616438349216\nEpoch 174/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7176736220921115\nEpoch 175/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7174752048450769\nEpoch 176/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0657 - dense_8_loss: 0.0328 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.717490237204352\nEpoch 177/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7187268002718539\nEpoch 178/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198659368726951\nEpoch 179/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7204519953084036\nEpoch 180/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.719934965146949\nEpoch 181/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0654 - dense_8_loss: 0.0337 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7199349638278856\nEpoch 182/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0659 - dense_8_loss: 0.0336 - dense_9_loss: 0.0328 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191246469179254\nEpoch 183/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0326 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7189675453398126\nEpoch 184/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0329 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.719776310266471\nEpoch 185/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0654 - dense_8_loss: 0.0339 - dense_9_loss: 0.0326 - val_loss: 0.0677 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7196766152976738\nEpoch 186/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7207211376166837\nEpoch 187/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215385313165695\nEpoch 188/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0658 - dense_8_loss: 0.0336 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7205398828417515\nEpoch 189/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7209310889131826\nEpoch 190/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0337 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204164815366704\nEpoch 191/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195552274925537\nEpoch 192/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190435711207979\nEpoch 193/1000\n21560/21560 [==============================] - 7s 342us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202397388498984\nEpoch 194/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 338us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201178653631746\nEpoch 195/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0653 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7203799177098987\nEpoch 196/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7203106257010421\nEpoch 197/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7210543073796513\nEpoch 198/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0657 - dense_8_loss: 0.0337 - dense_9_loss: 0.0326 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7201000336307144\nEpoch 199/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187758636716587\nEpoch 200/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0652 - dense_8_loss: 0.0328 - dense_9_loss: 0.0315 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7193771629955629\nEpoch 201/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0655 - dense_8_loss: 0.0337 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201816392872973\nEpoch 202/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0328 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196794732282848\nEpoch 203/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192336627511361\nEpoch 204/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191029202707548\nEpoch 205/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193480992231447\nEpoch 206/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0329 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185820962894099\nEpoch 207/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0656 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190405910317956\nEpoch 208/1000\n21560/21560 [==============================] - 7s 341us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718112518947842\nEpoch 209/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.717749283962118\nEpoch 210/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0658 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719248365198768\nEpoch 211/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719689326562442\nEpoch 212/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0317 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7191394535505078\nEpoch 213/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196329636083285\nEpoch 214/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0658 - dense_8_loss: 0.0334 - dense_9_loss: 0.0328 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7193005181960304\nEpoch 215/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0655 - dense_8_loss: 0.0326 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196611130239429\nEpoch 216/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0655 - dense_8_loss: 0.0338 - dense_9_loss: 0.0330 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191555195359911\nEpoch 217/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7196216268841729\nEpoch 218/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7205172225232882\nEpoch 219/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0338 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7194335485298314\nEpoch 220/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181878838313278\nEpoch 221/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718021307221326\nEpoch 222/1000\n21560/21560 [==============================] - 7s 335us/step - loss: 0.0654 - dense_8_loss: 0.0330 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7187070565093696\nEpoch 223/1000\n21560/21560 [==============================] - 7s 332us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196555398788174\nEpoch 224/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7195139913358286\nEpoch 225/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0327 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202662186990332\nEpoch 226/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0319 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7205995415159073\nEpoch 227/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0331 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7191039037465432\nEpoch 228/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191595643487545\nEpoch 229/1000\n21560/21560 [==============================] - 7s 344us/step - loss: 0.0656 - dense_8_loss: 0.0328 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7210052445142761\nEpoch 230/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0653 - dense_8_loss: 0.0333 - dense_9_loss: 0.0322 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.721925551232692\nEpoch 231/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202595166013577\nEpoch 232/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0335 - dense_9_loss: 0.0329 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.718931614434236\nEpoch 233/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7186340150751042\nEpoch 234/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7194792856973267\nEpoch 235/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0318 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7199929604960339\nEpoch 236/1000\n21560/21560 [==============================] - 7s 346us/step - loss: 0.0652 - dense_8_loss: 0.0334 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7184087355772997\nEpoch 237/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7195931862237778\nEpoch 238/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0660 - dense_8_loss: 0.0336 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.719515541990582\nEpoch 239/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205078788216239\nEpoch 240/1000\n21560/21560 [==============================] - 12s 570us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181944143204466\nEpoch 241/1000\n21560/21560 [==============================] - 13s 596us/step - loss: 0.0656 - dense_8_loss: 0.0335 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187227545001602\nEpoch 242/1000\n21560/21560 [==============================] - 8s 348us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203971522937564\nEpoch 243/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0327 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.720596919348132\nEpoch 244/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7207875766210157\nEpoch 245/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0654 - dense_8_loss: 0.0336 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207434664436025\nEpoch 246/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201795436376123\nEpoch 247/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0329 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.720935277094092\nEpoch 248/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0659 - dense_8_loss: 0.0329 - dense_9_loss: 0.0323 - val_loss: 0.0676 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0336\nVal Jaccard Similarity: 0.7191144254569722\nEpoch 249/1000\n21560/21560 [==============================] - 7s 340us/step - loss: 0.0655 - dense_8_loss: 0.0327 - dense_9_loss: 0.0313 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196033359335401\nEpoch 250/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204139390604747\nEpoch 251/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203843853922648\nEpoch 252/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0336 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7213387175968679\nEpoch 253/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0657 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7209641795472046\nEpoch 254/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7215667094303536\nEpoch 255/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7208783533403085\nEpoch 256/1000\n21560/21560 [==============================] - 7s 345us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197137092445229\nEpoch 257/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0329 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719320681115583\nEpoch 258/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0337 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191067425697113\nEpoch 259/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0652 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207119456048495\nEpoch 260/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7214693392048491\nEpoch 261/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205863937327434\nEpoch 262/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.722121689588768\nEpoch 263/1000\n21560/21560 [==============================] - 7s 345us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196378604108485\nEpoch 264/1000\n21560/21560 [==============================] - 7s 333us/step - loss: 0.0654 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7177363518025738\nEpoch 265/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7181631405352885\nEpoch 266/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7197924046064784\nEpoch 267/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0657 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.719522301501959\nEpoch 268/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7194036223072986\nEpoch 269/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7183442159726486\nEpoch 270/1000\n21560/21560 [==============================] - 7s 341us/step - loss: 0.0654 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181219796014425\nEpoch 271/1000\n21560/21560 [==============================] - 10s 453us/step - loss: 0.0658 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0682 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7190654887805693\nEpoch 272/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0329 - dense_9_loss: 0.0318 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7173332602495883\nEpoch 273/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718011431028875\nEpoch 274/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7189602617032758\nEpoch 275/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0654 - dense_8_loss: 0.0338 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191616216690666\nEpoch 276/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0318 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204414527356953\nEpoch 277/1000\n21560/21560 [==============================] - 7s 339us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.719090947798644\nEpoch 278/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0657 - dense_8_loss: 0.0336 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7164641243200587\nEpoch 279/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7175967000236113\nEpoch 280/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0653 - dense_8_loss: 0.0328 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7176966271232097\nEpoch 281/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192067757563314\nEpoch 282/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0329 - dense_9_loss: 0.0318 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7207636798676288\nEpoch 283/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7208978350454706\nEpoch 284/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207157101028987\nEpoch 285/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.720425003789951\nEpoch 286/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7211333334108403\nEpoch 287/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204389980260473\nEpoch 288/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.720679918456242\nEpoch 289/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0325 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7201831549681329\nEpoch 290/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7210718182423491\nEpoch 291/1000\n21560/21560 [==============================] - 7s 343us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7211089816236075\nEpoch 292/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7212195681103768\nEpoch 293/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0328 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7212380458792489\nEpoch 294/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7216016177523223\nEpoch 295/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.721280960126854\nEpoch 296/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0329 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7221261111634429\nEpoch 297/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7212370126719243\nEpoch 298/1000\n21560/21560 [==============================] - 8s 349us/step - loss: 0.0652 - dense_8_loss: 0.0331 - dense_9_loss: 0.0318 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203776003673227\nEpoch 299/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0655 - dense_8_loss: 0.0329 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198148059875928\nEpoch 300/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0327 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196232752644747\nEpoch 301/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0325 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7197980234982403\nEpoch 302/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0656 - dense_8_loss: 0.0335 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187248337168792\nEpoch 303/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7190945085848621\nEpoch 304/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7194121320646044\nEpoch 305/1000\n21560/21560 [==============================] - 7s 339us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7208313768919818\nEpoch 306/1000\n21560/21560 [==============================] - 7s 330us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718916007148324\nEpoch 307/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7179219341168074\nEpoch 308/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7183500851660153\nEpoch 309/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0651 - dense_8_loss: 0.0330 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7198027839336941\nEpoch 310/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.720257019153123\nEpoch 311/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7186198530889235\nEpoch 312/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0653 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189698535460621\nEpoch 313/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202257919964771\nEpoch 314/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0329 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.720620396220737\nEpoch 315/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186171765026119\nEpoch 316/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0329 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185973815858046\nEpoch 317/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0656 - dense_8_loss: 0.0337 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7188773667811961\nEpoch 318/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7181686547590058\nEpoch 319/1000\n21560/21560 [==============================] - 7s 332us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7185278415591397\nEpoch 320/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0652 - dense_8_loss: 0.0326 - dense_9_loss: 0.0320 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7189545493005163\nEpoch 321/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0656 - dense_8_loss: 0.0329 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7203058100906214\nEpoch 322/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7214891750276365\nEpoch 323/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0651 - dense_8_loss: 0.0326 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7208804241324835\nEpoch 324/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7204523605918638\nEpoch 325/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0657 - dense_8_loss: 0.0338 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190712719462082\nEpoch 326/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0318 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7171746263570016\nEpoch 327/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7175959599013636\nEpoch 328/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182525640807383\nEpoch 329/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201090798682337\nEpoch 330/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0336 - dense_9_loss: 0.0325 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7208341789397531\nEpoch 331/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7202924717488935\nEpoch 332/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0336 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7194415377811577\nEpoch 333/1000\n21560/21560 [==============================] - 7s 344us/step - loss: 0.0652 - dense_8_loss: 0.0333 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7208179635956748\nEpoch 334/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0657 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7209506516801487\nEpoch 335/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0337 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205721470234242\nEpoch 336/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7199477046075969\nEpoch 337/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197965545242184\nEpoch 338/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204926497989085\nEpoch 339/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0650 - dense_8_loss: 0.0334 - dense_9_loss: 0.0319 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7197273132981618\nEpoch 340/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7195213654424876\nEpoch 341/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190857334694155\nEpoch 342/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0656 - dense_8_loss: 0.0328 - dense_9_loss: 0.0319 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193741893178082\nEpoch 343/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0654 - dense_8_loss: 0.0329 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192187566998361\nEpoch 344/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0656 - dense_8_loss: 0.0340 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181206752811575\nEpoch 345/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7177503960433671\nEpoch 346/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0653 - dense_8_loss: 0.0329 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718133490245879\nEpoch 347/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0652 - dense_8_loss: 0.0328 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7192270846261455\nEpoch 348/1000\n21560/21560 [==============================] - 7s 341us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7192759433500828\nEpoch 349/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7190760710946078\nEpoch 350/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0655 - dense_8_loss: 0.0329 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7175125372807724\nEpoch 351/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0327 - dense_9_loss: 0.0318 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.717408471544168\nEpoch 352/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186274717884811\nEpoch 353/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195024789792025\nEpoch 354/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 317us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189545891672415\nEpoch 355/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7200597786485111\nEpoch 356/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0328 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718647199841614\nEpoch 357/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0333 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7180979917005117\nEpoch 358/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0337 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.717684560421544\nEpoch 359/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7166559504187381\nEpoch 360/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7175059548136549\nEpoch 361/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7179012035505552\nEpoch 362/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0653 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7178329883331241\nEpoch 363/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0653 - dense_8_loss: 0.0328 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.718413868192283\nEpoch 364/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7193749102765723\nEpoch 365/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0336 - dense_9_loss: 0.0325 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198002104042808\nEpoch 366/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0652 - dense_8_loss: 0.0329 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186450569296322\nEpoch 367/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186805648369889\nEpoch 368/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0330 - dense_9_loss: 0.0317 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196759395853368\nEpoch 369/1000\n21560/21560 [==============================] - 8s 350us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7178910141500717\nEpoch 370/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187765377080593\nEpoch 371/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193299703830588\nEpoch 372/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0337 - dense_9_loss: 0.0326 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7203357518897046\nEpoch 373/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195434716947668\nEpoch 374/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186636694406374\nEpoch 375/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207470401578174\nEpoch 376/1000\n21560/21560 [==============================] - 7s 344us/step - loss: 0.0654 - dense_8_loss: 0.0329 - dense_9_loss: 0.0322 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7206443337008075\nEpoch 377/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7179328388999214\nEpoch 378/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0651 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7184541555088141\nEpoch 379/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186512080619395\nEpoch 380/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0653 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7191391699963764\nEpoch 381/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0339 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193895541389193\nEpoch 382/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0651 - dense_8_loss: 0.0331 - dense_9_loss: 0.0318 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7195972577559496\nEpoch 383/1000\n21560/21560 [==============================] - 7s 342us/step - loss: 0.0653 - dense_8_loss: 0.0329 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7188919266779843\nEpoch 384/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0654 - dense_8_loss: 0.0344 - dense_9_loss: 0.0328 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7178650410648677\nEpoch 385/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0652 - dense_8_loss: 0.0329 - dense_9_loss: 0.0327 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7192675167523456\nEpoch 386/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 329us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192928682702053\nEpoch 387/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7207309896036643\nEpoch 388/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0655 - dense_8_loss: 0.0326 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7195605897615287\nEpoch 389/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0652 - dense_8_loss: 0.0335 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.718665607149647\nEpoch 390/1000\n21560/21560 [==============================] - 7s 343us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182816643709565\nEpoch 391/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0652 - dense_8_loss: 0.0328 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181881631705465\nEpoch 392/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0656 - dense_8_loss: 0.0329 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.718254993650603\nEpoch 393/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0651 - dense_8_loss: 0.0329 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7188140250101582\nEpoch 394/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0656 - dense_8_loss: 0.0338 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7184180838075537\nEpoch 395/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7184264333307838\nEpoch 396/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0328 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189746429883455\nEpoch 397/1000\n21560/21560 [==============================] - 7s 332us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185475570422287\nEpoch 398/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0652 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182211513530469\nEpoch 399/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7176813302923731\nEpoch 400/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0329 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182140763417699\nEpoch 401/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0334 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719319827193531\nEpoch 402/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0652 - dense_8_loss: 0.0329 - dense_9_loss: 0.0318 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7222001331054824\nEpoch 403/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7228347368396628\nEpoch 404/1000\n21560/21560 [==============================] - 7s 338us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7210419494951363\nEpoch 405/1000\n21560/21560 [==============================] - 7s 345us/step - loss: 0.0654 - dense_8_loss: 0.0329 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.719943667392786\nEpoch 406/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7176710340288962\nEpoch 407/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0317 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7182753672498657\nEpoch 408/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0338 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181878284783026\nEpoch 409/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189452715558851\nEpoch 410/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0319 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187376476583129\nEpoch 411/1000\n21560/21560 [==============================] - 7s 332us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0326 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.719796905398242\nEpoch 412/1000\n21560/21560 [==============================] - 7s 335us/step - loss: 0.0653 - dense_8_loss: 0.0328 - dense_9_loss: 0.0317 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7204841131173872\nEpoch 413/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197089556051263\nEpoch 414/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203020734202468\nEpoch 415/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7194107460447351\nEpoch 416/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196984413935504\nEpoch 417/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0653 - dense_8_loss: 0.0337 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181615448986375\nEpoch 418/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 332us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0319 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7199684421355531\nEpoch 419/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7198070658270792\nEpoch 420/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0317 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187744070221613\nEpoch 421/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0318 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192646513308929\nEpoch 422/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0652 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204228363810342\nEpoch 423/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.720193436382001\nEpoch 424/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0657 - dense_8_loss: 0.0334 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7187502493398599\nEpoch 425/1000\n21560/21560 [==============================] - 7s 332us/step - loss: 0.0654 - dense_8_loss: 0.0329 - dense_9_loss: 0.0318 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189297494254188\nEpoch 426/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7205871154558132\nEpoch 427/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0336 - dense_9_loss: 0.0327 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201238028339003\nEpoch 428/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0654 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7200103082247891\nEpoch 429/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0651 - dense_8_loss: 0.0332 - dense_9_loss: 0.0320 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7204683666325818\nEpoch 430/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197950242929866\nEpoch 431/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0327 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7191339995191858\nEpoch 432/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201696809958759\nEpoch 433/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7210079232096356\nEpoch 434/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7204639911257611\nEpoch 435/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0655 - dense_8_loss: 0.0329 - dense_9_loss: 0.0324 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7189975499318969\nEpoch 436/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0329 - dense_9_loss: 0.0318 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7173388724788684\nEpoch 437/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0337 - dense_9_loss: 0.0327 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7175828910209736\nEpoch 438/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0651 - dense_8_loss: 0.0329 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7179412880486397\nEpoch 439/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7178837042759039\nEpoch 440/1000\n21560/21560 [==============================] - 7s 338us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7169613873601236\nEpoch 441/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7183746901113369\nEpoch 442/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0654 - dense_8_loss: 0.0327 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193077883956615\nEpoch 443/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0652 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185297017047544\nEpoch 444/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0317 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7187553949192803\nEpoch 445/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0656 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203167948503385\nEpoch 446/1000\n21560/21560 [==============================] - 7s 317us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0327 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7198647617678129\nEpoch 447/1000\n21560/21560 [==============================] - 7s 345us/step - loss: 0.0655 - dense_8_loss: 0.0336 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7180943606777428\nEpoch 448/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0656 - dense_8_loss: 0.0331 - dense_9_loss: 0.0326 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191072496116044\nEpoch 449/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7184742938472772\nEpoch 450/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7181731606998976\nEpoch 451/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191058400810875\nEpoch 452/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0652 - dense_8_loss: 0.0335 - dense_9_loss: 0.0327 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.720172348890109\nEpoch 453/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7192230312776207\nEpoch 454/1000\n21560/21560 [==============================] - 8s 349us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182583572351896\nEpoch 455/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0652 - dense_8_loss: 0.0324 - dense_9_loss: 0.0315 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7188527778909823\nEpoch 456/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0657 - dense_8_loss: 0.0338 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7189097804470367\nEpoch 457/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190789319434677\nEpoch 458/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191530730322163\nEpoch 459/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7189965111668615\nEpoch 460/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7172703772204048\nEpoch 461/1000\n21560/21560 [==============================] - 8s 348us/step - loss: 0.0655 - dense_8_loss: 0.0339 - dense_9_loss: 0.0329 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7183554705768138\nEpoch 462/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0651 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0676 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0336\nVal Jaccard Similarity: 0.7185665465795563\nEpoch 463/1000\n21560/21560 [==============================] - 7s 316us/step - loss: 0.0656 - dense_8_loss: 0.0329 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7180905044293252\nEpoch 464/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0653 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0677 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7179282467292346\nEpoch 465/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7188631011020054\nEpoch 466/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182204110832111\nEpoch 467/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7188478986069714\nEpoch 468/1000\n21560/21560 [==============================] - 7s 336us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185683222369857\nEpoch 469/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7187331814022393\nEpoch 470/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0652 - dense_8_loss: 0.0333 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7183399299489353\nEpoch 471/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0652 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186165884365632\nEpoch 472/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7184755660356643\nEpoch 473/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0654 - dense_8_loss: 0.0339 - dense_9_loss: 0.0327 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7180165475938071\nEpoch 474/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.71797070213896\nEpoch 475/1000\n21560/21560 [==============================] - 7s 337us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189803880499797\nEpoch 476/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0650 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.719313846107497\nEpoch 477/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7189391338381542\nEpoch 478/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0652 - dense_8_loss: 0.0334 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7178916906023912\nEpoch 479/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0653 - dense_8_loss: 0.0335 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718893064134578\nEpoch 480/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7199939056534072\nEpoch 481/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0327 - dense_9_loss: 0.0323 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193818939348593\nEpoch 482/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 337us/step - loss: 0.0652 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190898485827038\nEpoch 483/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0657 - dense_8_loss: 0.0338 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7201767810180641\nEpoch 484/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0652 - dense_8_loss: 0.0329 - dense_9_loss: 0.0322 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7203188502367347\nEpoch 485/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0323 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7196724962300731\nEpoch 486/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7199312931975693\nEpoch 487/1000\n21560/21560 [==============================] - 8s 353us/step - loss: 0.0652 - dense_8_loss: 0.0333 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205359218569057\nEpoch 488/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7197547392440985\nEpoch 489/1000\n21560/21560 [==============================] - 7s 340us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.721296218522266\nEpoch 490/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0652 - dense_8_loss: 0.0333 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7206748150362309\nEpoch 491/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0652 - dense_8_loss: 0.0327 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7213516250667682\nEpoch 492/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0656 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201640331665817\nEpoch 493/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0327 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7194958831298346\nEpoch 494/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0652 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7204088748306396\nEpoch 495/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0328 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.720421663958005\nEpoch 496/1000\n21560/21560 [==============================] - 7s 335us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0319 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7200807801086593\nEpoch 497/1000\n21560/21560 [==============================] - 7s 346us/step - loss: 0.0652 - dense_8_loss: 0.0333 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0339 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7208126890410081\nEpoch 498/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7182617470788726\nEpoch 499/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7182710843701735\nEpoch 500/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204295387214105\nEpoch 501/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0656 - dense_8_loss: 0.0336 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7207152738053354\nEpoch 502/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0329 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196480035418819\nEpoch 503/1000\n21560/21560 [==============================] - 7s 331us/step - loss: 0.0655 - dense_8_loss: 0.0329 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718803315621983\nEpoch 504/1000\n21560/21560 [==============================] - 7s 341us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.716613622537139\nEpoch 505/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0328 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7176652505417648\nEpoch 506/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0657 - dense_8_loss: 0.0335 - dense_9_loss: 0.0328 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7180774257833319\nEpoch 507/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7183902865134025\nEpoch 508/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181984370597649\nEpoch 509/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0327 - dense_9_loss: 0.0320 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7187075891560621\nEpoch 510/1000\n21560/21560 [==============================] - 7s 334us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0328 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191081825136023\nEpoch 511/1000\n21560/21560 [==============================] - 7s 339us/step - loss: 0.0656 - dense_8_loss: 0.0333 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7193840048626193\nEpoch 512/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.718775150817184\nEpoch 513/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0326 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7168893966430899\nEpoch 514/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 323us/step - loss: 0.0655 - dense_8_loss: 0.0327 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7160967325386823\nEpoch 515/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0320 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7170263075541713\nEpoch 516/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0652 - dense_8_loss: 0.0332 - dense_9_loss: 0.0317 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7184489244330416\nEpoch 517/1000\n21560/21560 [==============================] - 7s 342us/step - loss: 0.0653 - dense_8_loss: 0.0329 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185919136730341\nEpoch 518/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0653 - dense_8_loss: 0.0329 - dense_9_loss: 0.0323 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7171460394409167\nEpoch 519/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0650 - dense_8_loss: 0.0327 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7177625559048727\nEpoch 520/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0653 - dense_8_loss: 0.0333 - dense_9_loss: 0.0319 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7172258101538749\nEpoch 521/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0652 - dense_8_loss: 0.0329 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7196994610507987\nEpoch 522/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0651 - dense_8_loss: 0.0330 - dense_9_loss: 0.0319 - val_loss: 0.0681 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.720528995023077\nEpoch 523/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0334 - dense_9_loss: 0.0327 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193974640435673\nEpoch 524/1000\n21560/21560 [==============================] - 7s 340us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0318 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7196516530404496\nEpoch 525/1000\n21560/21560 [==============================] - 7s 341us/step - loss: 0.0651 - dense_8_loss: 0.0331 - dense_9_loss: 0.0323 - val_loss: 0.0682 - val_dense_8_loss: 0.0344 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.719475112513378\nEpoch 526/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0652 - dense_8_loss: 0.0329 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185443116035788\nEpoch 527/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0320 - val_loss: 0.0682 - val_dense_8_loss: 0.0343 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7181173208546767\nEpoch 528/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0652 - dense_8_loss: 0.0327 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.718548484504802\nEpoch 529/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0654 - dense_8_loss: 0.0336 - dense_9_loss: 0.0322 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7186135648460422\nEpoch 530/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0655 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7169085671920601\nEpoch 531/1000\n21560/21560 [==============================] - 7s 328us/step - loss: 0.0655 - dense_8_loss: 0.0332 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7174110519503749\nEpoch 532/1000\n21560/21560 [==============================] - 7s 339us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7190808810743383\nEpoch 533/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0654 - dense_8_loss: 0.0336 - dense_9_loss: 0.0322 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7200042620267744\nEpoch 534/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0654 - dense_8_loss: 0.0331 - dense_9_loss: 0.0322 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7203057638168409\nEpoch 535/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0652 - dense_8_loss: 0.0333 - dense_9_loss: 0.0323 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0336\nVal Jaccard Similarity: 0.7176477695775125\nEpoch 536/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0655 - dense_8_loss: 0.0333 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7178004881914485\nEpoch 537/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0651 - dense_8_loss: 0.0334 - dense_9_loss: 0.0319 - val_loss: 0.0677 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7175660711988755\nEpoch 538/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0319 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7191715362924644\nEpoch 539/1000\n21560/21560 [==============================] - 7s 340us/step - loss: 0.0653 - dense_8_loss: 0.0327 - dense_9_loss: 0.0318 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7197472126453697\nEpoch 540/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0652 - dense_8_loss: 0.0330 - dense_9_loss: 0.0322 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7196052862493286\nEpoch 541/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0652 - dense_8_loss: 0.0327 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201242162421198\nEpoch 542/1000\n21560/21560 [==============================] - 7s 318us/step - loss: 0.0656 - dense_8_loss: 0.0335 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7195946720985394\nEpoch 543/1000\n21560/21560 [==============================] - 7s 324us/step - loss: 0.0651 - dense_8_loss: 0.0330 - dense_9_loss: 0.0328 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7195890239046939\nEpoch 544/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0652 - dense_8_loss: 0.0332 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7199006190013835\nEpoch 545/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185841138953464\nEpoch 546/1000\n","name":"stdout"},{"output_type":"stream","text":"21560/21560 [==============================] - 7s 335us/step - loss: 0.0654 - dense_8_loss: 0.0328 - dense_9_loss: 0.0320 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7201500162667661\nEpoch 547/1000\n21560/21560 [==============================] - 7s 329us/step - loss: 0.0654 - dense_8_loss: 0.0336 - dense_9_loss: 0.0326 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7201724589280843\nEpoch 548/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0318 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7188303349018377\nEpoch 549/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0654 - dense_8_loss: 0.0334 - dense_9_loss: 0.0323 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191477915056437\nEpoch 550/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0651 - dense_8_loss: 0.0333 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7199561053190374\nEpoch 551/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0650 - dense_8_loss: 0.0330 - dense_9_loss: 0.0318 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7202873828779621\nEpoch 552/1000\n21560/21560 [==============================] - 7s 326us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7175074520808291\nEpoch 553/1000\n21560/21560 [==============================] - 7s 343us/step - loss: 0.0656 - dense_8_loss: 0.0332 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7173081013431621\nEpoch 554/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0656 - dense_8_loss: 0.0334 - dense_9_loss: 0.0325 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7175345979896812\nEpoch 555/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0653 - dense_8_loss: 0.0330 - dense_9_loss: 0.0324 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7181361870818175\nEpoch 556/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0333 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7193642334638345\nEpoch 557/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0652 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7199516693322364\nEpoch 558/1000\n21560/21560 [==============================] - 7s 321us/step - loss: 0.0651 - dense_8_loss: 0.0330 - dense_9_loss: 0.0321 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7203533320304748\nEpoch 559/1000\n21560/21560 [==============================] - 7s 319us/step - loss: 0.0654 - dense_8_loss: 0.0339 - dense_9_loss: 0.0328 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0337\nVal Jaccard Similarity: 0.7191364062933457\nEpoch 560/1000\n21560/21560 [==============================] - 7s 346us/step - loss: 0.0654 - dense_8_loss: 0.0335 - dense_9_loss: 0.0324 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7204985145436554\nEpoch 561/1000\n21560/21560 [==============================] - 7s 330us/step - loss: 0.0655 - dense_8_loss: 0.0330 - dense_9_loss: 0.0325 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.720160000188288\nEpoch 562/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.718643640096329\nEpoch 563/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0334 - dense_9_loss: 0.0322 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7191232153729104\nEpoch 564/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0655 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7189864187593383\nEpoch 565/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0651 - dense_8_loss: 0.0331 - dense_9_loss: 0.0320 - val_loss: 0.0680 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7180842448583472\nEpoch 566/1000\n21560/21560 [==============================] - 7s 327us/step - loss: 0.0652 - dense_8_loss: 0.0329 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7185944366400193\nEpoch 567/1000\n21560/21560 [==============================] - 7s 346us/step - loss: 0.0651 - dense_8_loss: 0.0329 - dense_9_loss: 0.0320 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7186814939628808\nEpoch 568/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0319 - val_loss: 0.0681 - val_dense_8_loss: 0.0342 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7195479037199605\nEpoch 569/1000\n21560/21560 [==============================] - 7s 322us/step - loss: 0.0650 - dense_8_loss: 0.0329 - dense_9_loss: 0.0317 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0339\nVal Jaccard Similarity: 0.7208315643085772\nEpoch 570/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0653 - dense_8_loss: 0.0331 - dense_9_loss: 0.0319 - val_loss: 0.0678 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7205554941270179\nEpoch 571/1000\n21560/21560 [==============================] - 7s 323us/step - loss: 0.0654 - dense_8_loss: 0.0332 - dense_9_loss: 0.0324 - val_loss: 0.0679 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7201296518673718\nEpoch 572/1000\n21560/21560 [==============================] - 7s 320us/step - loss: 0.0653 - dense_8_loss: 0.0327 - dense_9_loss: 0.0321 - val_loss: 0.0678 - val_dense_8_loss: 0.0340 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7176068787657632\nEpoch 573/1000\n21560/21560 [==============================] - 7s 325us/step - loss: 0.0654 - dense_8_loss: 0.0333 - dense_9_loss: 0.0325 - val_loss: 0.0680 - val_dense_8_loss: 0.0341 - val_dense_9_loss: 0.0338\nVal Jaccard Similarity: 0.7188442826493309\nEpoch 574/1000\n20000/21560 [==========================>...] - ETA: 0s - loss: 0.0652 - dense_8_loss: 0.0331 - dense_9_loss: 0.0321","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Predictions on local test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = model_2.predict([test_manip_X, test_manip_X_sentiment_one_hot_repeated])\npreds = model_2.predict(\n    [\n        test_manip_X, test_numerics_repeated\n    ]\n)\ntemp_out = pd.concat([\n    handlers.get_preds_out(handlers.get_indexes_from_argmax(preds, test_df), test_df), \n    test_df['selected_text']],\n    axis=1\n)\ntemp_out.columns = ['predicted_text', 'selected_text']\n# temp_out['predicted_text']\ntemp_out.apply(lambda x: handlers.jaccard(x['predicted_text'], x['selected_text']), axis=1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Basic Config with Dropout No Stage: 0.7176681846688269\n- Add 200 epochs at 1000 batch: 0.7215144500264917"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Two Stage Result\nneuts = test_df[test_df['sentiment']=='neutral'].reset_index(drop=True)[['text', 'selected_text']]\nneuts.columns = ['predicted_text', 'selected_text']\n\npd.concat(\n    [\n        temp_out[test_df['sentiment']!='neutral'].reset_index(drop=True),\n        neuts\n    ]\n).reset_index(drop=True).apply(lambda x: handlers.jaccard(x['predicted_text'], x['selected_text']), axis=1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Basic Confif with Dropout Static First Stage: 0.7174145968259453\n- Add 200 epochs at 1000 batch: 0.7213928829363386\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"- Check if embedding matrix is correctly created"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Overall Jaccard from Model:')\n# print(test_df_1[['text', 'selected_text', 'out_pred_span']].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\n# print('Overall Jaccard baseline from predicting the complete text:')\n# print(test_df_1[['text', 'selected_text', 'out_pred_span']].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\n# print('Overall Jaccard using model for NOT neutral and baseline for neutral:')\n# print(pd.concat([\n#     test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1),\n#     test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1)\n# ]).mean())\n\n\n# print('Model Jaccard for neutral')\n# print(test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\n# print('Baseline Jaccard for neutral from predicting complete text')\n# print(test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\n# print('Model Jaccard for NOT neutral')\n# print(test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\n# print('Baseline Jaccard for NOT neutral from predicting complete text')\n# print(test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Predictions on Test-Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# (valid_manip_X, valid_numerics_repeated, valid_manip_Y, valid_manip_Y_categorical, valid_df) = data_mainpulation_pipeline.transform(df_objs.df_objects['test'])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = model_2.predict(\n#     [\n#         valid_manip_X, valid_numerics_repeated\n#     ]\n# )\n# df_objs.df_objects['test']['out_pred_span'] = handlers.get_preds_out(handlers.get_indexes_from_argmax(preds, df_objs.df_objects['test']), df_objs.df_objects['test'])\n# final_out = df_objs.df_objects['test'][['textID','out_pred_span']]\n# final_out.columns = ['textID', 'selected_text']","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_out.head(5)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_out.to_csv('submission.csv', index=False)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}