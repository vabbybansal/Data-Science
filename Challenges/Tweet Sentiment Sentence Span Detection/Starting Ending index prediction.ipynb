{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove-twitter/glove.twitter.27B.100d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.200d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.25d.txt\n/kaggle/input/glove-twitter/glove.twitter.27B.50d.txt\n/kaggle/input/tweet-sentiment-extraction/train.csv\n/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load the competetion datasets\n\nclass KaggleReader(object):\n    def __init__(self):\n        self.df_objects = {}        \n    def read_kaggle_df(self, df_name, df_path):\n        self.df_objects[df_name] = pd.read_csv(df_path)\n        print()\n        print(df_name + ' loaded')\n        print(\"Shape => \" + str(self.df_objects[df_name].shape))\n    \ndf_objs = KaggleReader()\ndf_objs.read_kaggle_df('train', \"../input/tweet-sentiment-extraction/train.csv\")\ndf_objs.read_kaggle_df('test', \"../input/tweet-sentiment-extraction/test.csv\")\ndf_objs.read_kaggle_df('submission', \"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":2,"outputs":[{"output_type":"stream","text":"\ntrain loaded\nShape => (27481, 4)\n\ntest loaded\nShape => (3534, 3)\n\nsubmission loaded\nShape => (3534, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom numpy import hstack, vstack\n# Tokenize text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.utils import to_categorical\nfrom nltk.tokenize import WordPunctTokenizer\n\n# Model\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, LSTM, Embedding, Dropout, Input, Bidirectional\nfrom keras.initializers import Constant\nimport keras\nfrom keras.layers.merge import concatenate\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Handlers(object):\n    def __init__(self):\n        self.run_tests()\n        pass\n    \n    def draw_pr_curve_plt(self, Y_valid, y_pred, x_range=1.0):\n    #     (precision, recall, x_range=1.0):\n\n            precision, recall, thresholds_pr = precision_recall_curve(Y_valid, y_pred)\n\n            # import dependencies\n            import matplotlib.pyplot as plt\n\n            plt.step(recall, precision, color='b', alpha=0.2,\n                     where='post')\n            plt.fill_between(recall, precision, alpha=0.2, color='b')\n            plt.xlabel('Recall')\n            plt.ylabel('Precision')\n            plt.ylim([0.0, 1.05])\n            plt.xlim([0.0, x_range])\n            plt.show()\n     \n    def draw_roc_curve_plt(self, Y_valid, y_pred):\n#     (fpr, tpr, auc):\n        # import dependencies\n        import matplotlib.pyplot as plt\n        fpr, tpr, thresholds_roc = roc_curve(Y_valid, y_pred)\n        auc_roc = auc(fpr, tpr)\n        \n        plt.figure(1)\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.plot(fpr, tpr, label='(area = {:.3f})'.format(auc_roc))\n        # plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n        plt.xlabel('False positive rate')\n        plt.ylabel('True positive rate')\n        plt.title('ROC curve')\n        plt.legend(loc='best')\n        plt.show()\n\n    # Optimize using KMP\n    def return_start_end_indices(self, big, small):\n        if len(small) <= 0 or len(big) <= 0:\n            return (-1,-1)\n        i = 0\n        j = 0\n        started = False\n        while i < len(big):\n            if big[i] == small[j]:\n                if started == False:\n                    started = True\n                    i_start = i\n\n                if j == len(small) - 1:\n                    return (i-len(small)+1, i)\n                j += 1\n\n            else:\n                if started == True:\n                    started = False\n                    i = i_start + 1\n                j = 0\n            i += 1\n        return (-1,-1)\n    \n    def run_tests(self):\n        \n        assert self.return_start_end_indices(['i', '`', 'd', 'have', 'responded', ',', 'if', 'i', 'were', 'going'], ['have', 'responded', ',']) == (3,5)\n        assert self.return_start_end_indices('abc', '') == (-1,-1)\n        assert self.return_start_end_indices('abc', 'a') == (0,0)\n        assert self.return_start_end_indices('abc', 'b') == (1,1)\n        assert self.return_start_end_indices('abc', 'c') == (2,2)\n        assert self.return_start_end_indices('abc', 'ab') == (0,1)\n        assert self.return_start_end_indices('abc', 'bc') == (1,2)\n        assert self.return_start_end_indices('abc', 'ac') == (-1,-1)\n        assert self.return_start_end_indices('abc', 'abc') == (0,2)\n        assert self.return_start_end_indices('abcabcabc', 'abc') == (0,2)\n        assert self.return_start_end_indices('ababcc', 'abc') == (2,4)\n        \n    \n    def jaccard(self, str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    \n    def load_gloVe_embeddings(self):\n        # Boilerplate taken from here - https://www.kaggle.com/stacykurnikova/using-glove-embedding\n        embeddings_index = {}\n        f = open('/kaggle/input/glove-twitter/glove.twitter.27B.25d.txt')\n        for line in f:\n            values = line.split(' ')\n            word = values[0] ## The first entry is the word\n            coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n            embeddings_index[word] = coefs\n        f.close()\n        print('GloVe data loaded')\n        return embeddings_index\n    \n    def load_embeddings_matrix(self, embeddings_index, index_tokenizer):\n        # https://www.kaggle.com/stacykurnikova/using-glove-embedding\n        # Create an embedding matrix with embedding vectors for the tokens recognized in the vocab of tweets\n        \n        EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n        # num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n        num_words = len(index_tokenizer.word_index) + 1\n\n        # To Do: constrain the vocab size\n        embedding_matrix = np.random.uniform(-1,+1,(num_words, EMBEDDING_DIM))\n        count_in_embedding_vocab = 0\n        for word, i in index_tokenizer.word_index.items():\n            embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n            if embedding_vector is not None:\n                # words not found in embedding index will be all-zeros.\n                embedding_matrix[i] = embedding_vector\n                count_in_embedding_vocab += 1\n#         TO DO:\n#             else:\n#                 PASS EMBEDDINGS OF SYNONYMS (!!! == !)\n        return embedding_matrix\n        # print(count_in_embedding_vocab)\n        # print(len(embedding_matrix))\n    \n    def get_token_indices(self, x):\n        span_generator = WordPunctTokenizer().span_tokenize(x)\n        spans = [span for span in span_generator]\n        return spans\n    \n    # TO DO: compensate for end and start on top of padding\n    def get_pred_text_span(self, x):\n        start_token_index = int(x['pred_start'])\n        end_token_index = int(x['pred_end'])\n        token_indices_list = x['tokens_indices']\n        text = x['text']\n\n        # start token ind > end token ind (could change logic to also have either 0:end)\n        if start_token_index > end_token_index:\n            return text\n\n        # start token ind and end token ind within bounds\n        elif start_token_index < len(token_indices_list) and end_token_index < len(token_indices_list):\n            return text[token_indices_list[start_token_index][0]: token_indices_list[end_token_index][1]]\n\n\n        # start token ind after bounds (could change logic to also have either 0:end or sth similar)\n        elif start_token_index >= len(token_indices_list):\n            return text\n\n        # only end token ind out of bounds\n        elif start_token_index < len(token_indices_list) and end_token_index >= len(token_indices_list):\n            return text[token_indices_list[start_token_index][0]: len(text)-1]\n        \n    def batch_jaccard(self, preds, test_df):\n        temp_df = pd.concat([test_df, pd.DataFrame(preds, columns=['pred_start', 'pred_end'])], axis=1)\n        temp_df['tokens_indices'] = temp_df['trans_text'].apply(handlers.get_token_indices)\n        # print((temp_df['tokens_indices'].apply(len) == temp_df['tokens'].apply(len)).value_counts())\n\n        # Get the predictions\n        temp_df['out_pred_span'] = temp_df.apply(handlers.get_pred_text_span, axis = 1)\n        return temp_df.apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean()\n\nhandlers = Handlers()\n\n\n# To add Jaccard Similarity Value on Validation set after each epoch\nclass Metrics(keras.callbacks.Callback):\n    # https://stackoverflow.com/questions/37657260/how-to-implement-custom-metric-in-keras\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, X_sentiment, y_val = self.validation_data[0], self.validation_data[1], self.validation_data[2]\n        y_predict = np.asarray(self.model.predict([X_val, X_sentiment]))\n        print('Val Jaccard Similarity: {}'.format(handlers.batch_jaccard(y_predict, test_df))) \n        return\n\n    def get_data(self):\n        return self._data\n    \nmetrics = Metrics()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass DataManipulationPipeline(object):\n    def __init__(self):\n        # Load dependencies\n        try:\n            self.handlers = handlers\n        except NameError:\n            self.handlers = Handlers()\n            \n        # init vars\n        self.vars = {\n            'MAX_SEQUENCE_LENGTH' : MAX_SEQUENCE_LENGTH\n        }\n\n    def pre_fit(self, X):\n        \n        # Strip the text\n        X['text'] = X['text'].str.strip()\n        # Lower case the text\n        X['trans_text'] = X['text'].apply(str.lower)\n        return X\n\n    def create_sequences(self, X):\n        # Tokenize the word tokens to word_indexes\n        sequences = self.vars['keras_index_tokenizer'].texts_to_sequences(X['tokens'])\n        # Pad the sequences to be fed to NN [Note that this will effectively change the start, end index if padded on post]\n        sequences_padded = pad_sequences(sequences, maxlen=self.vars['MAX_SEQUENCE_LENGTH'], padding='post', truncating='post')        \n\n        return sequences_padded\n    \n    def create_Y(self, X):\n        X['trans_selected_text'] = X['selected_text'].apply(str.lower)\n        X = self.punct_tokenize(X, 'trans_selected_text', 'tokens_selected_text', self.vars['punct_tokenizer'])\n        X['start_end_indices'] = X.apply(lambda x: handlers.return_start_end_indices(x['tokens'], x['tokens_selected_text']), axis=1)\n        # truncate the start end indices to end of MAX LEN of sequence\n        X['start_end_indices'] = X['start_end_indices'].apply(lambda x: (x[0], (MAX_SEQUENCE_LENGTH-1) if x[1]>= MAX_SEQUENCE_LENGTH else x[1]))\n        \n        X = X[(X['start_end_indices'] != (-1,-1))]\n        X = X.reset_index(drop=True)\n        X['start_ind'] = X['start_end_indices'].apply(lambda x: x[0])\n        X['end_ind'] = X['start_end_indices'].apply(lambda x: x[1])\n        Y = hstack(\n            (\n                X['start_ind'].values.reshape(X.shape[0],1),\n                X['end_ind'].values.reshape(X.shape[0],1)\n            )\n        )\n        return Y,X\n    \n    def handle_sentiment_feature_eng(self, X):\n        sentiment_transform = self.vars['sentiment_one_hot'].transform(X['sentiment'].values.reshape((X['sentiment'].shape[0],1)))\n        # Copy over each data row for MAX_SEQUENCE_LENGTH times to send it inside each LSTM sequence\n#         sentiment_transform_repeated = np.array([([sentiment_transform[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n        \n        # To Do: Scale?\n        return sentiment_transform\n    \n    def text_stats(self, X):\n        X['char_len'] = X['trans_text'].apply(len)\n        X['word_len'] = X['tokens'].apply(len)\n        X['char_word_ratio'] = X['char_len']/X['word_len']\n            \n        text_stats_singular = X[['char_len', 'word_len', 'char_word_ratio']].values\n        \n        # Standardize\n#         text_stats_singular        \n\n#         text_stats_repeated = np.array([([text_stats_singular[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n        return text_stats_singular\n\n    def repeater(self, X):\n        return np.array([([X[i] for x in range(MAX_SEQUENCE_LENGTH)]) for i in range(X.shape[0])])\n    \n        \n    \n    # Fits the parameters on train\n    def fit_transform(self, X):\n        \n        # {x}\n        # Remove nulls from train\n        X = X.dropna()\n        \n        X = self.pre_fit(X)\n        \n        # Create word tokens from sentences using NTLK\n        self.vars['punct_tokenizer'] = WordPunctTokenizer()\n        # put sth else inseatd of X foe expanded vocab?\n        X = self.punct_tokenize(X, 'trans_text', 'tokens', self.vars['punct_tokenizer'])\n\n        # Fit Index-Tokenizer the vocab using Keras\n        self.vars['keras_index_tokenizer'] = Tokenizer()\n        # Use all words for extended voacb\n        self.vars['keras_index_tokenizer'].fit_on_texts(\n            pd.concat(\n            [\n                df_objs.df_objects['train']['text'].dropna().str.strip().apply(str.lower),\n                df_objs.df_objects['test']['text'].dropna().str.strip().apply(str.lower)\n            ]\n        ).reset_index(drop=True).apply(WordPunctTokenizer().tokenize)\n        )\n        # len(keras_tokenizer.word_index)\n        \n        # load glove embeddings\n        self.vars['embeddings_index'] = self.handlers.load_gloVe_embeddings() \n        self.vars['embeddings_matrix'] = self.handlers.load_embeddings_matrix(self.vars['embeddings_index'], self.vars['keras_index_tokenizer'])\n\n        # {y}\n        # Create label column - Word Tokenize selected_text, and create label indices\n        Y,X = self.create_Y(X)\n        # Run final X transform after Y since Y transform filters out some X\n        sequences_padded = self.create_sequences(X)\n        \n        # Fit one hot encoder for sentiment (Try standard scalar downstream as well?)\n        self.vars['sentiment_one_hot'] = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        self.vars['sentiment_one_hot'].fit(X['sentiment'].values.reshape((X['sentiment'].shape[0],1)))\n        # print(enc.get_feature_names())\n        X_sentiment_one_hot = self.handle_sentiment_feature_eng(X)\n        \n        # Create text stats\n        X_text_stats = self.text_stats(X)\n        self.vars['X_text_stats_standard_scaler'] = StandardScaler()\n        self.vars['X_text_stats_standard_scaler'].fit(X_text_stats)\n        X_text_stats_scaled = self.vars['X_text_stats_standard_scaler'].transform(X_text_stats)\n        X_agg_numerics_all = hstack([X_sentiment_one_hot, X_text_stats_scaled])\n        \n        X_numerics_repeated = self.repeater(X_agg_numerics_all)\n        \n        return (sequences_padded, X_numerics_repeated, Y, X)\n\n\n    # Transforms using the parameters on train\n    def transform(self, X):\n        \n        X = self.pre_fit(X)\n        \n        # Create word tokens\n        X = self.punct_tokenize(X, 'trans_text', 'tokens', self.vars['punct_tokenizer'])\n        \n        Y = None\n        if 'selected_text' in X.columns:\n            Y,X = self.create_Y(X)\n        sequences_padded = self.create_sequences(X)\n        X_sentiment_one_hot = self.handle_sentiment_feature_eng(X)        \n        # Create text stats\n        X_text_stats = self.text_stats(X)\n        X_text_stats_scaled = self.vars['X_text_stats_standard_scaler'].transform(X_text_stats)\n        \n        X_agg_numerics_all = hstack([X_sentiment_one_hot, X_text_stats_scaled])\n        X_numerics_repeated = self.repeater(X_agg_numerics_all)\n    \n        \n        return (sequences_padded, X_numerics_repeated, Y, X)\n        \n        \n    def punct_tokenize(self, X,old_col, new_col, punct_tokenizer):\n        X[new_col] = X[old_col].apply(punct_tokenizer.tokenize)\n        return X\n        \n        \n\n\n\n# Get the optimum number of word length for sequence model - 40\n# plt.hist(df_objs.df_objects['train']['tokens'].apply(len), bins=100)\n# plt.show()\n    \n        \n        ","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tempy = df_objs.df_objects['train'][df_objs.df_objects['train']['sentiment'] != 'neutral'].reset_index(drop=True)\n# tempy.shape\n# df_objs.df_objects['train']","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_objs.df_objects['train'], test_size=0.15, random_state=42)\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n# X_train, X_test, Y_train, Y_test, idx1, idx2 = train_test_split(df_objs.df_objects['train'], Y, np.arange(Y.shape[0]), test_size=0.15, random_state=42)\n","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mainpulation_pipeline = DataManipulationPipeline()\n(train_manip_X, train_numerics_repeated, train_manip_Y, train_df) = data_mainpulation_pipeline.fit_transform(train)\n(test_manip_X, test_numerics_repeated, test_manip_Y, test_df) = data_mainpulation_pipeline.transform(test)","execution_count":49,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:155: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","name":"stderr"},{"output_type":"stream","text":"GloVe data loaded\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(train_numerics_repeated.shape)\n# print(test_numerics_repeated.shape)\n# # print(train_manip_X_sentiment_one_hot_repeated.shape)\n# # train_stats_repeated\n# # train_manip_X_sentiment_one_hot_repeated\n# # hstack([train_stats_repeated, train_manip_X_sentiment_one_hot_repeated]).shape\n\n# # train_numerics_repeated[120]\n# import matplotlib.pyplot as plt\n# plt.hist(train_df.tokens.apply(len), bins=100)\n# plt.show()\n\n# pd.concat([train_df['end_ind'], tempp['end_ind']])[(train_df['end_ind'] != tempp['end_ind'])]\n# train_manip_X[19132]\n# tempp = train_df.copy()\n","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"19421    49\n19421    60\nName: end_ind, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = len(data_mainpulation_pipeline.vars['keras_index_tokenizer'].word_index)+1\nEMBEDDING_DIM = len(data_mainpulation_pipeline.vars['embeddings_index']['a'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\n# ----------------------------------------------------\ninputLayer_words = Input(shape=(MAX_SEQUENCE_LENGTH,))\ninputLayer_agg_numerics = Input(shape=(\n    MAX_SEQUENCE_LENGTH,\n    train_numerics_repeated.shape[2]\n))\n\n# Embedding layer for the tap names\nwordEmbeddings = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputLayer_words)\nwordEmbeddings = Embedding(\n    input_dim=VOCAB_SIZE, \n    output_dim=EMBEDDING_DIM, \n    input_length=MAX_SEQUENCE_LENGTH, \n    weights=[data_mainpulation_pipeline.vars['embeddings_matrix']],\n    trainable=False)(inputLayer_words)\n# \nmerged_Input = concatenate([wordEmbeddings, inputLayer_agg_numerics])\n# inputLayer_agg_indices\n# # LSTM\nlstm_1 = Bidirectional(LSTM(50, return_sequences = False))(merged_Input)\n# input_shape=(25,(EMBEDDING_DIM))\n\n\ndrp1 = Dropout(0.2)(lstm_1)\n\n# # Dense\ndense_0 = Dense(75, activation='relu')(drp1)\ndrp2 = Dropout(0.2)(dense_0)\ndense_1 = Dense(30, activation='relu')(drp2)\noutputLayer = Dense(2, activation='relu')(dense_1)\n\nmodel_2 = Model(inputs=[inputLayer_words, inputLayer_agg_numerics], outputs=outputLayer)\n\nmodel_2.compile(loss='mse', optimizer='adam')\n# model.compile(loss='mse', optimizer='adam', metrics=[jaccard])\nmodel_2.summary()\n# ----------------------------------------------------\n\nmodel_2.fit([train_manip_X, train_numerics_repeated], train_manip_Y, validation_data=([test_manip_X, test_numerics_repeated], test_manip_Y), batch_size=100, epochs=200, callbacks=[metrics])","execution_count":14,"outputs":[{"output_type":"stream","text":"Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            (None, 50)           0                                            \n__________________________________________________________________________________________________\nembedding_4 (Embedding)         (None, 50, 25)       728175      input_3[0][0]                    \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            (None, 50, 6)        0                                            \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 50, 31)       0           embedding_4[0][0]                \n                                                                 input_4[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_2 (Bidirectional) (None, 100)          32800       concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 100)          0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 75)           7575        dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 75)           0           dense_4[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 30)           2280        dropout_4[0][0]                  \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 2)            62          dense_5[0][0]                    \n==================================================================================================\nTotal params: 770,892\nTrainable params: 42,717\nNon-trainable params: 728,175\n__________________________________________________________________________________________________\nTrain on 21561 samples, validate on 3804 samples\nEpoch 1/200\n21561/21561 [==============================] - 12s 565us/step - loss: 31.2206 - val_loss: 19.7742\nVal Jaccard Similarity: 0.4627901942821138\nEpoch 2/200\n21561/21561 [==============================] - 11s 519us/step - loss: 20.6247 - val_loss: 17.3331\nVal Jaccard Similarity: 0.4952312428662507\nEpoch 3/200\n21561/21561 [==============================] - 11s 504us/step - loss: 18.9766 - val_loss: 17.1112\nVal Jaccard Similarity: 0.514213271323668\nEpoch 4/200\n21561/21561 [==============================] - 11s 509us/step - loss: 18.0290 - val_loss: 16.2962\nVal Jaccard Similarity: 0.5143440830462104\nEpoch 5/200\n 7800/21561 [=========>....................] - ETA: 6s - loss: 17.6050","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-deae26c323ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# ----------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_manip_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_numerics_repeated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_manip_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_manip_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_numerics_repeated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_manip_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"Epoch 98/100\n21561/21561 [==============================] - 8s 357us/step - loss: 26.4123 - val_loss: 32.8301\nVal Jaccard Similarity: 0.5266293328859964\n\nEpoch 99/100\n21561/21561 [==============================] - 8s 356us/step - loss: 26.0379 - val_loss: 32.5830\nVal Jaccard Similarity: 0.5592650588400309\n\nEpoch 100/100\n21561/21561 [==============================] - 8s 352us/step - loss: 25.8462 - val_loss: 32.7917\nVal Jaccard Similarity: 0.5504962474510444"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2.fit([train_manip_X, train_manip_X_sentiment_one_hot_repeated], train_manip_Y, validation_data=([test_manip_X, test_manip_X_sentiment_one_hot_repeated], test_manip_Y), batch_size=500, epochs=100, callbacks=[metrics])","execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_manip_X_sentiment_one_hot_repeated' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-1cacd98d941b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_manip_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_manip_X_sentiment_one_hot_repeated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_manip_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_manip_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_manip_X_sentiment_one_hot_repeated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_manip_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_manip_X_sentiment_one_hot_repeated' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_2.fit([train_manip_X, train_manip_X_sentiment_one_hot_repeated], train_manip_Y, validation_data=([test_manip_X, test_manip_X_sentiment_one_hot_repeated], test_manip_Y), batch_size=5000, epochs=200, callbacks=[metrics])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_2.fit([train_manip_X, train_manip_X_sentiment_one_hot_repeated], train_manip_Y, validation_data=([test_manip_X, test_manip_X_sentiment_one_hot_repeated], test_manip_Y), batch_size=2000, epochs=200, callbacks=[metrics])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model_2.predict([test_manip_X, test_manip_X_sentiment_one_hot_repeated])\n\n# test_X_df = df_objs.df_objects['train'].iloc[idx2].reset_index(drop=True)\ntest_df_1 = pd.concat([test_df, pd.DataFrame(preds, columns=['pred_start', 'pred_end'])], axis=1)\ntest_df_1['tokens_indices'] = test_df_1['trans_text'].apply(handlers.get_token_indices)\nprint((test_df_1['tokens_indices'].apply(len) == test_df_1['tokens'].apply(len)).value_counts())\n\n# Get the predictions\ntest_df_1['out_pred_span'] = test_df_1.apply(handlers.get_pred_text_span, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Check if embedding matrix is correctly created"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Overall Jaccard from Model:')\nprint(test_df_1[['text', 'selected_text', 'out_pred_span']].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\nprint('Overall Jaccard baseline from predicting the complete text:')\nprint(test_df_1[['text', 'selected_text', 'out_pred_span']].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\nprint('Overall Jaccard using model for NOT neutral and baseline for neutral:')\nprint(pd.concat([\n    test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1),\n    test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1)\n]).mean())\n\n\nprint('Model Jaccard for neutral')\nprint(test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\nprint('Baseline Jaccard for neutral from predicting complete text')\nprint(test_df_1[test_df_1['sentiment'] == 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\nprint('Model Jaccard for NOT neutral')\nprint(test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean())\n\nprint('Baseline Jaccard for NOT neutral from predicting complete text')\nprint(test_df_1[test_df_1['sentiment'] != 'neutral'].apply(lambda x: handlers.jaccard(x['selected_text'], x['text']), axis=1).mean())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df_1.apply(lambda x: handlers.jaccard(x['selected_text'], x['out_pred_span']), axis=1).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df_1[['text', 'selected_text', 'out_pred_span']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df_1['selected_text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}