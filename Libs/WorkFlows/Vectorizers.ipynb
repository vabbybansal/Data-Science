{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########### COULD TRY https://scikit-learn.org/dev/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Should Try: Creating a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/udocker/phoenix-worker/environments/python2/pyenv2', '/home/udocker/phoenix-worker/nbextensions', '/home/udocker/phoenix-worker/environments/python2/lib/python2.7', '/home/udocker/phoenix-worker/environments/python2/lib/python2.7/plat-x86_64-linux-gnu', '/home/udocker/phoenix-worker/environments/python2/lib/python2.7/lib-tk', '/home/udocker/phoenix-worker/environments/python2/lib/python2.7/lib-old', '/home/udocker/phoenix-worker/environments/python2/lib/python2.7/lib-dynload', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/home/udocker/phoenix-worker/environments/python2/local/lib/python2.7/site-packages', '/home/udocker/phoenix-worker/environments/python2/lib/python2.7/site-packages', '/home/udocker/phoenix-worker/environments/python2/local/lib/python2.7/site-packages/IPython/extensions', '/home/udocker/.ipython', '/home/udocker/phoenix-worker/environments/python2/local/lib/python2.7/site-packages/jaeger_client/thrift_gen']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-28 18:28:36,676 jaeger_tracing WARNING Jaeger tracer already initialized, skipping\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.path\n",
    "import general.scripts.modeling as mdl\n",
    "import general.scripts.data_manipulation as manip\n",
    "import general.scripts.transformers as transformers\n",
    "import general.scripts.modeling as modeling\n",
    "import general.scripts.uber_helpers as uber_helpers\n",
    "import general.scripts.viz as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-28 18:28:36,719 jaeger_tracing WARNING Jaeger tracer already initialized, skipping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# General python libs\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')  # prettier plots\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', None)\n",
    "# from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier, Booster\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, average_precision_score, roc_curve, auc, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import plot_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import os\n",
    "# Import Spark libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features\n",
    "numerical_features\n",
    "id_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature extractor pipelines\n",
    "data_cleaning_pipeline = Pipeline([\n",
    "        ('df_clean', transformers.BasicDFCleaning())\n",
    "])\n",
    "\n",
    "categorical_feature_clean_pipeline = Pipeline([\n",
    "        ('categorical_feature_select', transformers.FeatureSelector(categorical_features)),\n",
    "        # Prefix for each column in the missing values\n",
    "        ('impute_missing_1', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('impute_missing_none', SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')),\n",
    "        ('makeDF', transformers.MakeDFFromNumpyPipeline(categorical_features)),\n",
    "        ('clean_categorical', transformers.CategoricalCleaning())\n",
    "])\n",
    "\n",
    "numerical_feature_clean_pipeline = Pipeline([\n",
    "        ('numerical_feature_select', transformers.FeatureSelector(numerical_features))\n",
    "])\n",
    "\n",
    "\n",
    "union_categ_numer_after_basic_clean = FeatureUnion( \n",
    "                                transformer_list = [ \n",
    "                                        ('id_features', transformers.FeatureSelector(id_features)),\n",
    "                                        ( 'categorical_pipeline', categorical_feature_clean_pipeline ),\n",
    "                                        ('numerical_pipeline', numerical_feature_clean_pipeline)\n",
    "] )\n",
    "\n",
    "# Categorical Column Pivot Value Vectorize\n",
    "feat = 'categ_feature_1'\n",
    "custom_feature_processing_categ_feature_1_pivot_time_diff = Pipeline([\n",
    "        ('reqd_features', transformers.FeatureSelector(['entity_uuid', feat,'time_min'])),\n",
    "        ('pivot_and_choose', transformers.GroupMultiCategoricalToPivotedColumnsWithExternalValue('entity_uuid', feat, 'time_min', True, 1000000000))\n",
    "])\n",
    "\n",
    "feat = 'categ_feature_2'\n",
    "custom_feature_processing_categ_feature_2_pivot_time_diff = Pipeline([\n",
    "        ('reqd_features', transformers.FeatureSelector(['entity_uuid', feat,'time_min'])),\n",
    "        ('pivot_and_choose', transformers.GroupMultiCategoricalToPivotedColumnsWithExternalValue('entity_uuid', feat, 'time_min', True, 1000000000))\n",
    "])\n",
    "\n",
    "feat = 'categ_feature_3'\n",
    "custom_feature_processing_categ_feature_3_pivot_time_diff = Pipeline([\n",
    "        ('reqd_features', transformers.FeatureSelector(['entity_uuid', feat,'time_min'])),\n",
    "        ('pivot_and_choose', transformers.GroupMultiCategoricalToPivotedColumnsWithExternalValue('entity_uuid', feat, 'time_min', True, 1000000000))\n",
    "])\n",
    "\n",
    "# Sklearn feature unions do not return column names or df, rather only return numpy arrays. Feature names can be later derived from the cutom vectorizers later using 'get_feature_names'\n",
    "union_custom_feature_processing = FeatureUnion( \n",
    "                                transformer_list = [ \n",
    "                                        ('processed_categ_feature_1_pivot', custom_feature_processing_action_pivot_time_diff),\n",
    "                                        ('processed_categ_feature_2_pivot', custom_feature_processing_error_key_pivot_time_diff),\n",
    "                                        ('processed_categ_feature_3_pivot', custom_feature_processing_action_type_pivot_time_diff)\n",
    "#                                         ( 'categorical_pipeline', categorical_feature_clean_pipeline ),\n",
    "] )\n",
    "\n",
    "\n",
    "\n",
    "feature_eng_pipeline = Pipeline([\n",
    "        ('df_clean', data_cleaning_pipeline), # Clean the overall dataframe\n",
    "        ('clean_categorical_and_numerical', union_categ_numer_after_basic_clean), # Run basic cleaning for both categoricals and numericals\n",
    "        ('makeDF', transformers.MakeDFFromNumpyPipeline(id_features+categorical_features+numerical_features)), # Return a DF object instead of an ndarray\n",
    "        ('run_custom_processing', union_custom_feature_processing),\n",
    "])\n",
    "\n",
    "# Count Vectorizers not used in the final pipeline\n",
    "# ------------------------------------------------\n",
    "# Categorical Column Count Vectorize (Restriction: Binary)\n",
    "categ_feature_1_count_vectorize = Pipeline([\n",
    "    ('feature_select', transformers.FeatureSelector(['entity_uuid','categ_feature_1'])),\n",
    "    ('make_text', transformers.GroupByCreateText(['entity_uuid'],'categ_feature_1')),\n",
    "    ('feature_select_action', transformers.FeatureSelector(['categ_feature_1'], True)),\n",
    "    ('count_vectorize', CountVectorizer(binary=True))\n",
    "])\n",
    "\n",
    "# Categorical Column Count Vectorize \n",
    "categ_feature_1_count_vectorize = Pipeline([\n",
    "    ('feature_select', transformers.FeatureSelector(['entity_uuid','categ_feature_1'])),\n",
    "    ('make_text', transformers.GroupByCreateText(['entity_uuid'],'categ_feature_1')),\n",
    "    ('feature_select_action', transformers.FeatureSelector(['categ_feature_1'], True)),\n",
    "    ('count_vectorize', CountVectorizer(binary=False))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_get_labels = Pipeline([\n",
    "    ('pick_one_label', transformers.GroupBySelectOne(['entity_uuid'],'label')),\n",
    "    ('feature_select_label', transformers.FeatureSelector(['label'], True)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test and Train Action Level grouped on contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all_class.drop('label', axis = 1)\n",
    "y = df_all_class['label'].values \n",
    "\n",
    "group_splitter = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=123)\n",
    "train_idx, test_idx = next(group_splitter.split(X, y, groups=df_all_class['entity_uuid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_all_class.iloc[train_idx]\n",
    "test = df_all_class.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transformer Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_eng_pipeline.fit_transform(train)\n",
    "test = feature_eng_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the features\n",
    "f1 = feature_eng_pipeline.named_steps['run_custom_processing'].transformer_list[0][1].named_steps['pivot_and_choose'].get_feature_names()\n",
    "f2 = feature_eng_pipeline.named_steps['run_custom_processing'].transformer_list[1][1].named_steps['pivot_and_choose'].get_feature_names()\n",
    "f3 = feature_eng_pipeline.named_steps['run_custom_processing'].transformer_list[2][1].named_steps['pivot_and_choose'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name_features = Pipeline([\n",
    "    ('makeDF_name_features', transformers.MakeDFFromNumpyPipeline(f1 + f2 + f3)), # Return a DF object instead of an ndarray\n",
    "])\n",
    "train_x = pipeline_name_features.fit_transform(train)\n",
    "test_x = pipeline_name_features.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pipeline_get_labels.fit_transform(train)\n",
    "test_y = pipeline_get_labels.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (XGBoost, [action],all population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbm = modeling.XGBoostModel(train_x, train_y, test_x, test_y, model_type='XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbm.fit_predict_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "plot_importance(xgbm.model_obj, max_num_features=20, importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (General DS)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
